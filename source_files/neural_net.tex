\section{Generalization of neural networks and deep learning}

In the previous section, we saw the concept of uniform convergence, and showed that with this concept, we can get pretty strong results on a variety of problems, including $\ell_2$/$\ell_1$-regularized linear functions, matrix completion, and two-layer neural networks.
However, it turns out that moving beyond these settings with the techniques we've developed is quite challenging.
A couple of key challenges are:
\begin{itemize}
    \item In terms of the sample/learning complexity, how could we go beyond two layers? How does this complexity depend on the data distribution?

    \item How could we incorporate the inductive bias induced by the choice of specific optimization algorithms into the learning complexity?
\end{itemize}
The goal of this section is to tackle the above questions.
In particular, we shall begin by establishing the folklore intuition that the learning complexity of an ML model scales with its number of parameters.
This kind of folklore is especially intriguing in the context of deep networks, as these models have millions (and now billions) of parameters.
However, this high complexity clearly doesn't explain/corroborate with the empirical results we typically see with model fine-tuning (now prompt tuning).

\subsection{The concept of shattering and VC dimension (Lecture 8)}

\paragraph{Motivation about shattering:}
Imagine we want to bound the complexity of a set of half-spaces passing through the origin:
\[ F = \set{z \rightarrow \mathbbm{1}_{w^{\top} z \ge 0} \mid w \in \real^d} \]
For example, think of $w$ as a linear predictor in the case of binary classification problems.
Question: What should the complexity of $F$ be?

Notice that this function class depends on $d$ parameters.
For example, there are $2^d$ possible output vectors for a given input.
However, while $F$ is an infinite set, it only has finitely many possible behaviors on a given set of $n$ samples $x_1, x_2, \dots, x_n$.

\paragraph{Example:} Consider two points in two dimensions:
$z_1 = [3, 0], z_2 = [-3, 0]$.
Recall the class of half-spaces passing through the origin can be written down as
\[ \cF = \set{z \rightarrow \mathbbm{1}_{w^{\top}z \ge 0} \mid w \in \real^2} \]
For this example, the second coordinate of $w$ does not matter.
Hence, the number of possible outputs here is only $2$, i.e., $[1, 0], [-1, 0]$ (instead of $4$).
In particular, this implies that the empirical Rademacher complexity of $\set{z_1, z_2}$ is the same as $\set{[1, 0], [-1, 0]}$ (since we essentially just take the sup over this output set).

More generally, for two function classes $\cF$ and $\cF'$, if
\[ \set{[f(z_1), f(z_2), \dots, f(z_n)] \mid f\in \cF} = \set{[f'(z_1), f'(z_2), \dots, f'(z_n)] \mid f' \in \cF'} \]
Then, $\hat R_n(\cF) = \hat R_n(\cF')$.
In other words, what matters is the output behavior of the function on the $n$ samples.
This observation is useful when we analyze Boolean functions (i.e., those that return either $0$ or $1$), an important example being the loss function class $\cA = \set{z \rightarrow \ell(z, h) \mid h \in \cH}$ where $\ell$ is the zero-one loss.
This observation regarding the output behavior of $\cF$ is captured by the shattering coefficient.

\paragraph{Definition of shattering:}
Let $\cF$ be a family of functions that map a dataset $Z$ to a finite set (such as $\set{0, 1}$).
The shattering coefficient of $\cF$ is the maximum number of behaviors observed on $n$ samples:
\[ s(\cF, n) = \max_{\set{z_1, z_2, \dots, z_n} \subseteq Z} \bigabs{\set{[f(z_1), f(z_2), \dots, f(z_n)] \mid f\in \cF}} \]
We can use Massart's finite lemma (see \eqref{eq_emp_massart}) to bound the empirical Rademacher complexity using the shattering coefficient.
In particular, suppose $\cF$ involves a set of Boolean functions, then clearly the second moment of any such function on $n$ inputs is at most $1$:
\[ \sup_{f\in\cF} \frac 1 n \sum_{i=1}^n (f(z_i))^2 \le 1 \]
Hence, by setting $M = 1$ in \eqref{eq_emp_massart}, we get that the empirical Rademacher complexity of $\cF$ is at most
\[ \hat R_n(\cF) \le \sqrt{\frac {2\log(s(\cF, n))} n} \]
In essence, we have reduced the complexity of learning a possibly infinite class of functions down to a finite number (i.e., the shattering).

As a remark, in the case of zero-one loss, the shattering coefficient $s(\cF, n)$ is at most $2^n$.
Hence, the empirical Rademacher complexity is at most $\sqrt{\frac {2 \log(2^n)} n} = \sqrt 2$.
Therefore, to get meaningful results, we would like the above quantity to go to zero instead, as $n$ goes to infinity.

\paragraph{Example (threshold functions):}
Consider $n$ real values placed on a line.
We claim that the function class $\cF = \set{z \rightarrow \mathbbm{1}_{z \ge t} \mid t \in \real}$ has shattering coefficient $s(\cF,n) = n+1$.

To see this, we notice that on $n$ real-valued inputs, all of the possible outputs are: $[0, 0, \dots, 0], [1, 0, \dots, 0], [1, 1, \dots, 0], \dots, [1, 1, \dots, 1]$, which has $n + 1$ different outcomes in total.

Now we are ready to define the concept of VC dimension.

\paragraph{Definition of VC dimension:} The VC dimension of a family of functions $\cH$ with Boolean outputs is the maximum number of samples that can be \hl{shattered by} $\cH$.
In other words,
\[ VC(\cH) = \sup_{n} \mathbbm{1}[s(\cH, n) = 2^n] \]
Thus, the VC dimension of $\cH$ captures the maximum number of samples whose output behaviors can always be realized using some function $h \in \cH$.

\paragraph{Example (intervals):}
Let $\cH = \set{z \rightarrow \mathbbm{1}[z\in [a, b]] \mid a, b \in \real}$.
We can verify that $s(\cH, 1) = 2$ (fix this point and vary $a, b$), and $s(\cH, 2) = 2$ (fix two different points and vary $a, b$ to achieve all four outcomes).
However, $s(\cH, 3) = 7 < 2^3$---because after fixing the three points, no matter how we set $a, b$, we cannot leave out the middle point.
In other words, we cannot realize the outcome $[1, 0, 1]$.
As a result, the VC dimension of $\cH$ is equal to $2$.

\paragraph{Example (VC dimension of finite-dimensional function classes):}
Let $\cF$ be a real-valued function class.
Let $\cH = \set{x \rightarrow \mathbbm{1}[f(x) \ge 0] \mid f\in \cF}$ be the set of binary classifiers defined by $\cF$.
Then we have that
\[ VC(\cH) \le dim(\cF) \]
where the dimension of $\cF$ refers to the number of basis elements in $\cF$.
For example, if $\cF = \set{x \rightarrow w^{\top} x \mid w\in\real^d}$, then $dim(\cF) = d$.
This result thus connects the algebraic dimension of $\cF$ with the combinatorial dimension of $\cH$.

\paragraph{Proof:} Take any $n$ samples $x_1, x_2, \dots, x_n$ with $n > dim(\cF)$.
We show that these $n$ points cannot be completely shattered by $\cH$.
In other words, we'd like to find some direction that $\cF$ does not cover.
To achieve this, consider the linear map $M(f) = [f(x_1), f(x_2), \dots, f(x_n)] \in \real^n$ that takes a function $f\in \cF$ and outputs a vector on the $n$ samples.

By definition, the dimension of $\set{M(f) \mid f\in\cF}$ is at most $dim(\cF)$.
Since $n > dim(\cF)$, there must exist a nonzero vector $c\in\real^n$ such that $\inner{c}{M(f)} = 0$ for all $f\in\cF$.
In particular, let us assume without loss of generality that there exists an entry of $c$ that is negative.
Otherwise, we could just consider $-c$ with the same argument (since $c$ is nonzero).

Thus, for all functions $f \in \cF$, we have that
\[ \sum_{i = 1}^n c_i f(x_i) = 0 \]
In particular, we now separate the sum into two buckets depending on whether $c_i$ is positive or negative, and re-write the above as
\[ \sum_{i: c_i \ge 0} c_i f(x_i) + \sum_{i: c_i < 0} c_i f(x_i) = 0 \]
Now suppose that the VC dimension of $\cH$ is greater than or equal to the dimension of $\cF$.
Then, for any zero one labeling of $x_1, x_2, \dots, x_n$, that is $y_1, y_2, \dots, y_n$, there must exist some function $f \in \cF$ such that
\[ h = [x_i \rightarrow \mathbbm{1}[f(x_i) \ge 0] = y_i] \in \cH \]
Based on these premises, we now derive a contradiction, which will imply that the $n$ samples cannot be shattered by $\cF$.
This will imply that the VC dimension of $\cH$ should be at most $dim(\cF)$.

To derive this contradiction, let us consider a function $h$ such that:
$h(x_i) = 1$ (hence $f(x_i) \ge 0$) whenever $c_i \ge 0$;
$h(x_i) = 0$ (hence $f(x_i) < 0$) whenever $c_i < 0$.
For such an $h$, the weighted sum of $c_i f(x_i)$ must be strictly positive. This is a contradiction!

\paragraph{Remark:} by invoking the set of linear half spaces through the origin, we can achieve the above dimension $d$.
For this case, we shall construct some set of $d$ points that can be shattered by $\cF$, and the set of $d$ basis vectors will do.
In particular, let us create $d$ samples corresponding to the basis vectors as
\begin{align*}
    z_1 = [1, 0, \dots, 0] \\
    z_2 = [0, 1, \dots, 0] \\
    \cdots \\
    z_d = [0, 0, \dots, 1]
\end{align*}
We'd like to show that for any binary vector $y \in \set{0, 1}^d$, we can find a linear predictor $w \in \real^d$ such that
\begin{align*}
    \mathbbm{1}[w^{\top} z_1 \ge 0] = y_1 \\
    \mathbbm{1}[w^{\top} z_2 \ge 0] = y_2 \\
    \cdots \\
    \mathbbm{1}{w^{\top} z_n \ge 0} = y_n
\end{align*}
We construct the linear predictor $w$ as follows.
Let $I \subseteq \set{1, 2, \dots, d}$ be the set of indices on which $y \in \set{0, 1}^d$ has value $1$.
We then set $w_i = 1$ for any $i \in I$ and set $w_i = -1$ for any $i \notin I$.
Now, we can verify that the above conditions all hold.
This implies that the VC dimension of these linear half spaces in dimension $d$ must be at least $d$.
In conclusion, the VC dimension is equal to $d$.


\paragraph{Connecting shattering to Rademacher complexity:}
For a function class $\cH$ whose VC dimension is at most $d$, its shattering coefficient is at most the following
\begin{align} s(\cH, n) \le \sum_{i=0}^d \binom{n}{i} \le \begin{cases}
2^n & \text{ if } n \le d\\
\big(\frac{en}{d}\big)^d & \text{ if } n > d
\end{cases} \label{eq_sauer}
\end{align}
Now, imagine the case that $n > d$.
Using this result we shall have that
\[ \log (s(\cA, n)) = \log (s(\cH, n)) \le d(\log (n) + 1 - \log (d)), \]
where $\cA$ is the loss function class of $\cH$.
Thus, we now have a result that bounds the empirical Rademacher complexity of $\cA$ using the VC dimension of $\cH$ as follows:
\begin{align} \hat R_n(\cA) \le \sqrt{ \frac {2 \log(s(\cF, n))} n } \le \sqrt{\frac {2d(\log(n) + 1 - \log(d))} n} \le \sqrt{\frac {2 VC(\cH) \ln(en)} {n}} \label{eq_rn} \end{align}
By plugging this result into \eqref{eq_rade_bound}, we thus got a learning bound that only depends on the VC dimension of the hypothesis class $\cH$ and the number of samples $n$.

The proof of \eqref{eq_sauer} is skipped and can be found in Section 3.11, \cite{liang2016cs229t}.

\paragraph{VC dimension of deep neural networks:}
It has been shown that for a network with $W$ weight parameters and $L$ number of layers, the VC dimension of such classes of neural networks with ReLU activation (or piecewise linear activation) function is at most $O(W L \log(W))$ \citep{bartlett2019nearly}.
Therefore, we could now combine this result with \eqref{eq_rn}---together, we now have the folklore statement that the learning complexity of a deep neural network scales with the number of parameters of the network!

\subsection{Analyzing over-parameterized neural networks using neural tangent kernels (Lecture 9)}

\subsubsection{Basics of kernel methods}
The performance of a machine learning model breaks down into two parts:
\begin{align*}
    L(\hat f) - \inf_f L(f) = \underbrace{L(\hat f) - \inf_{f\in F} L(f)}_{\text{Estimation error}} + \underbrace{\inf_{f\in F} L(f) - \inf_{f} L(f)}_{\text{Approximation error}}
\end{align*}
Both optimization and generalization results aim to reduce estimation error.
Approximation error results, on the other hand, relate to the expressivity of a function class.

\paragraph{Example (linear models and kernel features):}
For linear methods (i.e., $f_w(x) = w^{\top} x$ for some parameter $w\in\real^d$ and input feature vector $x\in\real^d$), the estimation error is small, but the approximation error is large.

Kernel methods represent one way to move beyond linear methods.
In kernel methods, we replace $\inner{x}{w}$ with $\inner{\phi(x)}{w}$, where $\phi(x)$ is an arbitrary feature map of $x$.
Then, we may use the kernel features in a regression model.
Suppose we are minimizing the mean squared error using a kernel method:
\[ \ell(w) = \frac 1 {2n} \sum_{i=1}^n (\inner{\phi(x_i)}{w} - y_i)^2  \]

\paragraph{Definition of kernels:} A function $k: \cX \times \cX \rightarrow \real$ is a positive semi-definite kernel if and only if for every subset of inputs $\set{x_1, x_2, \dots, x_n} \subseteq \cX$, the matrix $K\in\real^{n\times n}$ defined by using the kernel map
\[ K_{i, j} = k(x_i, x_j), \text{ for every } 1 \le i, j \le n \]
is a positive semi-definite matrix.\footnote{Recall that a symmetric matrix $K$ is positive semi-definite if for every $x$, $x^{\top} K x \ge 0$.}

\paragraph{Examples (kernels):}
\begin{itemize}
    \item Linear kernels: $k(x, x') = \inner{x}{x'}$. This follows from the fact that $K = XX^{\top}$ is positive semi-definite.

    \item Gaussian kernels: $k(x, x') = \exp\left( - \frac {\bignorm{x - x'}^2} {2\sigma^2} \right)$.
    This is followed by the fact that either the sum or the product of two kernels (the element-wise product (Hadamard product) of two PSD matrices is still PSD) is still a kernel.

    \item Polynomial kernels: $k(x, x') = (1 + \inner{x}{x'})^p$. 
\end{itemize}

\paragraph{Exercise:} verify that both the polynomial kernel and the Gaussian kernel are indeed kernels

\paragraph{Alternative definitions of kernels through feature maps:}
Recall that a feature map $\phi$ maps an input $x$ to a feature embedding $\phi(x)$.
Consider the kernel $k: \cX \times \cX \rightarrow \real$ defined by
\[ k(x, x') = \inner{\phi(x)} {\phi(x')} \]
This must be a kernel.
To show this, we will follow the definition of a kernel by taking a vector $\alpha$ and let
\[ \alpha^{\top} K \alpha = \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \inner{\phi(x_i)}{\phi(x_j)} = {A}^{\top} {A}, \]
where $A = \sum_{i=1}^n \alpha_i \phi(x_i)$.


\subsubsection{Motivation for introducing the neural tangent kernels}

It has been widely observed that gradient-based optimization algorithms often converge to small training errors on a complex neural network model.
A widely believed explanation for this surprising phenomenon is that the neural net is over-parametrized.
The neural tangent kernel represents one of the earliest attempts to mathematically formulate the theory of over-parameterized neural networks.
The neural tangent kernel arises from the dynamics of the predictions applied to the training data.

\paragraph{Problem setup:} Given a training dataset $\set{(x_i, y_i)}_{i = 1}^n$, where $x_i \in\real^d$ and $y_i \in\real$, let $w\in\real^N$ represent the parameters of a neural network.
Let $f_w(x)$ denote the output of the network.
We shall restrict our attention to the mean squared loss (for solving regression problems)
\[ \ell(w) = \frac 1 2 \sum_{i=1}^n (f_w(x_i) - y_i)^2 \]
For the purpose of illustration, let us consider the gradient flow update.
Let $w(t)$ denote the iterate configuration at time $t \ge 0$.
Then, $w(t)$ evolves according to
\[ \frac {d w(t)} {d t} = -\nabla \ell(w(t)) \]

\paragraph{Learning dynamics:} We now show that the dynamics are characterized by a positive semi-definite matrix $H(t) \in \real^{n \times n}$, defined as
\begin{align} H_{i, j}(t) = \biginner {\frac {\partial f_{w(t)}(x_i)} {\partial w_i}} {\frac {\partial f_{w(t)}(x_j)} {\partial w_j}} \label{eq_ht}
\end{align}
Then, let the network outputs at time $t$ be defined as
\[ u(t) = [ f_{w(t)}(x_1), f_{w(t)}(x_2), \dots, f_{w(t)}(x_n) ], \]
for any $t \ge 0$.
Finally, let $y = [y_1, y_2, \dots, y_n]$.
Then, we will show that the network outputs will follow the dynamics as
\[ \frac {d u(t)} {dt} = - H(t) ( u(t) - y ) \]

\paragraph{Proof:} Recall that the evolution of the network parameters follows an update as
\[ \frac {d w(t)} {d t} = -\nabla \ell(w(t)) = - \sum_{i=1}^n ( f_{w(t)}(x_i) - y_i ) \frac {d f_{w(t)}(x_i)} {d w} \]
Using chain rule and multi-variate calculus, the evolution of the network output satisfies
\[ \frac {d f_{w(t)}(x_i)} { d t } = \biginner {\frac {d f_{w(t)}(x_i)} {d w}} {\frac {d w(t)} {dt}} \]
By applying the first step to the second step, we get
\begin{align*} 
    \frac {d f_{w(t)}(x_i)} {d t} 
    &= \biginner{ \frac {d f_{w(t)}(x_i)} {dw} } {- \sum_{j=1}^n (f_{w(t)}(x_j) - y_j) \frac {d f_{w(t)}(x_j)} {d w}} \\
    &= - \sum_{j=1}^n (f_{w(t)}(x_j) - y_j) \underbrace{\biginner{ \frac {d f_{w(t)}(x_i)} {d w} } {\frac {d f_{w(t)} (x_j)} {d w}} }_{\text{$i, j$-th entry of the $H(t)$ matrix}} \\
    &= - H(t) (u(t) - y)
\end{align*}

In summary, the learning dynamics are captured by a symmetric matrix $H(t)$.
Now, how does the NTK arise?
We will define an ultra-wide neural net whose width goes to infinity.
In the limit, it can be shown that the matrix $H(0)$ remains constant during training, i.e., equal to $H(0)$.
Moreover, under a random initialization of parameters, $H(0)$ converges to a deterministic kernel matrix $H^{\star}$ --- the Neural Tangent Kernel (NTK).

\subsubsection{Defining the neural tangent kernel}

\paragraph{Two-layer neural networks:}
Consider the mapping
\begin{align} x \rightarrow \sum_{i=1}^m a_i \sigma(w_i^{\top} x), \label{eq_twolayer} \end{align}
where $\sigma$ represents a nonlinear activation function (such as ReLU or sigmoid).
Now consider the mapping rescaled by $\frac 1 {\sqrt m}$:
\[ x \rightarrow \frac 1 {\sqrt m} \sum_{i=1}^m a_i \sigma(w_i^{\top} x) \]
We shall linearize the right-hand side around the initialization $w_i(0)$, for all $i = 1, 2, \dots, m$;
Essentially, performing Taylor's expansion to derive the following
\begin{align*} 
    x &\rightarrow \frac 1 {\sqrt m} \sum_{i=1}^m a_i \left( \sigma( (w_i(0))^{\top} x ) + (w_i - w_i(0))^{\top} x \sigma'( w_i(0)^{\top} x ) \right) \\
    & = \frac 1 {\sqrt m } \sum_{i=1}^m a_i \left( \sigma( (w_i(0))^{\top} x ) - (w_i(0))^{\top} x \sigma'( (w_i(0))^{\top} x )  \right) + \frac 1 {\sqrt m} x^{\top} \left(\sum_{i=1}^m a_i w_i \sigma'( (w_i(0))^{\top} x ) \right)
\end{align*}

\paragraph{Defining the ultra-wide kernel:}
Recall that the dynamics of predictions is governed by $H(t) \in \real^{n \times n}$ (see \eqref{eq_ht}).
Recall that $f$ is given in \eqref{eq_twolayer}.
Therefore, the $i, j$-th entry of the kernel matrix $H(t)$ is
\[ H_{i, j}(0) = x_i^{\top} x_j \sum_{r = 1}^m \biginner{\frac {a_r \sigma'((w_r(0)^{\top} x_i) )} {\sqrt m}} { \frac {a_r \sigma'( (w_r(0))^{\top} x_j )} {\sqrt m} }  \]

Suppose we sample every $w_r(0)$ from a standard Gaussian distribution.
In addition, suppose we sample $a_r$ uniformly between $\set{+1, -1}$.
Then, one can view the above as the average of $m$ independent random variables.
When $m$ becomes very large, then by the law of large numbers, the average is close to its expectation.
This gives rise to the Neural Tangent Kernel evaluated at $x_, x_j$ as follows:
\[ H^{\star}_{i, j} = x_i^{\top} x_j \exarg{ w \sim \cN(0, \id)  } { \sigma'(w^{\top} x_i) \sigma'(w^{\top} x_j) } \]

\subsubsection{Convergence analysis (Lecture 10)}

Based on the NTK defined above, we now show that the neural network stays close to initialization long enough to get a small loss value.
There are essentially two steps here:
\begin{itemize}
    \item Step 1: For a sufficiently wide network, the randomly initialized neural net is close to the expectation -- the NTK
    
    \item Step 2: For a sufficiently wide network, the kernel matrix at time $t$ remains close to the initialization
\end{itemize}

\paragraph{$H(0)$ is close to $H^{\star}$ for sufficiently large $m$:}
Suppose the activation function (e.g., ReLU) is $1$-Lipschitz continuous.
Suppose for every input $x_i$, the Euclidean norm of $x_i$ is less than $1$, for any $i = 1, 2, \dots, n$.
For any $\epsilon > 0$, if $m \ge O(n^2 \log(\delta^{-1}) / \epsilon^2)$ (recall $n$ is the number of samples at the input), then with probability at least $1 - \delta$, we have that
\[ \bignormFro{H(0) - H^{\star}} \le \epsilon \]

To show that this is true, we will first show that every entry of $H(0)$ is close to $H^{\star}$ with probability $1 - O(\delta / n^2)$.
First, we can see that every individual entry of $H(0)$ is bounded from above by $1$:
\[ \bigabs{x_i^{\top} x_j \cdot \sigma'( (w_r(0))^{\top} x_i ) \sigma'( (w_r(0) )^{\top} x_j )} \le 1 \]
Now we shall apply Hoeffding's inequality (see \eqref{eq_hoeff}), and set the deviation as $\epsilon / n$.
Then, the failure probability becomes
\[ 2\exp\left( - \frac{2\epsilon^2 m} {n^2} \right) \]
When $m \ge O(\frac {n^2 \log(n\delta^{-1})} {\epsilon^2})$, the above probability can be reduced below $O(\delta / n^2)$.
By taking a union bound over all the entries of $H(0)$, we have that the above will hold with probability $1 - \delta$.

Finally, we convert the entry-wise error bound to the operator norm bound:
\begin{align*}
    %\bignorms{H(0) - H^{\star}}
    %\le
    \bignormFro{H(0) - H^{\star}}
    \le \sqrt{n^2 \cdot \frac {\epsilon^2} {n^2}} = \epsilon
\end{align*}

\paragraph{$H(t)$ remains close to $H^{\star}$:}
Suppose that $y_i = O(1)$ for all $i = 1, 2, \dots, n$.
Let $t > 0$, suppose that for all $0 < s < t$, $u_i(s) = O(1)$ for all $i = 1, 2, \dots, n$.
Then if $m \ge O(\frac {n^6 t^2 \log(n \delta^{-1})} {\epsilon^2})$, with probability $1 - \delta$,
\[ \bignormFro{H(t) - H^{\star}} \le \epsilon \]
There are two steps in the proof of this result:
\begin{itemize}
    \item We first show that every weight vector remains close to the initialization if the width is large

    \item We then show that this implies the kernel matrix remains close to the NTK
\end{itemize}

Consider the movement of a single weight vector $w_r$:
\begin{align*}
    \bignorms{ w_r(t) - w_r(0) } &= \bignorms{ \int_{0}^t \frac {d w_r(\tau)} {d \tau} d \tau } \\
    &= \bignorms{ \int_{0}^t \frac 1 {\sqrt m} \sum_{i = 0}^n (u_i(\tau) - y_i ) a_r x_i \sigma'( w_r(\tau)^{\top} x_i ) d \tau } \\
    &\le \int_{0}^t \bignorms{ \frac 1 {\sqrt m} \sum_{i= 0}^n (u_i(\tau) - y_i) a_r x_i \sigma'(w_r (\tau)^{\top} x_i) d\tau } \\
    &\le \frac 1 {\sqrt m} \sum_{i=0}^n \int_0^t \bignorms{ (u_i(\tau) - y_i) a_r x_i \sigma'(w_r(\tau)^{\top} x_i)  }d\tau \le O\left(\frac {t n} {\sqrt m}\right),
\end{align*}
where we use the fact that the inputs $x_i, y_i$ and $a_r, u_i(\tau)$ are all bounded by some constants, and the derivative of $\sigma$ is also bounded by a constant (e.g., sigmoid).

Consider the movement of a single entry of the kernel matrix
\begin{align*}
    H_{i, j}(t) - H_{i, j}(0) =& \frac 1 m \bigabs{\sum_{r = 1}^m \left( \sigma'(w_r(t)^{\top} x_i) \sigma'(w_r(t)^{\top} x_j) - \sigma'(w_r(0)^{\top} x_i) \sigma'(w_r(0)^{\top} x_j)\right) } \\
    \le& \frac 1 m \bigabs{\sum_{r = 1}^m \sigma'(w_r(t))^{\top} x_i ( \sigma'(w_r(t)^{\top} x_j ) - \sigma'(w_r(0)^{\top} x_j) ) } \\
    &+ \frac 1 m \bigabs{\sum_{r = 1}^m \sigma'(w_r(0)^{\top} x_j) (\sigma'(w_r(t)^{\top} x_i - \sigma'(w_r(0)^{\top} x_i))} \\
    \lesssim& \frac 1 m \sum_{r = 1}^m \max_{i = 1}^m \bignorms{ \sigma'(w_r(t)^{\top} x_i) - \sigma'(w_r(0)^{\top} x_i) } \\
    \le& O\left( \frac {t n} {\sqrt m} \right)
\end{align*}

Taken together, we have shown that if $m$ is large enough, then $H(t)$ will stay close to $H^{\star}$ throughout the entire gradient descent dynamic.

\subsubsection{Implications of the neural tangent kernel analysis (Lecture 10)}

The dynamics of network outputs are governed by the following approximation using the NTK matrix
\[ \frac {d u(t)} {d t} \approx - H^{\star} (u(t) - y) \]
We'll use this to describe two implications:
i) Convergence to global minima;
ii) Explaining why NTK converges faster using original labels than using random labels.

Notice that this is a linear dynamical system.
Denote the eigen-decomposition of $H^{\star}$ as $\sum_{ i= 1 }^n \lambda_i v_i v_i^{\top}$.
Now we consider the dynamics of $u(t)$ on each eigenvector separately.
Consider one eigenvector $v_i$ for instance.
We have
\[ \frac {d v_i^{\top} u(t)} {d t} \approx - v_i^{\top} H^{\star} (u(t) - y) = - \lambda_i v_i^{\top} u(t) + \lambda_i v_i^{\top} y, \]
which is equivalent to
\[ \frac {d v_i^{\top} (u(t) - y)} {d t} = - \lambda_i v_i^{\top} (u(t) - y) \]
One function that satisfies this differential equation is when
\[ v_i^{\top} (u(t) - y) = \exp(- \lambda_i t) \]
Notice that $u(t) - y$ is the difference between predictions and training labels at time $t$.

We would like to show that the difference converges to zero when $t$ is large.
The above implies that each component of $u(t) - y$ projected to eigenvector $v_i$ converges to zero exponentially fast at a rate of $\exp(- \lambda_i t)$.

Also notice that the eigenvectors $\set{v_i}_i$ form an orthonormal basis.
Since we know that each component converges to zero, together they imply that $u(t) - y$ must also converge to zero.
This shows that the outputs will eventually converge to the training labels, meaning the training loss will go to zero.

Next, we can see that each component goes to zero at different rates.
For larger eigenvalues, the convergence is faster.
Hence in order to have fast convergence, we would like the projections of the labels onto the top eigenvectors to be large.

For a set of labels, if they align with top eigenvectors, then gradient descent converges quickly.
On the other hand, if the projections are uniform, or aligns with eigenvectors with small eigenvectors, then gradient descent converges with a slow rate.
Below, we compare convergence rates of gradient descent between using original labels, random labels, and worst-case labels.
Data: two classes of MNIST.
Model: $f(a, w) = \frac 1 {\sqrt m} \sum_{i=1}^m a_i \sigma'(w_i^{\top} x)$.

\begin{figure}
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{source_files/figures/randomlabel1.png}    
    \end{minipage}\hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{source_files/figures/randomlabel2.png}
    \end{minipage}
    \caption{Illustration of the loss and the projected values onto the eigenspace.}
    \label{fig:enter-label}
\end{figure}

\subsection{Implicit regularization in over-parameterized matrix factorization (Lecture 11)}

Recall that we discussed the matrix completion problem in lecture 6.
Now, we shall study the gradient descent dynamic of this problem in an over-parameterized setting.

\paragraph{Problem setup:}
Suppose there is an underlying rank-$r$ matrix $X^{\star} \in \real^{d \times d}$.
In particular, for the illustration of the analysis, we shall focus on the case of $X^{\star}$ being positive semi-definite.
As for the input to the problem, suppose we have $m$ random Gaussian matrices whose entries are all drawn from $N(0, 1)$, denoted as $A_1, A_2, \dots, A_m$, all with dimension $d$ by $d$.
In addition, suppose they are symmetric, for example, we could transform each $A_i$ as $(A_i + A_i^{\top}) / 2$.
Let $y_i = \inner{A_i} {UU^{\top}}$, for $i = 1, 2, \dots, n$.

A solution would be to find another positive semi-definite matrix $UU^{\top}$ so that the distance between $UU^{\top}$ and $X^{\star}$ is minimized.
In particular, we shall consider the case that $U \in \real^{d \times d}$.
Then, we set up a mean squared loss as follows:
\[ \ell(U) = \frac 1 n \sum_{i=1}^n \left( \inner{UU^{\top}} {A_i} - y_i \right)^2 \]
In practice, we see that this loss function has many solutions that render its loss to near zero, even though $UU^{\top}$ can be quite far from $X^{\star}$.
A key observation is that the generalization behavior of $U$ should depend on the initialization.
Next we shall make this precise by analyzing the gradient descent dynamic.

Let $U_0 = \alpha \id$, for some small values of $\alpha$.
Then, let the gradient descent update be given by
\[ U_{t + 1} = U_t - \eta \nabla \ell(U_t) = (\id - \eta M_t) U_t, \]
where \[ M_t = \frac 1 m \sum_{i=1}^m \inner{A_i}{U_t U_t^{\top} - X^{\star}}A_i \]

\paragraph{Complete analysis in the rank-$1$ case:}
We shall restrict the case to $r = 1$, where we can write $X^{\star} = u^{\star} {u^{\star}}^{\top}$, for some $u^{\star} \in \real^d$.
Let $\id_{u^{\star}} = \id - \frac {u^{\star} {u^{\star}}^{\top}} {\bignorm{u^{\star}}^2}$.
For simplicity, let's take the norm of $u^{\star}$ as $1$.
Then, we decompose the iterates $U_t$ into two parts into the subspace of $u^{\star}$ and its complement:
\begin{align*} 
    U_t &= u^{\star} {u^{\star}}^{\top} U_t + (\id - \id_{u^{\star}} {u^{\star}}^{\top}) U_t \\
        &\define u^{\star} r_t^{\top} + E_t,
\end{align*}
where we denote by $r_t = U_t^{\top} u^{\star}$, and $E_t = (\id - \id_{u^{\star}}) U_t$.

We will show that the spectral norm and the Frobenius norm of the ``error term'' $E_t$ remains small throughout the iterations, whereas the ``signal'' term $r_t$ grows exponentially fast (in the sense that the norm of $r_t$ grows to the norm of $u^{\star}$.) 
Note that any solution with $\bignorm{r_t} = 1$ and $E_t = 0$ will give exact recovery, and for the purpose of this section we will show that $\bignorm{r_t}$ will converge approximately to $1$ and $E_t$ stays small.

\paragraph{Next lecture:} Wrapping up the implicit regularization analysis.
Office hour on Wednesday is canceled due to a travel arrangement (feel free to email me to schedule a zoom meeting if you'd like to chat!)
{\bf Suggested readings:} \cite{li2018algorithmic}.
