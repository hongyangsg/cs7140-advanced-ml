\section{Generalization of neural networks and deep learning}

In the previous section, we saw the concept of uniform convergence, and showed that with this concept, we can get pretty strong results on a variety of problems, including $\ell_2$/$\ell_1$-regularized linear functions, matrix completion, and two-layer neural networks.
However, it turns out that moving beyond these settings with the techniques we've developed is quite challenging.
A couple of key challenges are:
\begin{itemize}
    \item In terms of the sample/learning complexity, how could we go beyond two layers? How does this complexity depend on the data distribution?

    \item How could we incorporate the inductive bias induced by the choice of specific optimization algorithms into the learning complexity?
\end{itemize}
The goal of this section is to tackle the above questions.
In particular, we shall begin by establishing the folklore intuition that the learning complexity of an ML model scales with its number of parameters.
This kind of folklore is especially intriguing in the context of deep networks, as these models have millions (and now billions) of parameters.
However, this high complexity clearly doesn't explain/corroborate with the empirical results we typically see with model fine-tuning (now prompt tuning).

\subsection{The concept of shattering and VC dimension (Lecture 8)}

\paragraph{Motivation about shattering:}
Imagine we want to bound the complexity of a set of half-spaces passing through the origin:
\[ F = \set{z \rightarrow \mathbbm{1}_{w^{\top} z \ge 0} \mid w \in \real^d} \]
For example, think of $w$ as a linear predictor in the case of binary classification problems.
Question: What should the complexity of $F$ be?

Notice that this function class depends on $d$ parameters.
For example, there are $2^d$ possible output vectors for a given input.
However, while $F$ is an infinite set, it only has finitely many possible behaviors on a given set of $n$ samples $x_1, x_2, \dots, x_n$.

\paragraph{Example:} Consider two points in two dimensions:
$z_1 = [3, 0], z_2 = [-3, 0]$.
Recall the class of half-spaces passing through the origin can be written down as
\[ \cF = \set{z \rightarrow \mathbbm{1}_{w^{\top}z \ge 0} \mid w \in \real^2} \]
For this example, the second coordinate of $w$ does not matter.
Hence, the number of possible outputs here is only $2$, i.e., $[1, 0], [-1, 0]$ (instead of $4$).
In particular, this implies that the empirical Rademacher complexity of $\set{z_1, z_2}$ is the same as $\set{[1, 0], [-1, 0]}$ (since we essentially just take the sup over this output set).

More generally, for two function classes $\cF$ and $\cF'$, if
\[ \set{[f(z_1), f(z_2), \dots, f(z_n)] \mid f\in \cF} = \set{[f'(z_1), f'(z_2), \dots, f'(z_n)] \mid f' \in \cF'} \]
Then, $\hat R_n(\cF) = \hat R_n(\cF')$.
In other words, what matters is the output behavior of the function on the $n$ samples.
This observation is useful when we analyze Boolean functions (i.e., those that return either $0$ or $1$), an important example being the loss function class $\cA = \set{z \rightarrow \ell(z, h) \mid h \in \cH}$ where $\ell$ is the zero-one loss.
This observation regarding the output behavior of $\cF$ is captured by the shattering coefficient.

\paragraph{Definition of shattering:}
Let $\cF$ be a family of functions that map a dataset $Z$ to a finite set (such as $\set{0, 1}$).
The shattering coefficient of $\cF$ is the maximum number of behaviors observed on $n$ samples:
\[ s(\cF, n) = \max_{\set{z_1, z_2, \dots, z_n} \subseteq Z} \bigabs{\set{[f(z_1), f(z_2), \dots, f(z_n)] \mid f\in \cF}} \]
We can use Massart's finite lemma (see \eqref{eq_emp_massart}) to bound the empirical Rademacher complexity using the shattering coefficient.
In particular, suppose $\cF$ involves a set of Boolean functions, then clearly the second moment of any such function on $n$ inputs is at most $1$:
\[ \sup_{f\in\cF} \frac 1 n \sum_{i=1}^n (f(z_i))^2 \le 1 \]
Hence, by setting $M = 1$ in \eqref{eq_emp_massart}, we get that the empirical Rademacher complexity of $\cF$ is at most
\[ \hat R_n(\cF) \le \sqrt{\frac {2\log(s(\cF, n))} n} \]
In essence, we have reduced the complexity of learning a possibly infinite class of functions down to a finite number (i.e., the shattering).

As a remark, in the case of zero-one loss, the shattering coefficient $s(\cF, n)$ is at most $2^n$.
Hence, the empirical Rademacher complexity is at most $\sqrt{\frac {2 \log(2^n)} n} = \sqrt 2$.
Therefore, to get meaningful results, we would like the above quantity to go to zero instead, as $n$ goes to infinity.

\paragraph{Example (threshold functions):}
Consider $n$ real values placed on a line.
We claim that the function class $\cF = \set{z \rightarrow \mathbbm{1}_{z \ge t} \mid t \in \real}$ has shattering coefficient $s(\cF,n) = n+1$.

To see this, we notice that on $n$ real-valued inputs, all of the possible outputs are: $[0, 0, \dots, 0], [1, 0, \dots, 0], [1, 1, \dots, 0], \dots, [1, 1, \dots, 1]$, which has $n + 1$ different outcomes in total.

Now we are ready to define the concept of VC dimension.

\paragraph{Definition of VC dimension:} The VC dimension of a family of functions $\cH$ with Boolean outputs is the maximum number of samples that can be \hl{shattered by} $\cH$.
In other words,
\[ VC(\cH) = \sup_{n} \mathbbm{1}[s(\cH, n) = 2^n] \]
Thus, the VC dimension of $\cH$ captures the maximum number of samples whose output behaviors can always be realized using some function $h \in \cH$.

\paragraph{Example (intervals):}
Let $\cH = \set{z \rightarrow \mathbbm{1}[z\in [a, b]] \mid a, b \in \real}$.
We can verify that $s(\cH, 1) = 2$ (fix this point and vary $a, b$), and $s(\cH, 2) = 2$ (fix two different points and vary $a, b$ to achieve all four outcomes).
However, $s(\cH, 3) = 7 < 2^3$---because after fixing the three points, no matter how we set $a, b$, we cannot leave out the middle point.
In other words, we cannot realize the outcome $[1, 0, 1]$.
As a result, the VC dimension of $\cH$ is equal to $2$.

\paragraph{Example (VC dimension of finite-dimensional function classes):}
Let $\cF$ be a real-valued function class.
Let $\cH = \set{x \rightarrow \mathbbm{1}[f(x) \ge 0] \mid f\in \cF}$ be the set of binary classifiers defined by $\cF$.
Then we have that
\[ VC(\cH) \le dim(\cF) \]
where the dimension of $\cF$ refers to the number of basis elements in $\cF$.
For example, if $\cF = \set{x \rightarrow w^{\top} x \mid w\in\real^d}$, then $dim(\cF) = d$.
This result thus connects the algebraic dimension of $\cF$ with the combinatorial dimension of $\cH$.

\paragraph{Proof:} Take any $n$ samples $x_1, x_2, \dots, x_n$ with $n > dim(\cF)$.
We show that these $n$ points cannot be completely shattered by $\cH$.
In other words, we'd like to find some direction that $\cF$ does not cover.
To achieve this, consider the linear map $M(f) = [f(x_1), f(x_2), \dots, f(x_n)] \in \real^n$ that takes a function $f\in \cF$ and outputs a vector on the $n$ samples.

By definition, the dimension of $\set{M(f) \mid f\in\cF}$ is at most $dim(\cF)$.
Since $n > dim(\cF)$, there must exist a nonzero vector $c\in\real^n$ such that $\inner{c}{M(f)} = 0$ for all $f\in\cF$.
In particular, let us assume without loss of generality that there exists an entry of $c$ that is negative.
Otherwise, we could just consider $-c$ with the same argument (since $c$ is nonzero).

Thus, for all functions $f \in \cF$, we have that
\[ \sum_{i = 1}^n c_i f(x_i) = 0 \]
In particular, we now separate the sum into two buckets depending on whether $c_i$ is positive or negative, and re-write the above as
\[ \sum_{i: c_i \ge 0} c_i f(x_i) + \sum_{i: c_i < 0} c_i f(x_i) = 0 \]
Now suppose that the VC dimension of $\cH$ is greater than or equal to the dimension of $\cF$.
Then, for any zero one labeling of $x_1, x_2, \dots, x_n$, that is $y_1, y_2, \dots, y_n$, there must exist some function $f \in \cF$ such that
\[ h = [x_i \rightarrow \mathbbm{1}[f(x_i) \ge 0] = y_i] \in \cH \]
Based on these premises, we now derive a contradiction, which will imply that the $n$ samples cannot be shattered by $\cF$.
This will imply that the VC dimension of $\cH$ should be at most $dim(\cF)$.

To derive this contradiction, let us consider a function $h$ such that:
$h(x_i) = 1$ (hence $f(x_i) \ge 0$) whenever $c_i \ge 0$;
$h(x_i) = 0$ (hence $f(x_i) < 0$) whenever $c_i < 0$.
For such an $h$, the weighted sum of $c_i f(x_i)$ must be strictly positive. This is a contradiction!

\paragraph{Remark:} by invoking the set of linear half spaces through the origin, we can achieve the above dimension $d$.
For this case, we shall construct some set of $d$ points that can be shattered by $\cF$, and the set of $d$ basis vectors will do.
In particular, let us create $d$ samples corresponding to the basis vectors as
\begin{align*}
    z_1 = [1, 0, \dots, 0] \\
    z_2 = [0, 1, \dots, 0] \\
    \cdots \\
    z_d = [0, 0, \dots, 1]
\end{align*}
We'd like to show that for any binary vector $y \in \set{0, 1}^d$, we can find a linear predictor $w \in \real^d$ such that
\begin{align*}
    \mathbbm{1}[w^{\top} z_1 \ge 0] = y_1 \\
    \mathbbm{1}[w^{\top} z_2 \ge 0] = y_2 \\
    \cdots \\
    \mathbbm{1}{w^{\top} z_n \ge 0} = y_n
\end{align*}
We construct the linear predictor $w$ as follows.
Let $I \subseteq \set{1, 2, \dots, d}$ be the set of indices on which $y \in \set{0, 1}^d$ has value $1$.
We then set $w_i = 1$ for any $i \in I$ and set $w_i = -1$ for any $i \notin I$.
Now, we can verify that the above conditions all hold.
This implies that the VC dimension of these linear half spaces in dimension $d$ must be at least $d$.
In conclusion, the VC dimension is equal to $d$.


\paragraph{Connecting shattering to Rademacher complexity:}
For a function class $\cH$ whose VC dimension is at most $d$, its shattering coefficient is at most the following
\begin{align} s(\cH, n) \le \sum_{i=0}^d \binom{n}{i} \le \begin{cases}
2^n & \text{ if } n \le d\\
\big(\frac{en}{d}\big)^d & \text{ if } n > d
\end{cases} \label{eq_sauer}
\end{align}
Now, imagine the case that $n > d$.
Using this result we shall have that
\[ \log (s(\cA, n)) = \log (s(\cH, n)) \le d(\log (n) + 1 - \log (d)), \]
where $\cA$ is the loss function class of $\cH$.
Thus, we now have a result that bounds the empirical Rademacher complexity of $\cA$ using the VC dimension of $\cH$ as follows:
\begin{align} \hat R_n(\cA) \le \sqrt{ \frac {2 \log(s(\cF, n))} n } \le \sqrt{\frac {2d(\log(n) + 1 - \log(d))} n} \le \sqrt{\frac {2 VC(\cH) \ln(en)} {n}} \label{eq_rn} \end{align}
By plugging this result into \eqref{eq_rade_bound}, we thus got a learning bound that only depends on the VC dimension of the hypothesis class $\cH$ and the number of samples $n$.

The proof of \eqref{eq_sauer} is skipped and can be found in Section 3.11, \cite{liang2016cs229t}.

\paragraph{VC dimension of deep neural networks:}
It has been shown that for a network with $W$ weight parameters and $L$ number of layers, the VC dimension of such classes of neural networks with ReLU activation (or piecewise linear activation) function is at most $O(W L \log(W))$ \citep{bartlett2019nearly}.
Therefore, we could now combine this result with \eqref{eq_rn}---together, we now have the folklore statement that the learning complexity of a deep neural network scales with the number of parameters of the network!

\subsection{Analyzing over-parameterized neural networks using neural tangent kernels (Lecture 9)}

\subsubsection{Basics of kernel methods}
The performance of a machine learning model breaks down into two parts:
\begin{align*}
    L(\hat f) - \inf_f L(f) = \underbrace{L(\hat f) - \inf_{f\in F} L(f)}_{\text{Estimation error}} + \underbrace{\inf_{f\in F} L(f) - \inf_{f} L(f)}_{\text{Approximation error}}
\end{align*}
Both optimization and generalization results aim to reduce estimation error.
Approximation error results, on the other hand, relate to the expressivity of a function class.

\paragraph{Example (linear models and kernel features):}
For linear methods (i.e., $f_w(x) = w^{\top} x$ for some parameter $w\in\real^d$ and input feature vector $x\in\real^d$), the estimation error is small, but the approximation error is large.

Kernel methods represent one way to move beyond linear methods.
In kernel methods, we replace $\inner{x}{w}$ with $\inner{\phi(x)}{w}$, where $\phi(x)$ is an arbitrary feature map of $x$.
Then, we may use the kernel features in a regression model.
Suppose we are minimizing the mean squared error using a kernel method:
\[ \ell(w) = \frac 1 {2n} \sum_{i=1}^n (\inner{\phi(x_i)}{w} - y_i)^2  \]

\paragraph{Definition of kernels:} A function $k: \cX \times \cX \rightarrow \real$ is a positive semi-definite kernel if and only if for every subset of inputs $\set{x_1, x_2, \dots, x_n} \subseteq \cX$, the matrix $K\in\real^{n\times n}$ defined by using the kernel map
\[ K_{i, j} = k(x_i, x_j), \text{ for every } 1 \le i, j \le n \]
is a positive semi-definite matrix.\footnote{Recall that a symmetric matrix $K$ is positive semi-definite if for every $x$, $x^{\top} K x \ge 0$.}

\paragraph{Examples (kernels):}
\begin{itemize}
    \item Linear kernels: $k(x, x') = \inner{x}{x'}$. This follows from the fact that $K = XX^{\top}$ is positive semi-definite.

    \item Gaussian kernels: $k(x, x') = \exp\left( - \frac {\bignorm{x - x'}^2} {2\sigma^2} \right)$.
    This is followed by the fact that either the sum or the product of two kernels (the element-wise product (Hadamard product) of two PSD matrices is still PSD) is still a kernel.

    \item Polynomial kernels: $k(x, x') = (1 + \inner{x}{x'})^p$. 
\end{itemize}

\paragraph{Exercise:} verify that both the polynomial kernel and the Gaussian kernel are indeed kernels

\paragraph{Alternative definitions of kernels through feature maps:}
Recall that a feature map $\phi$ maps an input $x$ to a feature embedding $\phi(x)$.
Consider the kernel $k: \cX \times \cX \rightarrow \real$ defined by
\[ k(x, x') = \inner{\phi(x)} {\phi(x')} \]
This must be a kernel.
To show this, we will follow the definition of a kernel by taking a vector $\alpha$ and let
\[ \alpha^{\top} K \alpha = \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \inner{\phi(x_i)}{\phi(x_j)} = {A}^{\top} {A}, \]
where $A = \sum_{i=1}^n \alpha_i \phi(x_i)$.


\subsubsection{Motivation for introducing the neural tangent kernels}

It has been widely observed that gradient-based optimization algorithms often converge to small training errors on a complex neural network model.
A widely believed explanation for this surprising phenomenon is that the neural net is over-parametrized.
The neural tangent kernel represents one of the earliest attempts to mathematically formulate the theory of over-parameterized neural networks.
The neural tangent kernel arises from the dynamics of the predictions applied to the training data.

\paragraph{Problem setup:} Given a training dataset $\set{(x_i, y_i)}_{i = 1}^n$, where $x_i \in\real^d$ and $y_i \in\real$, let $w\in\real^N$ represent the parameters of a neural network.
Let $f_w(x)$ denote the output of the network.
We shall restrict our attention to the mean squared loss (for solving regression problems)
\[ \ell(w) = \frac 1 2 \sum_{i=1}^n (f_w(x_i) - y_i)^2 \]
For the purpose of illustration, let us consider the gradient flow update.
Let $w(t)$ denote the iterate configuration at time $t \ge 0$.
Then, $w(t)$ evolves according to
\[ \frac {d w(t)} {d t} = -\nabla \ell(w(t)) \]

\paragraph{Learning dynamics:} We now show that the dynamics are characterized by a positive semi-definite matrix $H(t) \in \real^{n \times n}$, defined as
\begin{align} H_{i, j}(t) = \biginner {\frac {\partial f_{w(t)}(x_i)} {\partial w_i}} {\frac {\partial f_{w(t)}(x_j)} {\partial w_j}} \label{eq_ht}
\end{align}
Then, let the network outputs at time $t$ be defined as
\[ u(t) = [ f_{w(t)}(x_1), f_{w(t)}(x_2), \dots, f_{w(t)}(x_n) ], \]
for any $t \ge 0$.
Finally, let $y = [y_1, y_2, \dots, y_n]$.
Then, we will show that the network outputs will follow the dynamics as
\[ \frac {d u(t)} {dt} = - H(t) ( u(t) - y ) \]

\paragraph{Proof:} Recall that the evolution of the network parameters follows an update as
\[ \frac {d w(t)} {d t} = -\nabla \ell(w(t)) = - \sum_{i=1}^n ( f_{w(t)}(x_i) - y_i ) \frac {d f_{w(t)}(x_i)} {d w} \]
Using chain rule and multi-variate calculus, the evolution of the network output satisfies
\[ \frac {d f_{w(t)}(x_i)} { d t } = \biginner {\frac {d f_{w(t)}(x_i)} {d w}} {\frac {d w(t)} {dt}} \]
By applying the first step to the second step, we get
\begin{align*} 
    \frac {d f_{w(t)}(x_i)} {d t} 
    &= \biginner{ \frac {d f_{w(t)}(x_i)} {dw} } {- \sum_{j=1}^n (f_{w(t)}(x_j) - y_j) \frac {d f_{w(t)}(x_j)} {d w}} \\
    &= - \sum_{j=1}^n (f_{w(t)}(x_j) - y_j) \underbrace{\biginner{ \frac {d f_{w(t)}(x_i)} {d w} } {\frac {d f_{w(t)} (x_j)} {d w}} }_{\text{$i, j$-th entry of the $H(t)$ matrix}} \\
    &= - H(t) (u(t) - y)
\end{align*}

In summary, the learning dynamics are captured by a symmetric matrix $H(t)$.
Now, how does the NTK arise?
We will define an ultra-wide neural net whose width goes to infinity.
In the limit, it can be shown that the matrix $H(0)$ remains constant during training, i.e., equal to $H(0)$.
Moreover, under a random initialization of parameters, $H(0)$ converges to a deterministic kernel matrix $H^{\star}$ --- the Neural Tangent Kernel (NTK).

\subsubsection{Defining the neural tangent kernel}

\paragraph{Two-layer neural networks:}
Consider the mapping
\begin{align} x \rightarrow \sum_{i=1}^m a_i \sigma(w_i^{\top} x), \label{eq_twolayer} \end{align}
where $\sigma$ represents a nonlinear activation function (such as ReLU or sigmoid).
Now consider the mapping rescaled by $\frac 1 {\sqrt m}$:
\[ x \rightarrow \frac 1 {\sqrt m} \sum_{i=1}^m a_i \sigma(w_i^{\top} x) \]
We shall linearize the right-hand side around the initialization $w_i(0)$, for all $i = 1, 2, \dots, m$;
Essentially, performing Taylor's expansion to derive the following
\begin{align*} 
    x &\rightarrow \frac 1 {\sqrt m} \sum_{i=1}^m a_i \left( \sigma( (w_i(0))^{\top} x ) + (w_i - w_i(0))^{\top} x \sigma'( w_i(0)^{\top} x ) \right) \\
    & = \frac 1 {\sqrt m } \sum_{i=1}^m a_i \left( \sigma( (w_i(0))^{\top} x ) - (w_i(0))^{\top} x \sigma'( (w_i(0))^{\top} x )  \right) + \frac 1 {\sqrt m} x^{\top} \left(\sum_{i=1}^m a_i w_i \sigma'( (w_i(0))^{\top} x ) \right)
\end{align*}

\paragraph{Defining the ultra-wide kernel:}
Recall that the dynamics of predictions is governed by $H(t) \in \real^{n \times n}$ (see \eqref{eq_ht}).
Recall that $f$ is given in \eqref{eq_twolayer}.
Therefore, the $i, j$-th entry of the kernel matrix $H(t)$ is
\[ H_{i, j}(0) = x_i^{\top} x_j \sum_{r = 1}^m \biginner{\frac {a_r \sigma'((w_r(0)^{\top} x_i) )} {\sqrt m}} { \frac {a_r \sigma'( (w_r(0))^{\top} x_j )} {\sqrt m} }  \]

Suppose we sample every $w_r(0)$ from a standard Gaussian distribution.
In addition, suppose we sample $a_r$ uniformly between $\set{+1, -1}$.
Then, one can view the above as the average of $m$ independent random variables.
When $m$ becomes very large, then by the law of large numbers, the average is close to its expectation.
This gives rise to the Neural Tangent Kernel evaluated at $x_, x_j$ as follows:
\[ H^{\star}_{i, j} = x_i^{\top} x_j \exarg{ w \sim \cN(0, \id)  } { \sigma'(w^{\top} x_i) \sigma'(w^{\top} x_j) } \]

\subsubsection{Convergence analysis (Lecture 10)}

Based on the NTK defined above, we now show that the neural network stays close to initialization long enough to get a small loss value.
There are essentially two steps here:
\begin{itemize}
    \item Step 1: For a sufficiently wide network, the randomly initialized neural net is close to the expectation -- the NTK
    
    \item Step 2: For a sufficiently wide network, the kernel matrix at time $t$ remains close to the initialization
\end{itemize}

\paragraph{$H(0)$ is close to $H^{\star}$ for sufficiently large $m$:}
Suppose the activation function (e.g., ReLU) is $1$-Lipschitz continuous.
Suppose for every input $x_i$, the Euclidean norm of $x_i$ is less than $1$, for any $i = 1, 2, \dots, n$.
For any $\epsilon > 0$, if $m \ge O(n^2 \log(\delta^{-1}) / \epsilon^2)$ (recall $n$ is the number of samples at the input), then with probability at least $1 - \delta$, we have that
\[ \bignormFro{H(0) - H^{\star}} \le \epsilon \]

To show that this is true, we will first show that every entry of $H(0)$ is close to $H^{\star}$ with probability $1 - O(\delta / n^2)$.
First, we can see that every individual entry of $H(0)$ is bounded from above by $1$:
\[ \bigabs{x_i^{\top} x_j \cdot \sigma'( (w_r(0))^{\top} x_i ) \sigma'( (w_r(0) )^{\top} x_j )} \le 1 \]
Now we shall apply Hoeffding's inequality (see \eqref{eq_hoeff}), and set the deviation as $\epsilon / n$.
Then, the failure probability becomes
\[ 2\exp\left( - \frac{2\epsilon^2 m} {n^2} \right) \]
When $m \ge O(\frac {n^2 \log(n\delta^{-1})} {\epsilon^2})$, the above probability can be reduced below $O(\delta / n^2)$.
By taking a union bound over all the entries of $H(0)$, we have that the above will hold with probability $1 - \delta$.

Finally, we convert the entry-wise error bound to the operator norm bound:
\begin{align*}
    %\bignorms{H(0) - H^{\star}}
    %\le
    \bignormFro{H(0) - H^{\star}}
    \le \sqrt{n^2 \cdot \frac {\epsilon^2} {n^2}} = \epsilon
\end{align*}

\paragraph{$H(t)$ remains close to $H^{\star}$:}
Suppose that $y_i = O(1)$ for all $i = 1, 2, \dots, n$.
Let $t > 0$, suppose that for all $0 < s < t$, $u_i(s) = O(1)$ for all $i = 1, 2, \dots, n$.
Then if $m \ge O(\frac {n^6 t^2 \log(n \delta^{-1})} {\epsilon^2})$, with probability $1 - \delta$,
\[ \bignormFro{H(t) - H^{\star}} \le \epsilon \]
There are two steps in the proof of this result:
\begin{itemize}
    \item We first show that every weight vector remains close to the initialization if the width is large

    \item We then show that this implies the kernel matrix remains close to the NTK
\end{itemize}

Consider the movement of a single weight vector $w_r$:
\begin{align*}
    \bignorms{ w_r(t) - w_r(0) } &= \bignorms{ \int_{0}^t \frac {d w_r(\tau)} {d \tau} d \tau } \\
    &= \bignorms{ \int_{0}^t \frac 1 {\sqrt m} \sum_{i = 0}^n (u_i(\tau) - y_i ) a_r x_i \sigma'( w_r(\tau)^{\top} x_i ) d \tau } \\
    &\le \int_{0}^t \bignorms{ \frac 1 {\sqrt m} \sum_{i= 0}^n (u_i(\tau) - y_i) a_r x_i \sigma'(w_r (\tau)^{\top} x_i) d\tau } \\
    &\le \frac 1 {\sqrt m} \sum_{i=0}^n \int_0^t \bignorms{ (u_i(\tau) - y_i) a_r x_i \sigma'(w_r(\tau)^{\top} x_i)  }d\tau \le O\left(\frac {t n} {\sqrt m}\right),
\end{align*}
where we use the fact that the inputs $x_i, y_i$ and $a_r, u_i(\tau)$ are all bounded by some constants, and the derivative of $\sigma$ is also bounded by a constant (e.g., sigmoid).

Consider the movement of a single entry of the kernel matrix
\begin{align*}
    H_{i, j}(t) - H_{i, j}(0) =& \frac 1 m \bigabs{\sum_{r = 1}^m \left( \sigma'(w_r(t)^{\top} x_i) \sigma'(w_r(t)^{\top} x_j) - \sigma'(w_r(0)^{\top} x_i) \sigma'(w_r(0)^{\top} x_j)\right) } \\
    \le& \frac 1 m \bigabs{\sum_{r = 1}^m \sigma'(w_r(t)^{\top} x_i) ( \sigma'(w_r(t)^{\top} x_j ) - \sigma'(w_r(0)^{\top} x_j) ) } \\
    &+ \frac 1 m \bigabs{\sum_{r = 1}^m \sigma'(w_r(0)^{\top} x_j) (\sigma'(w_r(t)^{\top} x_i - \sigma'(w_r(0)^{\top} x_i))} \\
    \lesssim& \frac 1 m \sum_{r = 1}^m \max_{i = 1}^n \bignorms{ \sigma'(w_r(t)^{\top} x_i) - \sigma'(w_r(0)^{\top} x_i) } \\
    \le& O\left( \frac {t n} {\sqrt m} \right)
\end{align*}

Taken together, we have shown that if $m$ is large enough, then $H(t)$ will stay close to $H^{\star}$ throughout the entire gradient descent dynamic.

\subsubsection{Implications of the neural tangent kernel analysis (Lecture 10)}

The dynamics of network outputs are governed by the following approximation using the NTK matrix
\[ \frac {d u(t)} {d t} \approx - H^{\star} (u(t) - y) \]
We'll use this to describe two implications:
i) Convergence to global minima;
ii) Explaining why NTK converges faster using original labels than using random labels.

Notice that this is a linear dynamical system.
Denote the eigen-decomposition of $H^{\star}$ as $\sum_{ i= 1 }^n \lambda_i v_i v_i^{\top}$.
Now we consider the dynamics of $u(t)$ on each eigenvector separately.
Consider one eigenvector $v_i$ for instance.
We have
\[ \frac {d v_i^{\top} u(t)} {d t} \approx - v_i^{\top} H^{\star} (u(t) - y) = - \lambda_i v_i^{\top} u(t) + \lambda_i v_i^{\top} y, \]
which is equivalent to
\[ \frac {d v_i^{\top} (u(t) - y)} {d t} = - \lambda_i v_i^{\top} (u(t) - y) \]
One function that satisfies this differential equation is when
\[ v_i^{\top} (u(t) - y) = \exp(- \lambda_i t) \]
Notice that $u(t) - y$ is the difference between predictions and training labels at time $t$.

We would like to show that the difference converges to zero when $t$ is large.
The above implies that each component of $u(t) - y$ projected to eigenvector $v_i$ converges to zero exponentially fast at a rate of $\exp(- \lambda_i t)$.

Also notice that the eigenvectors $\set{v_i}_i$ form an orthonormal basis.
Since we know that each component converges to zero, together they imply that $u(t) - y$ must also converge to zero.
This shows that the outputs will eventually converge to the training labels, meaning the training loss will go to zero.

Next, we can see that each component goes to zero at different rates.
For larger eigenvalues, the convergence is faster.
Hence in order to have fast convergence, we would like the projections of the labels onto the top eigenvectors to be large.

For a set of labels, if they align with top eigenvectors, then gradient descent converges quickly.
On the other hand, if the projections are uniform, or aligns with eigenvectors with small eigenvectors, then gradient descent converges with a slow rate.
Below, we compare convergence rates of gradient descent between using original labels, random labels, and worst-case labels.
Data: two classes of MNIST.
Model: $f(a, w) = \frac 1 {\sqrt m} \sum_{i=1}^m a_i \sigma'(w_i^{\top} x)$.

\begin{figure}
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{source_files/figures/randomlabel1.png}    
    \end{minipage}\hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{source_files/figures/randomlabel2.png}
    \end{minipage}
    \caption{Illustration of the loss and the projected values onto the eigenspace.}
    \label{fig:enter-label}
\end{figure}

\subsection{Implicit regularization in over-parameterized matrix factorization (Lecture 11)}

Recall that we discussed the matrix completion problem in lecture 6.
Now, we shall study the gradient descent dynamic of this problem in an over-parameterized setting.

\paragraph{Warm-up example:}
Suppose we are doing linear regression on $n$ input samples:
\[ \ell(w) = \frac 1 n \sum_{i=1}^n (x_i^{\top} \beta - y_i) \]
Consider gradient descent:
\[ w_{t+1} = w_t - \eta \nabla \ell(w_t) \]
Suppose $n$ is smaller than the dimension of $w \in \real^d$.
Suppose $w_0 = 0$.
Then, given that $\nabla \ell(w_t) = \frac 2 n\sum_{i=1}^n (x_i^{\top} \beta - y_i) x_i$, $w_{t+1}$ will always lie inside the span of $\set{x_1, x_2, \dots, x_n}$, whose dimension is smaller than $d$.
In other words, gradient descent searches inside a space whose dimension is much smaller than the actual dimension of the entire space.

\paragraph{Problem setup:}
Suppose there is an underlying rank-$r$ matrix $X^{\star} \in \real^{d \times d}$.
In particular, for the illustration of the analysis, we shall focus on the case of $X^{\star}$ being positive semi-definite.
As for the input to the problem, suppose we have $m$ random Gaussian matrices whose entries are all drawn from $N(0, 1)$, denoted as $A_1, A_2, \dots, A_m$, all with dimension $d$ by $d$.
In addition, suppose they are symmetric, for example, we could transform each $A_i$ as $(A_i + A_i^{\top}) / 2$.
Let $y_i = \inner{A_i} {UU^{\top}}$, for $i = 1, 2, \dots, n$.

A solution would be to find another positive semi-definite matrix $UU^{\top}$ so that the distance between $UU^{\top}$ and $X^{\star}$ is minimized.
In particular, we shall consider the case that $U \in \real^{d \times d}$.
Then, we set up a mean squared loss as follows:
\[ \ell(U) = \frac 1 n \sum_{i=1}^n \left( \inner{UU^{\top}} {A_i} - y_i \right)^2 \]
In practice, we see that this loss function has many solutions that render its loss to near zero, even though $UU^{\top}$ can be quite far from $X^{\star}$.
A key observation is that the generalization behavior of $U$ should depend on the initialization.
Next we shall make this precise by analyzing the gradient descent dynamic.

Let $U_0 = \alpha \id$, for some small values of $\alpha$.
Then, let the gradient descent update be given by
\[ U_{t + 1} = U_t - \eta \nabla \ell(U_t) = (\id - \eta M_t) U_t, \]
where \[ M_t = \frac 1 m \sum_{i=1}^m \inner{A_i}{U_t U_t^{\top} - X^{\star}}A_i \]

\paragraph{Complete analysis in the rank-$1$ case:}
We shall restrict the case to $r = 1$, where we can write $X^{\star} = u^{\star} {u^{\star}}^{\top}$, for some $u^{\star} \in \real^d$.
Let $\id_{u^{\star}} = \id - \frac {u^{\star} {u^{\star}}^{\top}} {\bignorm{u^{\star}}^2}$.
For simplicity, let's take the norm of $u^{\star}$ as $1$.
Then, we decompose the iterates $U_t$ into two parts into the subspace of $u^{\star}$ and its complement:
\begin{align*} 
    U_t &= u^{\star} {u^{\star}}^{\top} U_t + (\id - \id_{u^{\star}} {u^{\star}}^{\top}) U_t \\
        &\define u^{\star} r_t^{\top} + E_t,
\end{align*}
where we denote by $r_t = U_t^{\top} u^{\star}$, and $E_t = (\id - \id_{u^{\star}}) U_t$.

We will show that the spectral norm and the Frobenius norm of the ``error term'' $E_t$ remains small throughout the iterations, whereas the ``signal'' term $r_t$ grows exponentially fast (in the sense that the norm of $r_t$ grows to the norm of $u^{\star}$.)
Note that any solution with $\bignorm{r_t} = 1$ and $E_t = 0$ will give exact recovery, and for the purpose of this section we will show that $\bignorm{r_t}$ will converge approximately to $1$ and $E_t$ stays small.

Based on the gradient update above, we derive the update for $E_t$:
\begin{align}
    E_{t+1} &= (\id - \id_{u^{\star}}) (\id - \eta M_t) U_t \notag\\
    &= E_t - \eta (\id - \id_{u^{\star}}) M_t U_t \label{eq_Et_1}
\end{align}

We shall also assume that the set of measurement matrices $(A_1, A_2, \dots, A_m)$ satisfies the restricted isometry property.
A set of linear measurement matrices $A_1, \dots, A_m$ satisfies $(r, \delta)$-restricted isometry property (RIP) if for any $d$ by $d$ matrix $X$ with rank at most $r$, we have that
\begin{align}\label{eq_rip}
    (1 - \delta) \bignormFro{X}^2 \le \frac 1 m \sum_{i=1}^m \inner{A_i}{X}^2 \le (1 + \delta) \bignormFro{X}^2
\end{align}
In the following, we shall assume that the set of measurement matrices satisfies $(4, \delta)$-RIP with $\delta \le c$ (for some small enough constant $c$).
One corollary of the above definition is that for any matrices $X, Y \in \real^{d \times d}$ with rank at most $r$, we also have:
\begin{align}\label{eq_rip_cross} \bigabs{ \frac 1 m \sum_{i=1}^m \inner{A_i}{X} \inner{A_i}{Y} - \inner{X}{Y} } \le \delta \bignormFro{X} \bignormFro{Y}
\end{align}
If $Y$ is at most rank $r$ but $X$ may not be, then we have the following instead:
\begin{align}\label{eq_rip_oneside}
    \bigabs{\frac 1 m \sum_{i=1}^m \inner{A_i}{X} \inner{A_i}{Y} - \inner{X}{Y}} \le \delta \bignorm{X}_{\star} \bignormFro{Y}
\end{align}

\paragraph{Convergence of gradient descent from small initialization in over-parameterized matrix sensing:}
Suppose $\alpha \le \delta \sqrt{\frac 1 d \log(\frac 1 {\delta})}$ and $\eta \lesssim c \delta^2 \log^{-1} (\frac 1 {\delta \alpha})$.
Then after $T = \Theta(\frac {(\alpha \delta)^{-1}} {\eta})$ iterations, we have that
\begin{align}
    \bignormFro{U_T U_T^{\top} - X^{\star}} \lesssim \delta \log(\delta^{-1})
\end{align}

\paragraph{Error dynamics:} Based on the update rule \eqref{eq_Et_1} for $E_t$, we have that
\[ \bignormFro{E_{t+1}}^2 = \bignormFro{E_t}^2 - 2\eta \inner{E_t}{(\id - \id_{u^{\star}}) M_t U_t} + \eta^2 \bignormFro{(\id - \id_{u^{\star}}) M_t U_t }^2 \]
When $\eta$ is sufficiently small, and $\bignormFro{M_t}, \bignorms{U_t}$ are both bounded from above, the third term on the right-hand side is negligible compared to the second term.
Therefore, we focus on the second term.
\begin{align*}
    \inner{E_t}{(\id - \id_{u^{\star}}) M_t U_t} 
    &= \frac 1 m \sum_{i=1}^m \inner{A_i} {U_t U_t^{\top} - X^{\star}} \inner{A_i} {(\id - \id_{u^{\star}}) E_t U_t^{\top}} \\
    &= \frac 1 m \sum_{i=1}^m \inner{A_i} {U_t U_t^{\top} - X^{\star}} \inner{A_i} {E_t U_t^{\top}}
\end{align*}
We use \eqref{eq_rip_cross} to analyze the above, but one gap here is that $U_t U_t^{\top}$ is not necessarily low-rank.
Thus, we decompose it into a low-rank part and an error part with small trace norm.
\begin{align}
    & \frac 1 m \sum_{i=1}^m \inner{A_i} {U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \inner{A_i} {E_t r_t {u^{\star}}^{\top}} \\
    \ge & \inner{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} {E_t r_t {u^{\star}}^{\top} } - \delta \bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \bignormFro{E_t r_t {u^{\star}}^{\top}} \\
    \ge & \inner{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}}{E_t r_t {u^{\star}}^{\top}} - 2\delta \bignorm{E_t r_t}
\end{align}
Above we use the fact that $\bignormFro{U_t U_t^{\top} - X^{\star} E_t E_t^{\top}}, \bignorm{r_t}, \bignorm{u^{\star}}$ are all bounded from above by $1$.
As for the $E_t E_t^{\top}$ part, we first note that $\inner{A_i}{E_t E_t^{\top}}^2 \ge 0$.
Thus, we can focus on the cross terms.
\begin{align*}
    \frac 1 m \sum_{i=1}^m \inner{A_i} {E_t E_t^{\top}} \inner{A_i} {E_t r_t {u^{\star}}^{\top}}
    &\ge \inner{E_t E_t^{\top}} {E_t r_t {u^{\star}}^{\top}} - \delta \bignorm{E_t E_t^{\top}}_{\star} \bignorm{E_t r_t {u^{\star}}^{\top}} \\
    &\ge \inner{E_t E_t^{\top}}{E_t r_t {u^{\star}}^{\top}} - \frac {\delta} 2 \bignorm{E_t r_t},
\end{align*}
where we are using the fact that $\bignorm{E_t E_t^{\top}}_{\star}$ is bounded from above by some constant.
The other cross term would be
\begin{align*}
    & \frac 1 m \sum_{i=1}^m \inner{A_i}{E_t E_t^{\top}} \inner{A_i}{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \\
    \ge& \inner{E_t E_t^{\top}} {U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} - \delta \bignorm{E_t E_t^{\top}}_{\star} \bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \\
    \ge& \inner{E_t E_t^{\top}}{U_tU_t^{\top} - X^{\star}} - \delta \bignorm{E_t E_t^{\top}}_{\star}
\end{align*}
Because $E_t^{\top} X^{\star} = 0$ and also $\bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}}$ is bounded from above by $1$.
All put together, we can conclude that
\begin{align*}
    \inner{E_t}{(\id - \id_{u^{\star}}) M_t U_t}
    \ge \inner{U_t U_t^{\top} - X^{\star}}{E_t U_t^{\top}} - 2.5\delta \bignorm{E_t r_t} - \delta \bignormFro{E_t}^2
\end{align*}
Next, we have
\begin{align*}
    \inner{U_t U_t^{\top} - X^{\star}} {E_t U_t^{\top}}
    = \inner{U_t U_t^{\top}} {E_t U_t^{\top}}
    &= \inner{U_t^{\top}} {U_t^{\top} E_t U_t^{\top}} \\
    &= \inner{U_t^{\top}} {E_t^{\top} E_t U_t^{\top}} \\
    &= \inner{E_t U_t^{\top}} {E_t U_t^{\top}} \ge 0
\end{align*}
In summary, we have proved that
\begin{align}
    \bignormFro{E_{t+1}}^2 \le (1 + \eta \delta) \bignormFro{E_t}^2 + 2.5\delta\eta \bignorm{E_t r_t} + O(\eta^2)
\end{align}
We could also control the growth of the largest singular value of $E_t$ as
\begin{align}
    E_{t+1} &= E_t - \eta (\id - \id_{u^{\star}}) M_t U_t \\ 
    &= E_t - \eta (\id - \id_{u^{\star}}) E_t U_t^{\top} U_t + \eta(\id - \id_{u^{\star}}) (M_t U_t - E_t U_t^{\top} U_t)
\end{align}
Next, notice that $(\id - \id_{u^{\star}}) (U_t U_t^{\top} - X^{\star}) U_t = E_t U_t^{\top} U_t$, and
\begin{align*}
    & \bignorms{(\id - \id_{u^{\star}}) M_t U_t - (\id - \id_{u^{\star}}) (U_t U_t^{\top} - X^{\star}) U_t} \\
    \le & 4 \delta \left(\bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} + \bignorm{E_t E_t^{\top}}_{\star} \right) \bignorms{U_t} \\
    \le & 8 \delta \bignorms{U_t} \le 8 \delta (\bignorm{r_t} + \bignorms{E_t})
\end{align*}
Here, we are using two more properties about RIP measurements:
if $X$ is at most rank $r$, then
\begin{align} \bignorms{\frac 1 m \sum_{i=1}^m \inner{A_i}{X} A_i R - XR} \le \delta \bignormFro{X} \bignorms{R} \label{eq_rip_ur_rank}\end{align}
In addition, for any $X$, we have
\begin{align} \bignorms{\frac 1 m \sum_{i=1}^m \inner{A_i}{X}A_i R - XR} \le \delta \bignorm{X}_{\star} \bignorms{R} \end{align}
The same is true if we have another matrix $U$ where we insert before $A_i$:
\begin{align} \label{eq_rip_ur}
    \bignorms{\frac 1 m \sum_{i=1}^m \inner{A_i}{X} U A_i R - U X R}
    \le \delta \bignorms{U} \bignorm{X}_{\star} \bignorms{R}
\end{align}

Therefore, we can conclude that
\begin{align*}
    \bignorms{E_{t+1}} &\le \bignorms{E_t (\id - \eta U_t^{\top} U_t)} + 2 \eta \delta(\bignorm{r_t} + \bignorms{E_t}) \\
    &\le (1 + 2\eta\delta) \bignorms{E_t} + 2\eta\delta \bignorm{r_t}
\end{align*}

\paragraph{Signal dynamics:} Next, we show that
\begin{align}
    \bignorm{r_{t+1} - (1 + \eta(1 - \bignorm{r_t}^2)) r_t }
    \le \eta \bignorms{E_t}^2 \bignorm{r_t} + 2\eta \delta (\bignorms{E_t} + \bignorm{r_t})
\end{align}
By the update rule $U_{t+1} + (\id - \eta M_t) U_t$.
We have that
\begin{align*}
    r_{t+1} = U_{t+1}^{\top} u^{\star}
    = U_t^{\top} (\id - \eta M_t^{\top}) u^{\star}
    = r_t - \eta U_t^{\top} M_t^{\top} u^{\star}
\end{align*}
Recall that $M_t = \frac 1 m \sum_{i=1}^m \inner{A_i}{U_t U_t^{\top} - X^{\star}} A_i$.
Using \eqref{eq_rip_ur_rank} and \eqref{eq_rip_ur}, we have that
\begin{align}
    \bignorm{r_{t+1} - (r_t - \eta U_t^{\star} (U_t U_t^{\top} - X^{\star}) u^{\star})}
    \le \delta (\bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} + \bignorm{E_t E_t^{\top}}_{\star}) \bignorms{U_t}
\end{align}
Here we observe that
\begin{align}
    U_t^{\top} (U_t U_t^{\top} - X^{\star}) u^{\star}
    = U_t^{\top} U_t r_t - r_t
    = (r_t r_t^{\top} + E_t^{\top} E_t) r_t - r_t
    = (\bignorm{r_t}^2 - 1) r_t - E_t^{\top} E_t r_t
\end{align}
Therefore, we have that
\begin{align}
    \bignorm{r_{t+1} - (1 + \eta(1 - \bignorm{r_t}^2)) r_t}
    \le \eta \bignorm{E_t^{\top} E_t r_t} + 2 \eta\delta \bignorms{U_t}
\end{align}
The above suggests that $r_t$ will eventually drift towards a vector whose norm is equal to one.

Putting things together, we essentially showed that $E_t$ will remain relatively smaller than $r_t$, since $E_t$ only grows by a rate of $1 + \eta \delta$.
Meanwhile, $r_t$ grows by a rate of $1 + O(\eta)$, when it is still bounded away from $1$.

\subsection{Benign overfitting (Lecture 12)}

Benign overfitting represents yet another approach to tackle the mystery behind the generalization of deep neural networks \citep{bartlett2020benign}.
The basic hypothesis behind \hl{benign overfitting} stipulates that for a prediction rule $\hat f$, we can decompose it into
\[ \hat f = \hat f_0 + \Delta,  \]
where $\hat f_0$ is a simple component useful for prediction, $\Delta$ is a useful component useful for benign overfitting.
While $\Delta$ is not useful for prediction, it is benign.
In the setting of over-parameterized matrix sensing from the previous section, we see that $u^{\star }{r_t}^{\top}$ corresponds to $\hat f_0$, whereas $E_t$ relates to $\Delta$.

To better understand the phenomenon of benign overfitting, we shall now consider the case of linear regression.
\begin{itemize}
    \item We denote the covariates as $x \in \real^d$ and the label as $y \in \real$.
    \item We assume that $x$ is sub-Gaussian, meaning that there exists $c > 0$ such that $\Pr\left[\bignorm{x}^2 > c \ex{\bignorm{x}^2}\right] \le \delta$.
    Moreover, $\ex{y \mid x} = x^{\top} \theta^{\star}$.
    \item Let us define
    \begin{align*}
        \Sigma &= \ex{x x^{\top}} = \sum_{i=1}^d \lambda_i v_i v_i^{\top}, \text{ for } i = 1, 2, \dots, d \\
        \theta^{\star} &= \arg \min_{\theta} \ex{(y - x^{\top} \theta)^2 } \\
        \sigma^2 &= \ex{(y - x^{\top} \theta^{\star})^2}
    \end{align*}
\end{itemize}

A key concept here to understand benign overfitting is the \emph{minimum norm interpolator}.
Let $X \in \real^{n \times d}$ denote the features of $n$ samples.
Let $y = [y_1, y_2, \dots, y_n]$ denote the labels.
We have the estimator $\hat \theta = (X^{\top} X)^{\dagger} X^{\top} y$, which solves the problem
\begin{align*}
    \min & \bignorm{\theta}^2 \\
    \mbox{s.t.} & \bignorm{X \theta - y}^2 = \min_{\beta} \bignorm{X \beta - y}^2
\end{align*}

Now we define the excess prediction risk of $\hat \theta$ as
\begin{align*}
    R(\hat \theta) &\define \exarg{x, y}{(y - x^{\top} \hat \theta)^2} - \min_{\theta} \exarg{x, y}{(y - x^{\top} \theta)^2} \\
    & = \exarg{x, y} { (y - x^{\top} \hat \theta)^2 - (y - x^{\top} \theta^{\star})^2 } \\
    & = (\hat \theta - \theta^{\star})^{\top} \Sigma (\hat \theta - \theta^{\star}),
\end{align*}
where we are using the fact that $\ex{xx^{\top}} = \Sigma$ and $\ex{y | x} = x^{\top}\theta^{\star}$.
Therefore, $\Sigma$ determines the importance of parameter directions.
Here is the question: when can the label noise be hidden in $\hat\theta$ without hurting the prediction accuracy?

\paragraph{Characterization of benign overfitting \citep{bartlett2020benign}:}
There exists universal constants $b, c$ such that the excess risk of the minimum norm interpolator satisfies:
\[ R(\hat \theta) \le c \left( \textup{bias}(\theta^{\star}, \Sigma, n) + \sigma^2 \left( \frac {k^{\star}} {n } + \frac n {R_{k^{\star}}(\Sigma)} \right) \right), \]
where $k^{\star} = \min \set{k \ge 0 : r_k(\Sigma) \ge bn}$, and
\begin{align*}
    r_k(\Sigma) &\define \frac {\sum_{i > k} \lambda_i } {\lambda_{k+1}} \\
    R_k(\Sigma) &\define \frac {\Big(\sum_{i > k} \lambda_i\Big)^2} {\sum_{i > k} \lambda_i^2}
\end{align*}
And the bias is given by
\begin{align*}
    \textup{\bias}(\theta^{\star}, \Sigma, n)
    = \bignorm{\theta^{\star}_{k+1 : \infty}}_{\Sigma_{k+1 : \infty}}^2 + \bignorm{\theta^{\star}_{1 : k}}^2_{\Sigma^{-1}_{1 : k}} \left( \frac {\sum_{i > k} \lambda_i} n \right)^2
\end{align*}
Recall that the benign overfitting prediction rule $\hat f$ decomposes as $\hat f_0 + \Delta$.
\begin{itemize}
    \item $\hat f_0$ is the prediction component: $k^{\star}$-dimensional subspace corresponding to $\lambda_1, \dots, \lambda_{k^{\star}}$

    \item $\Delta$ is the benign overfitting component: orthogonal subspace.
    $\Delta$ is benign only if $R_{k^{\star}} \gg n$
\end{itemize}
Intuition: To avoid harming prediction accuracy, the noise energy must be distributed across many unimportant directions.

\subsection{PAC-Bayes generalization bounds (Lecture 13)}

Next, we present a PAC-Bayes generalization bound that depends on the trace of the Hessian.
Our bound can be related to the notion of trace norm, which can be viewed as a measure of model complexity.
For instance, in the context of matrix recovery, the trace norm refers to the sum of the singular values of a matrix and links to the recovery quality of a matrix completion algorithm \citep{srebro2005rank}.
%Its use as a generalization measure for fine-tuning has not been studied before.

To motivate this analysis, let's suppose that we have a pretrained model and we would like to fine-tune this model on a downstream task of interest.
The weights of this pretrained model can be viewed as our prior belief of the target hypothesis in the PAC-Bayes analysis.
Once we have learned a model through fine-tuning the pretrained model on the target task data, we can view this fine-tuned as our adjusted posterior in PAC-Bayes analysis.

More precisely, let $\cD \subseteq \cX \times \cY$ be an unknown data distribution, supported on the feature space $\cX$ and the label space $\cY$.
Given $n$ random samples $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$ drawn from $\cD$, the empirical loss measured by loss function $\ell$ applied to a model $f_W$ with $W \in \real^d$ is:
\[ \hat L(W) = \frac 1 n \sum_{i = 1}^n \ell(f_W(x_i), y_i). \]
The population loss is %given by the expected loss of an unseen sample $(x, y)$ drawn from $\cD$
\[ L(W) = \exarg{(x, y) \sim \cD}{\ell(f_W(x), y)}. \]
It is sufficient to think that the empirical loss is less than the population loss, and the goal is to bound the gap between $\hat L(W)$ and $L(W)$ from above \citep{shalev2014understanding}.
With this setting in mind, we now describe several PAC-Bayesian bounds, which incorporate the prior into the analysis without assuming that the prior is ``correct.''

\paragraph{Occam bound:} Here we assume a countable hypothesis class, and the algorithm outputs a single hypothesis.
More specifically, let $\cH$ be a countable hypothesis class.
Let the loss function be bounded $\ell(z, h) \le [0, 1]$.
Let $\cP$ be any prior distribution over $\cH$.
Then with probability at least $1 - \delta$, we have that
\[ \forall h \in \cH, L(h) \le \hat L(h) + \sqrt{\frac {\log\big(\frac 1 {\cP(h)}\big) + \log(\delta^{-1})} {2n}} \]

\paragraph{Proof:} By Hoeffding's inequality, we have that for any fixed $h\in \cH$:
\[ \Pr[L(h) \ge \hat L(h) + \epsilon] \le \exp(-2n \epsilon^2) \]
If we set the RHS to $\delta \cP(h)$, then we get that with probability at least $\delta \cP(h)$,
\[ L(h) \ge \hat L(h) + \sqrt{ \frac {\log\Big( \frac 1 {\delta \cP(h)}\Big)} {2n}} \]
Applying the union bound across all of $h \in \cH$, we have that with probability at most $\delta$,
there exists an $h \in \cH$ such that the above is true.
By taking the contra-positive, we have derived Occam's bound.

There are two things lacking about the Occam bound.
It only applies to countable $\cH$, which does not include the set of all weight vectors, for example.
It only embraces half of the Bayesian story.
While we have a prior $\cP(h)$, only a single $h \in \cH$ is returned rather than a full posterior $\cQ(h)$.
This is fixed by McAllester's bound.

\paragraph{PAC-Bayesian theorem (McAllester):}
Let the loss function be bounded between zero and one.
Let $\cP$ be any prior distribution over $\cH$.
Let $\cQ$ be any posterior distribution over $\cH$, which is a function of the training data.
Then with probability at least $1 - \delta$,
\[ \exarg{h \sim \cQ}{L(h)} \le \exarg{h \in \cQ}{\hat L(h)} + \sqrt{\frac {KL(\cQ || \cP) + \log(4n \delta^{-1})} {2n-1}}, \]
which holds for all posterior $\cQ$.

\paragraph{Proof:}
For a prior $\cP$, the change of measure inequality states that for any measure $\cQ$,
\[ \log \exarg{f \in \cP}{\exp(\phi(f))} \ge \exarg{f\sim\cQ}{\phi(f)} - D_{KL}(\cQ || \cP), \]
for any function $\phi: \cH \rightarrow \real$.
To see this, let us define a distribution $\cP_G$ via its density with respect to $\cP$ as:
\[ \frac {d\cP_{G}} {d\cP} (f) = \frac {\exp(\phi(f))} {\exarg{h\sim\cP}{\exp(\phi(h))} } \]
This is called the Gibbs measure.
Notice that
\begin{align*}
    D_{KL}(\cQ || \cP) - D_{KL}(\cQ || \cP_G)
    &= \int \left( \log\left(\frac {d \cQ} {d \cP}\right) - \log\left( \frac {d\cQ} {d \cP_G} \right) \right) d \cQ \\
    &= \int \log\left( \frac {d \cP_G} {d \cP} \right) d \cQ \\
    &= \exarg{f\sim\cQ}{\log\left( \frac {\exp(\phi(f))} {\exarg{h\sim\cP}{\exp(\phi(h))}} \right)} \\
    &= \exarg{f\sim\cQ}{\phi(f)} - \log \exarg{h\sim\cP}{\exp(\phi(h))}
\end{align*}
Thus,
\begin{align*}
    \log\exarg{h\sim\cP}{\exp(\phi(h))}
    &= \exarg{f\sim\cQ}{\phi(f)} - D_{KL}(\cQ || \cP) + D_{KL}(\cQ || \cP_G) \\
    &\ge \exarg{f\sim\cQ}{\phi(f)} - D_{KL}(\cQ || \cP),
\end{align*}
since the KL divergence is always non-negative.
Notice that the inequality holds if and only if $\cQ = \cP_G$.

Next, in order to prove McAllester's bound, we consider the function $2(n-1) (\Delta(f))^2$ as $\phi$, where $\Delta(f) = L(f) - \hat L_n(f)$.
Applying this to the above result gives
\begin{align*}
    \exarg{h\sim\cP}{\exp\left(2(n-1) (\Delta(h))^2\right)}
    \ge \exp\left( 2(n-1) \exarg{f\sim\cQ}{(\Delta(f))^2} - D_{KL}(\cQ || \cP)\right)
\end{align*}
Let us take the expectation over the training sample set $S$ (recall that $\cQ$ depends on the training samples) on both sides and notice that the prior $\cP$ does not depend on the training samples.
We get
\begin{align*}
    \exarg{S}{\exarg{h\sim\cP}{\exp(2(n-1) (\Delta(h))^2)}}
    \ge \exarg{S}{\exp\left( 2(n-1) \exarg{f\sim\cQ}{(\Delta(f))^2} - D_{KL}(\cQ || \cP)\right)}
\end{align*}
For a fixed $f$, we can bound $\Delta(f)$ using Hoeffding as:
\[ \Pr[\abs{\Delta(f)} \ge \epsilon] \le 2\exp(-2n \epsilon^2), \]
which holds for any positive $\epsilon < 1$.
Therefore, we have
\begin{align*}
    \exarg{S}{\exp(2(n-1) (\Delta(f))^2)}
    &= \int_{0}^{\infty} \Pr\left[\exp\left(2(n-1) (\Delta(f))^2\right) \ge s\right] ds \\
    &\le 1 + \int_1^{\infty} \Pr\left[\bigabs{\Delta(f)} \ge \sqrt{\frac {\log(s)} {2(n-1)}}\right] ds \\
    &\le 1 + \int_1^{\infty} 2\exp\left(-2n \cdot \frac {\log(s)} {2(n-1)}\right)  ds \\
    & = 1 + 2(n-1) \le 2n
\end{align*}
As for the last step, we notice that
\begin{align*}
    -\int_1^{\infty} s^{-1 - \frac 1 {n-1}} ds
    = -\int_1^{\infty}  d \left( (n-1) s^{-\frac 1 {n-1}} \right) = n-1
\end{align*}
Then, by Markov's inequality,
\begin{align*}
    \Pr\left[\exp\left(2(n-1) \exarg{f\sim\cQ}{(\Delta(f))^2} - D_{KL}(\cQ || \cP)\right) \ge \frac {2n} {\delta}\right]
    \le \delta
\end{align*}
That is, with probability at least $1 - \delta$,
\[ 2(n-1) \exarg{f\sim\cQ}{(\Delta(f))^2} \le \log\left(\frac {2n} {\delta}\right) + D_{KL}(\cQ || \cP) \]
Using Jensen's inequality, the left is greater than \[2(n-1)\left(\exarg{f\sim\cQ}{\Delta(f)}\right)^2 \]
Taking square root on both sides gives us the McAllester's bound.

\paragraph{PAC-Bayesian bound (Catoni):}
Another version of the PAC-Bayesian theorem is shown by \cite{catoni2007pac} (see also Theorem 3.5, \cite{alquier2021user}).
Suppose the loss function $\ell(f_W(x),y)$ lies in a bounded range $[0, C]$ given any $x\in\cX$ with label $y$.
For any $\beta\in (0,1)$ and $\delta\in (0,1)$, with probability at least $1-\delta$, the following holds:
\begin{equation}
    L_{\cQ}(W)\leq \frac{1}{\beta}\hat{L}_{\cQ}(W) + \frac{C \big(KL(\cQ||\cP)+\log\frac{2 \sqrt n}{\delta}\big)}{2\beta(1-\beta)n}, \label{eq_pac}
\end{equation}
%which holds with probability $1 - \delta$ for any $\delta\in(0, 1)$ and $\beta \in (0, 1)$:
%\begin{align}
%    L_{\cQ}(W) \le \frac 1 {\beta} \hat L_{\cQ}(W) + \frac{C(KL(\cQ || \cP) + \log(\delta^{-1}))}{2\beta(1 - \beta) n}, \label{eq_pacbayes}
%\end{align}
where $\cP$ refers to the \textit{prior} distribution, $C$ refers to the upper bound on the loss value $\ell$.
For analyzing fine-tuning, we view $\cP$ as centered at the pretrained model, with covariance matrix $\sigma^2 \id_p$.

\subsection{Noise sensitivity generalization bounds (Lecture 14)}

Let $W$ be any learned hypothesis within the hypothesis space, denoted as $\cH$.
This generalization bound will apply uniformly to $W$ within the hypothesis space.
%assuming that $\cH$ is centered at the pretrained initialization and has a bounded radius of $r > 0$.
We state this result, including the required assumptions, as follows.
Assume that the loss function $\ell$ is bounded between $0$ and $C$ for a fixed constant $C > 0$ on the data distribution $\cD$.
Suppose $\ell(f_W(\cdot), \cdot)$ is twice-differentiable in $W$ and the Hessian matrix $\nabla^2[\ell(f_W(\cdot), \cdot)]$ is Lipschitz continuous within the hypothesis space.
Suppose for any $W$ in $\cH$, the trace norm of the Hessian is less than $\alpha$:
\begin{align}
    \alpha \define \max_{W\in\cH} \max_{(x, y) \sim \cD} \bigtr{\nabla^2 \ell(f_W(x), y)}, \label{eq_trace_norm}
\end{align}
    %(in the sense of equation \eqref{eq_trace_norm}), 
and the $\ell_2$-norm of $W$ is at most $r$ for any $W \in \cH$.
Then, for any $W$ in $\cH$, with probability at least $1 - \delta$ for any $\delta > 0$, the following must hold, for any $\epsilon$ close to zero:
\begin{align}
    L(W) \le (1 + \epsilon) \hat L(W) + (1 + \epsilon) \sqrt{\frac {C \alpha r^2}{n}} + O\Big(n^{-\frac 3 4}\log(\delta^{-1})\Big). \label{eq_main_1}
\end{align}
    %where the trace norm of the hypothesis space taken over the data distribution $\cD$ is given by

An important takeaway of this bound is that we could now measure the RHS in practice, and this leads to a non-vacuous bound on the generalization error.
%We demonstrate that the generalization bound of Theorem \ref{theorem_hessian} accurately correlates with empirical generalization errors.
We experiment with seven methods, including fine-tuning with and without early stopping, distance-based regularization, label smoothing, mixup, etc.
Figure \ref{fig:hessian_reg} shows that the Hessian measure (i.e., the second part of equation \eqref{eq_main_1}) correlates with the generalization errors of these methods.
Second, we plot the Hessian measure (averaged over all layers) between fine-tuning with implicit (early stopping) regularization and explicit (distance-based) regularization.
%We have also included training from scratch as a contrast illustration.
We find that the Hessian measure is smaller for explicit regularization than implicit regularization.

\begin{figure*}[!t]
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/intro_indoor_3_2.pdf}
        \caption{ResNet-50}\label{fig_reg_res1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/intro_caltech_3_2.pdf}
        \caption{ResNet-50}\label{fig_reg_res2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/intro_mrpc_3_2.pdf}
        \caption{BERT-Base}\label{fig_reg_bert}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/average_di.pdf}
        \caption{Average $\cD_i$}\label{fig_avg_d}
    \end{subfigure}
    \caption{
    \ref{fig_reg_res1}, \ref{fig_reg_res2}, \ref{fig_reg_bert}: The Hessian measures accurately correlate with empirical generalization errors for seven fine-tuning methods.
    \ref{fig_avg_d}: $\cD_i$ is smaller for fine-tuning with explicit distance-based regularization than implicit early-stopped regularization.
    Each dot represents the generalization error and the Hessian measures for one fine-tuned model using a particular regularization method.}
    \label{fig:hessian_reg}
\end{figure*}

\paragraph{Proof Sketch:}
We provide a high-level illustration of the proof of equation \eqref{eq_trace_norm}.
Let $\cQ$ denote the \textit{posterior} distribution. Specifically, we consider $\cQ$ as being centered at the learned hypothesis $W$ (which could be anywhere within the hypothesis space), given by a Gaussian distribution $\cN(W, \sigma^2 \id_p)$, where $\id_p$ denotes the $p$ by $p$ identity matrix.
Given a sample $U\sim \cN(0, \sigma^2\id_p)$, let the perturbed loss be given by 
\begin{align}
    \ell_{\cQ}(f_W(x), y) = \exarg{U}{\ell(f_{W + U}(x), y)}. \label{eq_lq}
\end{align}
Then, let $\hat L_{\cQ}(W)$ be the averaged value of $\ell_{\cQ}(f_W(\cdot), \cdot)$, taken over $n$ empirical samples from the training dataset.
Likewise, let $L_{\cQ}(W)$ be the population average of $\ell_{\cQ}(f_W(\cdot), \cdot)$, in expectation over an unseen data sample from the underlying data distribution.



By Taylor's expansion of $\ell_{\cQ}$, we show that:
\begin{align}
    L_{\cQ}(W) &= L(W) + \frac {\sigma^2} 2 \exarg{(x,y)\sim\cD}{\bigtr{\nabla^2 \ell(f_{W}(x), y)}} + O(\sigma^3) \label{eq_taylorh1}\\
    \hat L_{\cQ}(W) &= \hat L(W) + \frac {\sigma^2} {2n} \sum_{i = 1}^n \bigtr{\nabla^2 \ell(f_W(x_i), y_i)} + O(\sigma^3). \label{eq_taylorh2}
\end{align}
Since the Hessian operator is Lipschitz continuous by assumption, we can bound the gap between the above two quantities with $\epsilon$-covering arguments. % (see Lemma \ref{lemma_union_bound} for the precise statement).
By plugging in these results back to the PAC-Bayes bound of equation \eqref{eq_pac}, after some calculation, we can get:
\begin{align}
    L(W) \le \frac 1 {\beta} \hat L(W) + \frac{\sigma^2(1 - \beta) \alpha}{2\beta} + \frac{C r^2 / 2\sigma^2}{2\beta(1 - \beta) n} + O\left(\sigma^3 + \frac{\sigma^2 \sqrt p}{\sqrt n} + \frac{\log(\delta^{-1})}{n}\right). \label{eq_intermediate}
\end{align}
In particular, the above uses the fact that the $\ell_2$-norm of $W$ is less than $r$ for any $W \in \cH$. The KL divergence is discussed below.
Suppose $\cP = N(X, \Sigma)$ and $\cQ = N(Y, \Sigma)$ are both Gaussian distributions with mean vectors given by $X\in\real^p, Y\in\real^p$, and population covariance matrix $\Sigma \in \real^{p \times p}$.
The KL divergence between $\cP$ and $\cQ$ is equal to
\begin{align*}
    KL(\cQ||\cP) = \frac{1}{2}(X- Y)^\top \Sigma^{-1} (X - Y).
\end{align*}
Specifically, if $\Sigma = \sigma^2\id_p$, then the above simplifies to
\begin{align*}
    KL(\cQ||\cP) = \frac{\bignorms{X - Y}^2}{2\sigma^2}.
\end{align*}

By choosing $\sigma^2$ and $\beta$ to minimize equation \eqref{eq_intermediate}, we will obtain equation \eqref{eq_main_1}.
This summarizes the high-level proof idea.

\paragraph{Next lecture:} Start talking about statistical transfer learning.
{\bf Suggested readings:} \cite{zhang2024noise,ju2023generalization}.
