\section{Uniform convergence and generalization}

Recall that we have introduced the empirical risk and the expected risk of a hypothesis (denoted by $L(h)$ and $\hat L(h)$) for some $h$ in a hypothesis class $\cH$.
Suppose we minimize the empirical risk to get $\hat h_{\erm}$. Two questions:
\begin{itemize}
    \item Generalization gap: how does the expected and empirical risks compare for ERM, i.e., $L(\hat h_{\erm}) - \hat L(\hat h_{\erm})$? This is called the \hl{generalization gap}.

    \item Excess risk: how well does ERM do with respect to the best possible hypothesis in the hypothesis class, i.e., $L(\hat h_{\erm}) - \min_{h \in \cH} L(h)$?
    This is also called the \hl{excess risk}.
\end{itemize}

A particularly fruitful framework for analyzing learning algorithms is the probably approximately correct (PAC) framework \citep{valiant1984theory}:

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    A learning algorithm $A$ PAC learns a hypothesis class $\cH$ if
    \begin{enumerate}[label=\alph*)]
        \item For any distribution $\bP^{\star}$ supported over $\cX \times \cY$, and any $\epsilon > 0$, $\delta > 0$
        \item Upon taking $n$ I.I.D. samples from $\bP^{\star}$, $A$ produces an output $\hat h \in \cH$ such that with probability at least $1 - \delta$ (over the randomness of the samples)
        \[ L(\hat h) - \hat L(\hat h) \le \epsilon \]
        \item Further, $A$ runs in time polynomial in $n, d, \epsilon^{-1}, \delta^{-1}$ (where $d$ is the dimension of the input)
    \end{enumerate}
\end{mdframed}

\paragraph{Remark:} Notice that the running time complexity places a bound on the sample complexity as well. We will assume that the empirical risk minimizer can be computed efficiently.
For instance, think of a large neural network whose training loss can be efficiently reduced to reach zero using stochastic gradient descent

\subsection{Learning a realizable, finite hypothesis class (Lecture 2)}

The ERM framework is very general -- we now give a concrete example to illustrate some basic results.

\paragraph{Assumptions (realizable, finite hypothesis):}
i) The size of the hypothesis space, $\cH$, is finite;
ii) There exists a hypothesis $h^{\star} \in \cH$ such that $h^{\star}$ achieves perfect performance, i.e., \[ L(h^{\star}) = \exarg{(x, y) \in \bP^{\star}}{\ell( h^{\star}(x), y )} = 0. \]

Under these assumptions, we shall prove the following property of ERM:

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    Under the above assumptions, with probability $1 - \delta$,
    \begin{align}\label{eq_realizable} L(\hat h_{\erm}) \le \frac {\log(\abs{\cH}) + \log(\delta^{-1})} {n} \end{align}
\end{mdframed}

In particular, to reduce the expected risk below $\epsilon$, we want $n \ge \frac {\log(\abs{\cH}) + \log(\delta^{-1})} {\epsilon}$.
Remarks:
\begin{itemize}
    \item The excess risk only grows logarithmically with the size of the hypothesis class, so we affort to use an exponentially large hypothesis space.

    \item The result is independent of $\bP^{\star}$. This is nice because typically we don’t know the true distribution.
\end{itemize}

\begin{proof}
    We’d like to upper bound the probability of the bad event that $L(\hat h_{\erm}) > \epsilon$:
    \begin{enumerate}[label=\alph*)]
        \item Let $B \subseteq \cH$ be the set of bad hypotheses: $\set{B \in \cH : L(h) > \epsilon}$

        \item We can rewrite our goal as upper bounding the probability of selecting a bad hypothesis $\Pr[L(\hat h_{\erm}) > \epsilon] = \Pr[\hat h_{\erm} \in B]$

        \item Recall the empirical risk of ERM is always zero because at least $\hat L(h^{\star}) = 0$

        \item Hence for any “bad” hypothesis in $B$, they must have zero empirical risk
        \[ \Pr[\hat h_{\erm} \in B] \le \Pr[\exists h \in B : \hat L(h) = 0] \]

        \item Now we shall deal with the above in two steps.
        First, bound $\Pr[ \hat L(h) = 0 ]$ for a fixed $h \in B$.

        Notice that on a random example from $\cP^{\star}$, the accuracy of $h$ should be $1 - L(h)$.
        Since the training data is drawn IID from $\cP^{\star}$, and $L(h) \ge \epsilon$ for any $h \in B$, we get that
        \[ \Pr[ \hat L(h) = 0 ] \le (1 - L(h))^{n} \le (1 - \epsilon)^n \le \exp^{-\epsilon n},  \]
        where we use the fact that $1 - x \le \exp(-x)$.

        \item Second, we want the above to hold simultaneously for all $h \in B$.
        We can apply the union bound to bound the probability of all bad events:
        \begin{align*} 
            \Pr[\exists h \in B : \hat L(h) = 0] &\le \sum_{h \in B} \Pr[ \hat L(h) = 0 ] \\
            & \le \abs{B} \exp(- \epsilon n) \\
            & \le \abs{\cH} \exp( - \epsilon n)
        \end{align*}
        By setting the above at most $\delta$, we conclude that $\epsilon$ must be at least $\frac {\log(\abs{\cH}) + \log(\delta^{-1}) } n$.
    \end{enumerate}
    This concludes the proof for learning finite, realizable hypothesis spaces.
\end{proof}

\paragraph{Takeaway:}
The proof of this result is elementary but illustrates an important pattern that will recur in more complex scenarios.
We are interested in the expected risk, but only have access to empirical risk to choose the ERM:
\begin{itemize}
    \item Step 1 (convergence): for a fixed $h$, show that $\hat L(h)$ is close to $L(h)$ with high probability
    \item Step 2 (uniform convergence): show that the above holds simultaneously for all hypotheses $h \in \cH$
\end{itemize}
However, the assumptions are restrictive.
There exists a perfect hypothesis (realizability). What happens when the problem is not realizable? To answer this, we introduce the tools of concentration estimates.

Second, the hypothesis class is finite. What happens when the number of hypotheses is infinite?
To answer this, we need better ways of measuring the ``size'' of a set -- leading to Rademacher complexity, VC, PAC-Bayes, etc.


\subsection{Using uniform convergence to reason about generalization (Lecture 2)}

We now give a high-level picture of the logic behind how we can use uniform convergence to reason about generalization (in the context of ERM).
We’d like to show that ERM’s excess risk is small:
\begin{align}
    \Pr\left[ L(\hat h_{\erm}) - L(h^{\star}) \ge \epsilon \right] \le \delta
\end{align}
We can expand the excess risk as
\begin{align}
    L(\hat h) - L(h^{\star}) = \underbrace{\left(L(\hat h) - \hat L(\hat h) \right)}_{\text{Uniform convergence}} + \underbrace{\left( \hat L(\hat h) - \hat L(h^{\star}) \right)}_{\le 0} + \underbrace{\left( \hat L(h^{\star} )- L(h^{\star}) \right)}_{\text{Concentration}} 
\end{align}
We'll see how concentration estimates can be used to control this difference in the third part.
However, the same reasoning does not apply to the first part because the ERM $\hat h_{\erm}$ depends on the training examples $\hat L$.
In particular,
\begin{align}
    \hat L(\hat h_{\erm}) = \frac 1 n\sum_{i=1}^n \ell( \hat h_{\erm}(x_i), y_i ).
\end{align}
Due to the correlation, the above is not a sum of independent random variables.
The central thesis of uniform convergence is that if we could ensure that $L(h)$ and $\hat L(h)$ are close for all $h\in\cH$, then $L(\hat h_{\erm})$ must be close to $\hat L(\hat h_{\erm})$ as well.

In summary, our goal of deriving a uniform convergence result can be stated as
\begin{align}
    \Pr[L(\hat h_{\erm}) - L(h^{\star}) \ge \epsilon]
    \le \Pr\left[ \left(\sup_{h \in \cH} \abs{L(h) - \hat L(h)} \right) \ge \frac {\epsilon} 2 \right]
    \le \delta
\end{align}
In particular, the $1/2$ above comes from combining the error terms from the first and third parts together.
Put it in words, we'd like to upper bound the probability that the largest difference between the empirical risk and the expected risk is larger than $\epsilon /2$.

%\paragraph{Next lecture:}
%We'll talk about concentration estimates, which form the most fundamental techniques for dealing with IID random variables.
%Then we'll talk about a generalization bound for finite (not necessarily realizable) hypothesis classes.
%\textbf{Suggested reading:} Chapter 3.1-3.3 of Statistical learning theory lecture notes by Percy Liang.


\subsection{Concentration estimates (Lecture 3)}

\hl{Concentration inequalities} are powerful tools from probability theory that show the average of independent random variables will be concentrated around its expectation.
Concentration estimates are the basis of a large branch of learning theory \citep{bach2024learning} and high-dimensional statistics \citep{wainwright2019high}.
They are one of the most basic tools for studying supervised learning algorithms and models \citep{zhang2023mathematical}, primarily because much of supervised learning deals with in-distribution samples.

\paragraph{Example (mean estimation):}
Let $X_1, X_2, \dots, X_n$ be independent and identically distributed random variables with mean $\mu = \ex{X_i}$, for all $i = 1, 2, \dots, n$.
Define the empirical mean as \[ \hat \mu_n = \frac 1 n \sum_{i=1}^n X_i \]
How does $\hat \mu_n - \mu$ behave?

Three types of statements from probability:
\begin{itemize}
    \item \hl{Consistency:} by law of large numbers,
    \[ \hat \mu_n - \mu \rightarrow 0 \]

    \item \hl{Asymptotic normality:} let the variance of $X_i$ (for all $i$) be equal to $\sigma^2$, by the central limit theorem, we have
    \[ \sqrt{n} (\hat \mu_n - \mu) \sim \cN(0, \sigma^2) \]

    \item \hl{Tail estimates:} for our purpose, we would like to draw a statement of the following type
    \[ \Pr\left[ \bigabs{\hat \mu_n - \mu} \ge \epsilon \right] \le \delta \]
    For getting such tail estimates, we typically need to study the tail of a distribution, for instance, the tail of a Gaussian distribution, etc.
\end{itemize}

\paragraph{Markov's inequality:}
Let $Z \ge 0$ be a non-negative random variable. Then
\[ \Pr\left[Z \ge t \right] \le \frac {\ex{Z}} t \]

\paragraph{Proof:}
Since $Z$ is a non-negative quantity, we always have the condition that
\[ t \mathbbm{1}_{Z \ge t} \le Z \]
To see this, notice that if $Z \ge t$, then the above is true.
On the other hand, if $Z < t$, then the left-hand side above is zero, whereas $Z \ge 0$.
Next, take the expectation over $Z$ on both sides, we get
\[ \ex{t \mathbbm{1}_{Z \ge t}} \le \ex{Z} \Rightarrow \ex{\mathbbm{1}_{Z \ge t}} \le \frac {\ex{Z}} {t} \]
Notice that $\ex{\mathbbm{1}_{Z \ge t}} = \Pr[Z \ge t]$.
Thus, we have shown that Markov's inequality is true.

\paragraph{Chebyshev's inequality:}
Let $X$ be a random variable with mean equal to $\mu$. Then
\[ \Pr\left [ \bigabs{X - \mu} \ge \epsilon \right] \le \frac {\Var{X}} {\epsilon^2} \]

\paragraph{Proof:}
We will use Markov's inequality to get this result.
Let $Z = (X - \mu)^2$ and let $t = \epsilon^2$.
Notice that $Z \ge 0$. Thus, based on Markov's inequality
\[ \Pr\left[Z \ge t \right] \le \frac {\ex{Z}} {t} = \frac {\ex{(Z - \mu)^2}} t = \frac {\Var{Z}} t,  \]
which completes the proof.

\paragraph{Hoeffding's inequality:}
Let $Z_1, Z_2, \dots, Z_n$ be $n$ independent and identically distributed random variables drawn from a distribution with expectation $\mu$ and whose values are bounded from above by one.

Let $\hat\mu_n = \frac 1 n \sum_{i=1}^n Z_i$ denote the mean of the $n$ random variables.
Then, for any $\epsilon \in (0, 1)$, we have
\[ \Pr\left[ \bigabs{\hat\mu_n - \mu} > \epsilon \right] \le 2\exp(-2\epsilon^2 n) \]

The Hoeffding's inequality is a very powerful result when we work with the average of $n$ random variables.
Variants of this inequality (which is restricted to bounded random variables) are also called Chernoff bound.\footnote{\url{https://en.wikipedia.org/wiki/Chernoff_bound}}
Next, we shall see a proof through the use of moment generating functions (MGF).

\paragraph{Definition (Moment generating function):}
For a random variable $X$, its MGF is given by
\[ M_X(t) \define \ex{\exp(t X)} \]
One can also think of the MGF in terms of Taylor's expansion of $\exp(tX)$ as
\[ M_X(t) = 1 + t \ex{X} + \frac{t^2}{2} \ex{X^2} + \frac{t^3} 6 \ex{X^3} + \cdots  \]
Thus, the first moment is the mean of $X$.
The second moment is the variance of $X$ (assuming the mean of $X$ is zero).
And so on.

\paragraph{Property:} The MGF of the sum of two independent random variables $X_1$ and $X_2$ is the product of the MGF of $X_1$ and $X_2$, respectively.
\begin{itemize}
    \item To see this, notice that
    \[ M_{X + Y}(t) = \ex{e^{t(X + Y)}} = \ex{e^{tX} e^{tY}} = \ex{e^{tX}} \ex{e^{tY}} = M_X(t) M_Y(t) \]

    \item Here we have used that $X$ and $Y$ are independent, and hence $e^{tX}$ and $e^{tY}$ are independent, to conclude that $\ex{e^{tX} e^{tY}} = \ex{e^{tX}} \ex{e^{tY}}$
\end{itemize}

The high-level idea for showing the Hoeffding's inequality is obtained by applying Markov's inequality to $e^{tX}$ for some well-chosen value $t$.
From Markov's inequality, we can derive the following useful inequality: for any $t > 0$,
\[ \Pr[X \ge a] = \Pr\left[e^{tX} \ge e^{ta}\right] \le \frac {\ex{e^{tX}}} {e^{ta}}  \]
In particular,
\begin{align} \Pr[X \ge a] \le \min_{t > 0} \frac {\ex{e^{tX}}} {e^{ta}} \label{eq_tail_pos} \end{align}
Similarly, for any $t < 0$,
\[ \Pr[X \le a] = \Pr[e^{tX} \ge e^{ta}] \le \min_{t < 0} \frac {\ex{e^{tX}}} {e^{ta}} \]
Bounds for specific distributions are obtained by choosing appropriate values for $t$.

\subsubsection{Chernoff bounds for the sum of Poisson trials}

We now develop the most commonly used version of the Chernoff bound for the tail distribution of a sum of independent $0$-$1$ random variables, which are also known as Poisson trials.\footnote{Poisson trials differ from Poisson random variables.}

Let $X_1, \dots, X_n$ be a sequence of independent Poisson trials with $\Pr[X_i = 1] = p_i$.
Let $X = \sum_{i=1}^n X_i$, and let
\[ \mu = \ex{X} = \ex{\sum_{i=1}^n X_i} = \sum_{i=1}^n \ex{X_i} = \sum_{i=1}^n p_i \]

For a given $\delta > 0$, we are interested in bounds on $\Pr[X \ge (1 + \delta) \mu]$ and $\Pr[X \le (1 - \delta) \mu]$, that is, the probability that $X$ deviates from its expectation $\mu$ by $\delta \mu$ or more.
To develop a Chernoff bound, we need to compute the moment generating function of $X$.
We start with the MGF of each $X_i$:
\begin{align*}
    M_{X_i} (t) = \ex{e^{t X_i}} = p_i e^t + (1 - p_i) &= 1 + p_i (e^t - 1) \\
    &\le e^{p_i(e^t - 1)},
\end{align*}
where in the last step, we have used the fact that, for any $y$, $1 + y \le e^y$.
Since the $X_i$'s are independent from each other, we take the product of the $n$ MGF to obtain
\begin{align*}
    M_X(t) = \prod_{i=1}^n M_{X_i}(t) &\le \prod_{i=1}^n e^{p_i(e^t - 1)} \\
    &= \exp\left(\sum_{i=1}^n p_i (e^t - 1) \right) = e^{(e^t - 1) \mu}
\end{align*}
Based on this result, we now apply Markov's inequality: for any $t > 0$, we have
\begin{align*}
    \Pr[X \ge (1 + \delta) \mu] &= \Pr[e^{tX} \ge e^{t(1 + \delta) \mu}] \\
    &\le \frac {\ex{e^{tX}}} {e^{t(1 + \delta) \mu}} \\
    &\le \frac {e^{(e^t - 1) \mu}} {e^{t(1 + \delta) \mu}}
\end{align*}
For any $\delta >0$, we can set $t = \ln(1 + \delta) > 0$ to get
\[ \Pr[X \ge (1 + \delta) \mu] \le \left( \frac {e^{\delta}} {(1 + \delta)^{1 + \delta}} \right)^{\mu} \]
For $0 < \delta \le 1$, we need to show that
\[ \frac {e^{\delta}} {(1 + \delta)^{1 + \delta}} \le e^{-\delta^2 / 3} \]
Taking the logarithm of both sides, we obtain
\[ f(\delta) = \delta - (1 + \delta) \ln(1 + \delta) + \frac {\delta^2} 3 \le 0 \]
Computing the derivatives of $f(\delta)$, we have:
\begin{align*}
    f'(\delta) = 1 - \frac {1 + \delta}{1 + \delta} - \ln(1 + \delta) + \frac {2 \delta} 3\\
    f''(\delta) = - \frac 1 {1 + \delta} + \frac 2 3
\end{align*}
We see that the second derivative is less than zero if $\delta < 1/2$, and it is positive otherwise.
Hence, $f'(\delta)$ first decreases and then increases in the interval $[0, 1]$.
Since $f'(0) = 0$ and $f'(1) < 0$, we conclude that $f'(\delta) \le 0$ in the interval $[0, 1]$.
Since $f(0) = 0$, it follows that $f(\delta) \le 0$, which shows that for any $\delta$ between $0$ and $1$, we have
\[ \Pr[X \ge (1 + \delta)\mu] \le e^{-\mu\delta^2 / 3}\]

\subsubsection{Example: Coin flips}

Let $X$ be the number of heads in a sequence of $n$ independent fair coin flipts.
Applying the Chernoff bound, we have
\[ \Pr\left[ \bigabs{X - \frac n 2} \ge \frac 1 2 \sqrt{6n\ln n} \right] \le 2\exp\left( - \frac 1 3 \frac n 2  \frac {6\ln n}{n}\right) = \frac 2 n \]
This demonstrates that the concentration of the number of heads around the mean $n/2$ is very tight.
Most of the time, the deviations from the mean are on the order of $O(\sqrt {n \ln n})$.

\paragraph{Other use cases of Chernoff bound:}
\begin{itemize}
    \item Suppose we are interested in evaluating the probability that a particular gene mutation occurs in the population.
    Given a DNA sample, a lab test can determine if it carries the mutation.
    However, the lab test is expensive and we would like to obtain a relatively reliable estimate from a small number of tests.
\end{itemize}

\subsubsection{Example: Gaussian random variables}

In the next example, we look at the MGF of Gaussian random variables.
Let $X \sim \cN(0, \sigma^2)$. Then, $M_X(t) = e^{\sigma^2 t^2 / 2}$.
To derive this, we use the definition of Gaussian probability density:
\begin{align*}
    M_X(t) = \ex{e^{tX}} &= \int \frac 1 {\sqrt{2\pi\sigma^2}} \exp\left( - \frac {x^2 - 2\sigma^2 tx} {2\sigma^2} \right) dx \\
    &= \int \frac 1 {\sqrt{2 \pi\sigma^2}} \exp\left(- \frac {(x - \sigma^2 t)^2 - \sigma^4 t^2} {2\sigma^2} \right) dx \\
    &= \exp\left( \frac {\sigma^2 t^2} 2 \right)
\end{align*}

Based on the above MGF, we can derive a tail bound by plugging the form of MGF to equation \eqref{eq_tail_pos} to get:
\begin{align*}
    \Pr[X \ge \epsilon] \le \inf_t \exp\left(\frac {\sigma^2 t^2} 2 - t\epsilon \right)
\end{align*}
The infimum of the RHS is attained by setting $e = \frac{\epsilon} {\sigma^2}$, yielding:
\[ \Pr[X \ge \epsilon] \le \exp\left( -\frac{ \epsilon^2} {2\sigma^2} \right) \]

What about non-Gaussian random variables? Notice that the bound still holds if we replace $M_X(t)$ with an upper bound.
This motivates the following definition.

\paragraph{Sub-Gaussian:} A mean zero random variable $X$ is \hl{sub-Gaussian} with parameter $\sigma^2$ if its MGF is bounded as follows:
\begin{align}
    M_X(t) \le \exp\left(\frac{\sigma^2 t^2}2\right)
\end{align}
It follows that for sub-Gaussian $X$, we again have that $\Pr[X \ge \epsilon] \le \exp(-\epsilon^2 /(2\sigma^2))$.

%\paragraph{Next lecture:} Based on the concentration estimates we have established, next time we'll talk about how to get a generalization bound for learning finite hypothesis classes. We'll then start to talk about the Rademacher complexity, which is perhaps the most fundamental tool for deriving generalization bounds. {\bf Suggested readings:} Chapter 3.5 of \cite{liang2016cs229t}, and Chapter 3 of \cite{mitzenmacher2017probability}.

\subsection{Learning finite hypothesis space (Lecture 4)}

Recall from last lecture that we developed the Hoeffding's inequality.
Next we use this result to bound the excess risk of learning a finite hypothesis class (without the realizable condition).

\paragraph{Learning finite hypothesis classes:}
Let $\cH$ be a finite hypothesis class. Let $\ell$ be the zero-one loss function: $\ell(h(x), y) = \mathbbm{1}_{h(x) \neq y}$.
Suppose we minimize the empirical risk to get the minimizer $\hat h\in\cH$.
Then, with probability at least $1 - \delta$ over the randomness of training samples, the excess risk must be bounded by
\begin{align} L(\hat h) - L(h^{\star}) \le \sqrt{\frac {2(\log(\abs{\cH}) + \log(2\delta^{-1}))} n } \label{eq_finite_real} \end{align}
We can contrast this result with \eqref{eq_realizable} (of learning finite, realizable hypothesis space).
The difference is that we now get a slower convergence rate ($n^{-1}$ to $n^{-1/2}$).

\paragraph{Proof:} Recall that the excess risk can be decomposed to
\begin{align*}
    L(\hat h_{\erm}) - L(h^{\star})
    = L(\hat h_{\erm}) - \hat L(\hat h_{\erm}) + \underbrace{\hat L(\hat h_{\erm}) - \hat L(h^{\star})}_{\le 0} + \hat L(h^{\star}) - L(h^{\star})
\end{align*}
Notice that the second term is at most zero.
Thus, we focus on the first and the third terms.
\begin{itemize}
    \item Use the Hoeffding’s inequality to deal with the third term.
    
    \item Apply uniform convergence to upper bound the first term by $\sup_{h\in\cH} \abs{L(h) - \hat L(h)}$ (Use the union bound).
\end{itemize}

\underline{Step 1:} bound $\Pr[L(h) - \hat L(h) \ge \epsilon]$ for a fixed $h \in \cH$.
For a fixed $h \in \cH$, notice that $\hat L(h)$ is the averaged loss among $n$ IID samples.
Each of the loss term is bounded between zero and one (with expectation equal to $L(h)$).
Therefore, by Hoeffding's inequality,
\[ \Pr\left[ \abs{L(h) - \hat L(h)} \ge \epsilon\right] \le 2\exp(-2n\epsilon^2) \]

\underline{Step 2:} apply Step 1 uniformly over all possible $h \in \cH$.
In particular, we can apply union bound over all the possible bad events to get
\[ \Pr\left[ \sup_{h\in\cH} \abs{L(h) - \hat L(h)} \ge \epsilon \right] \le \abs{\cH} \cdot 2\exp(-2n\epsilon^2) = \delta \]
By setting $\epsilon = \sqrt{\frac {2(\log\abs{\cH} + \log(2\delta^{-1}))} n }$, we can get the probability set to $\delta$.

\paragraph{Remarks:} We have removed the realizable assumption by suffering a $\sqrt n$ factor in the generalization bound. The $\sqrt n$ factor arises from sampling noise.
It makes sense that learning is faster when there is no noise.


\subsection{Rademacher complexity I (Lecture 4)}

Both of our generalization bounds require finite hypothesis classes. What about infinite hypothesis classes?

We can't directly apply the union bound to infinite hypothesis classes.
Need a more sophisticated way to measure complexity of a hypothesis class.

With Rademacher complexity, the key idea is \hl{symmetrization}. Along the way, we need an extension of the Hoeffding's inequality to some function of bounded random variables, which is known as McDiarmid's inequality.
Let us first start by motivating why we need these tools.

\subsubsection{Motivation}

Recall that, within the uniform convergence framework, we want to get a statement of the following

\begin{align}
    \Pr\left[ L(\hat h) - L(h^{\star}) \ge \epsilon \right]
    \le \Pr\left[ \sup_{h\in\cH} \abs{L(h) - \hat L(h)} \ge \frac{\epsilon} 2 \right] \le \delta
\end{align}

Since there are two cases here, let us denote

\[ G_n \define \sup_{h\in\cH} L(h) - \hat L(h), \quad G'_n \define \sup_{h\in\cH} \hat L(h) - L(h) \]
Then we have that
\[ \Pr\left[\sup_{h\in\cH} \abs{L(h) - \hat L(h)} \ge \frac {\epsilon} 2 \right] 
\le \Pr\left[G_n \ge \frac {\epsilon} 2 \right] + \Pr\left[G'_n \ge \frac {\epsilon} 2\right] \]
Let us focus on the first case, since the second case will be similar.

Now, our main object becomes $G_n$: This is a rather non-trivial function, because of taking the supremum over a sum of random variables.

Hence, let’s look at its expectation first!
Usually, when we encounter a complicated random variable, we start by examining its first moment, then second moment, and so on.

\subsubsection{Defintion of Rademacher complexity}

Our main object of interest is now $\ex{G_n} = \ex{\sup_{h\in\cH} (L(h) - \hat L(h))}$.
Recall that $\hat L(h)$ is the empirical risk, and $L(h)$ is the expected risk.

This quantity is still quite difficult because it depends on the expected risk, an expectation over the unknown data distribution.
The key idea of symmetrization is to remove this expected risk term with a ``simpler'' term.

\paragraph{Definition of Rademacher complexity:}
Let us imagine $n$ data points $Z'_1 = (x'_1, y'_1), Z'_2 = (x'_2, y'_2), \dots, Z'_n = (x'_n, y'_n)$, sampled from the true data distribution.
Then, clearly $L(h) = \ex{\hat L'(h)}$, where $\hat L'(h)$ is the empirical risk on this ``copy'' dataset.

Hence,
\begin{align*}
    \ex{G_n} &= \exarg{Z_{1:n}}{\sup_{h\in\cH} (L(h) - \hat L(h))} \\
    &= \exarg{Z_{1:n}}{\sup_{h \in\cH} \exarg{Z'_{1:n}} {\hat L'(h) - \hat L(h)} } \\
    &= \exarg{Z_{1:n}} {\exarg{Z'_{1:n}} {\sup_{h \in \cH} (\hat L'(h) - \hat L(h)) }  }
\end{align*}

Let us remove the dependence on the copy dataset. To that end, we introduce IID Rademacher random variables $\sigma_1, \sigma_2, \dots, \sigma_n$ sampled uniformly from $\set{+1, -1}$.

Notice that \[ \hat L'(h) - \hat L(h) = \sum_{i=1}^n (\ell(h(x'_i), y'_i) - \ell(h(x_i), y_i)), \]
is symmetric around zero.
Hence, multiplying each individual term by $\sigma_i$ does not change its distribution.
Thus,
\begin{align*}
    \ex{G_n} &\le \exarg{Z_{1:n}, Z'_{1:n}}{\sup_{h \in \cH} (\hat L'(h) - \hat L(h))} \\
    &= \exarg{Z_{1:n}, Z'_{1:n}, \sigma_{1:n}}{\sup_{h\in \cH} \sigma_i (\ell(h(x'_i), y'_i) - \ell(h(x_i), y_i))} \\
    &\le 2 \exarg{Z_{1:n}, \sigma_{1:n}}{\sup_{h\in\cH}\frac 1 n \sum_{i=1}^n \sigma_i \ell(h(x_i), y_i)}
\end{align*}
The last line (without the factor of $2$) is the \hl{Rademacher complexity} of $\cH$.

In summary, let $\cH$ be a hypothesis class consisting of a class of real-valued functions. Example: two-layer neural nets, linear models.
Define the Rademacher complexity (or Rademacher average) of $\cH$ to be
\[ R_n(\cH) \define \exarg{Z_{1:n}, \sigma_{1:n}}{\sup_{h\in\cH} \frac 1 n \sum_{i=1}^n \sigma_i \ell(h(x_i), y_i)}, \]
where $Z_i = (x_i, y_i)$ is a random sample from the underlying data distribution, and $\sigma_i$ is sampled uniformly from $\set{+1, -1}$.

\subsubsection{Generalization bounds based on Rademacher complexity}

To see the power of the concept we just introduced, here is a very general statement where we can always rely on when we work with supervised learning algorithms and models. 

\paragraph{Generalization bounds based on Rademacher complexity:}
Define \[ \cA = \set{(x, y) \rightarrow \ell(h(x), y) : h \in \cH}\] to be the loss function composed with the hypothesis space.
Let $\cR_n(\cA)$ denote the Rademacher complexity of the function class $\cA$.

With probability at least $1 - \delta$,
\begin{align} L(\hat h_{\erm}) - L(h^{\star}) \le 4 R_n(\cA) + \sqrt{\frac {2\log(2\delta^{-1})} n} \label{eq_rade_bound}\end{align}
To be clear, recall that $\hat h_{\erm}$ is the empirical risk minimizer (ERM), $h^{\star}$ is the expected risk minimizer, and $n$ is the size of the training set.

\paragraph{Proof sketch:}
\begin{itemize}
    \item Show that empirical Rademacher complexity is close to the expectation
    \item Use McDiarmid's inequality, which is essentially a concentration result for functions of IID random variables. We'll introduce this tool shortly
    \item Show that the Rademacher complexity upper bounds the excess risk. We've seen this logic from the motivation
\end{itemize}
In more detail, recall that $G_n = \sup_{h\in\cH} L(h) - \hat L(h)$.
Our first claim is to show that $G_n$ is close to $\ex{G_n}$:
\begin{align}
    \Pr\left[ G_n \ge \ex{G_n} + \epsilon \right] \le \exp(-2n\epsilon^2)
\end{align}
This shows that $G_n$ is indeed close to its expectation plus a small error. Hence it suffices to upper bound its expectation.

Our second claim is to show that $\ex{G_n}$ is upper bounded by the Rademacher complexity.
\begin{align*}
    \ex{G_n} \le 2R_n(\cA).
\end{align*}
We have already seen the proof of this claim.
Combined together, we can derive \eqref{eq_rade_bound}.

\subsubsection{Properties of Rademacher complexity}\label{sec_property_rade}

\paragraph{Boundedness:}
\begin{align*}
    R_n(\cH) \le \max_{h\in \cH} \max_{x, y} \ell(h(x), y)
\end{align*}
This only shows that the Rademacher complexity is bounded by some constant. Usually, we'd like to show it goes to zero as $n$ goes to infinity.

\paragraph{Monotonicity:} If $\cH_1 \subseteq \cH_2$, then $R_n(\cH_1) \le R_n(\cH_2)$.

This is because we now take the supreme over a larger set, hence $R_n$ increases.

\paragraph{Scaling:} $R_n(c\cdot \cH) = c\cdot R_n(\cH)$.

\paragraph{Lipschitz composition:} Suppose $\phi$ is a Lipschitz-continuous function, bounded by some constant $c_{\phi}$. Recall a function is Lipschitz-continuous if changing $x$ only changes the function value by $c_{\phi}$ times.  In addition, $\phi(0) = 0$.

Let $\phi \circ \cH =  \set{(x, y) \rightarrow \phi(h(x), y) : h \in \cH}$, i.e., compose $\phi$ with $h$.
Then, we have that \[ R_n(\phi \circ \cH) \le c_{\phi} \cdot R_n(\cH). \]

The proof essentially uses the contraction property of the sum of Rademacher random variables.
For details, see Corollary 3.17 of \cite{ledoux2013probability}.
This property is useful because we can start by studying a simpler hypothesis class, and then compose more functions with the class without going through the calculation again.

\paragraph{Convex hull:} The convex hull of a set $\cH$ is when we take all possible linear combinations of the hypotheses in $\cH$.
One useful property is that taking the convex of $\cH$ preserves the Rademacher complexity: $R_n(\cH) = R_n(\textup{convex-hull}(\cH))$.

Proof: Notice that the supreme over the convex hull must be attained at a vertex of the convex hull.
This property is quite useful if we want to calculate the Rademacher complexity of a polytope. Though it is an infinite set, it suffices to examine the vertices of the polytope. We'll use this property when we examine the $\ell_1$-regularization.

\subsubsection{Wrapping up the proof of \eqref{eq_rade_bound} (Lecture 5)}



\subsection{Examples of Rademacher complexity (Lecture 5)}

So far, we have set up Rademacher complexity for bounding the complexity of an infinite hypothesis class.
Now, let's apply Rademacher complexity for the case when the function class is finite.
Recall that we have already derived a nice generalization bound for finite hypothesis classes, using concentration inequalities (see result \eqref{eq_finite_real}).
Still, this is a nice sanity check through a different route. Besides, we'll derive a useful result that will be helpful later on.


\subsubsection{Learning finite hypothesis classes}

\paragraph{Massart's finite lemma:} Let $\cH$ be a set of functions/hypotheses.
Let $M^2$ be a bound on the second moment of functions of $\cH$:
\[ \sup_{f \in \cH} \frac 1 n \sum_{i = 1}^n (f(Z_i))^2 \le M^2, \]
where $Z_i = (x_i, y_i)$ denotes an input data sample.
Then, the empirical Rademacher complexity is bounded by:
\begin{align}
    \hat{R}_n(F) \le \sqrt{ \frac {2M^2 \log(\abs{\cH})} n } \label{eq_emp_massart}
\end{align}
Notice that if we combine the above result \eqref{eq_emp_massart} with the Rademacher complexity-based generalization bound of equation \eqref{eq_rade_bound}, we recover the result in equation \eqref{eq_finite_real}.

\paragraph{Proof:} For simplicity, let us denote $W_f = \frac 1 n \sum_{i=1}^n \sigma_i f(Z_i)$.
Recall that the empirical Rademacher complexity is defined as
\[ \hat R_n(F) = \exarg{\sigma_{1:n}} {\sup_{f \in \cH} W_f \mid Z_{1:n} } \]
Let us write down the moment generating function of $\sup_{f\in\cH} W_f$, and we'll use the convexity of the exponential function
\begin{align*}
    \exp\left(t \cdot \ex{\sup_{f \in \cH} W_f \mid Z_{1:n}} \right)
    &\le \ex{\exp\left(t \cdot \sup_{f\in \cH} W_f \right) \mid Z_{1:n }} \\
    &= \ex{\sup_{f\in\cH} \exp\left(t \cdot W_f \right) \mid Z_{1:n}}
\end{align*}
The above can be further bounded by
\begin{align*}
    \bigabs{\cH}\cdot \ex{\exp\left(t \cdot W_f \right) \mid Z_{1:n}}
\end{align*}
Finally, recall that $W_f$ is the sum of $n$ independent random variables.
Hence, we invoke the product property of the individual MGFs, to get that $M_{W_f}(t)$ is the product of the individual random variables.
Recall that we've conditioned on $Z_{1:n}$, which can be treated as constants.
Thus, the randomness is now just on $\sigma_{1:n}$---each $\sigma_i$ is either $+1$ or $-1$, scaled by $f(Z_i)$ (which is a constant conditioned on $Z_i$).
We can show that $\sigma_i$ is sub-Gaussian with parameter $1$, since we have the fact that for any random variable $X$ bounded between $a$ and $b$ (for some $a < b$), then $X$ must be $(b-a)^2/4$ sub-Gaussian.\footnote{For a proof see this blog post: \url{https://statisticaloddsandends.wordpress.com/2018/10/05/bounded-random-variables-are-sub-gaussian/}}
%\begin{align*}
%    M_{\sigma_i}(t) = \ex{\exp(t \sigma_i)} = \frac 1 2 \left( \exp(t) + \exp(-t) \right)
%\end{align*}

As a result, $W_f$ is sub-Gaussian with parameter at most $\frac 1 {n^2} \sum_{i=1}^n (f(Z_i))^2 \le M^2 / n$.
Therefore, we now get that
\[ \ex{\exp(t \cdot W_f) \mid Z_{1:n}} \le \exp\left(\frac{t \cdot M^2} {2n}\right) \]
We can now conclude that
\[ \exp(t \hat R_n(f)) \le \bigabs{\cH} \exp\left( \frac{t M^2} {2n} \right) \]
Taking log on both sides and divide by $t$, we get:
\[ \hat R_n(\cH) \le \frac{\log(\bigabs{\cH})} t + \frac{ t M^2} {2n} \]
By setting $t$ to minimize the right-hand side above, we conclude that $\hat R_n(\cH) \le \sqrt{\frac {2\log(\bigabs{\cH}) M^2} {n}}$.

We can apply the above result along with \eqref{eq_rade_bound} to derive the result for learning from a bounded set of $0$-$1$ valued functions.

\subsubsection{Generalization bounds for $\ell_2$-norm/$\ell_1$-norm constrained linear models}

The next two examples involve learning a linear model from a norm-bounded hypothesis space.
The first example applies to Ridge regression, for instance, training models with weight decay.
The second example applies to LASSO regression, i.e., training with $\ell_1$-regularization instead.
Next, we'll derive the Rademacher complexity of these two examples.

\paragraph{Rademacher complexity of $\ell_2$ balls:}
Let $\cH = \set{z \rightarrow \inner{w}{z} \mid w \in \cW}$ be a set of linear functions such that $\bignorm{w} \le B$ for any $w \in \cW$.
Furthermore, assume the data distribution has a bounded second moment:
\[ \exarg{z\sim\bP}{\bignorm{z}^2 } \le C^2 \]
Then we have
\[ R_n(\cH) = \ex{\sup_{h\in\cH} \frac 1 n \sum_{i=1}^n h(z_i)} \le \frac {B \cdot C} {\sqrt n} \]

\paragraph{Proof:} The key idea is to exploit the linear algebraic structure of the Rademacher complexity
\begin{align*}
    R_n(\cH) = \ex{\sup_{h\in\cH} \frac 1 n \sum_{i=1}^n \sigma_i h(z_i)}
    &= \ex{\sup_{w\in\cW} \frac 1 n\sum_{i=1}^n \sigma_i \inner{w}{z_i}} \\
    &\le \ex{\sup_{w \in \cW} \bignorm{\frac {w} n} \cdot \bignorm{\sum_{i=1}^n \sigma_i z_i}} \\
    &\le \frac{ B } n \cdot \ex{\bignorm{ \sum_{i=1}^n \sigma_i z_i}}
\end{align*}
by the condition placed on $\cW$.
Finally, by Jensen's inequality, the above is at most
\begin{align*}
    \frac B n \cdot \sqrt{\ex{\bignorm{\sum_{i=1}^n \sigma_i z_i}^2 }}
    = \frac B n \cdot \sqrt{\sum_{i=1}^n \ex{\bignorm{z_i}^2}}
    \le \frac {B \cdot C} {\sqrt n}
\end{align*}

\paragraph{Rademacher complexity of $\ell_1$ balls:}
In some applications, we have a finite but large set of features, and we believe that only a small subset of them are relevant to our task.
For such applications, $\ell_1$-regularization has proven to be a working strategy (e.g., pruning, compression).
Assume that the entries of the input vector are bounded from above by some constant $C > 0$: $\max_{j=1}^d z_i[j] \le C$, for $i = 1, 2, \dots, n$.
Let $\cH = \set{z \rightarrow \inner{w}{z} \mid \bignorm{w}_1 \le B}$ be a set of linear functions.
Then, the Rademacher complexity of $\cH$ is bounded as follows
\begin{align*}
    R_n(\cH) \le \frac{ B \cdot C \cdot \sqrt{2\log(2d)}} {\sqrt n}
\end{align*}

\paragraph{Proof:} The key idea is that the $\ell_1$ ball is the convex hull of $2d$ weight vectors aligned with the basis
\[ W = \set{Be_1, -Be_1, Be_2, -Be_2, \dots, Be_d, -Be_d} \]
Since the Rademacher complexity of this class is equal to the Rademacher complexity of its convex hull, we just look at the finite class $W$ since
\[ R_n(\cH) = \ex{\sup_{w\in W} \frac 1 n \sum_{i=1}^n \sigma_i \inner{w}{z_i}} \]
Since $W$ is a finite set, we can apply Massart's finite lemma (see \eqref{eq_emp_massart}).
In order to do so, we shall first check the moment condition:
\[ \frac 1 n \sum_{i=1}^n (\sigma_i \inner{w}{z_i})^2 \le \bignorm{w}_1^2 \cdot \bignorm{z}_{\infty}^2 = B^2 \cdot C^2 \]
Thus, by result \eqref{eq_emp_massart}, we now get that
\[ \hat R_n(\cH) \le \sqrt{\frac {2B^2 C^2 \log(2d)} n} \]

\subsubsection{Binary classification}

In the next example, we study the case of binary classification.
For an input with feature $x$ and label $y$, we focus on the set of linear predictors with bounded $\ell_2$-norm: $\cW = \set{w: \bignorm{w} \le B}$.
Next, we select a loss function for our linear predictor.
The output of the linear predictor is given by $y \cdot \inner{w}{x}$.
\begin{itemize}
    \item Zero-one loss: $\ell(m) = \mathbbm{1}_{m \le 0}$

    \item Hinge loss: $\ell(m) = \max(0, 1 - m)$

    \item Logistic loss: $\ell(m) =  {\log(1 + \exp(-m)) }$ %\frac {\log(2)}
\end{itemize}
The function class is $\cH = \set{z \rightarrow \inner{z}{w} \mid w \in\cW}$.
The loss function composed with $\cH$ is thus
\[ \cA = \set{ (x, y) \rightarrow \ell(y \inner{w}{x}) \mid w \in \cW} \]
\begin{itemize}
    \item The linear predictors are constrained inside an $\ell_2$-norm bounded ball

    \item Both the Hinge loss and the logistic loss are Lipschitz-continuous
    
    \item Using the composition property of Rademacher complexity (see Section \ref{sec_property_rade}), we get a generalization bound for solving binary classification using Hinge/logistic loss
\end{itemize}
The zero-one loss, however, is not Lipschitz, so we cannot directly apply our result to the zero-one loss.
Notice that the zero-one loss is not sensitive to the norm of $w$. Think of two points close to zero, one positive and the other negative.
To handle this situation, we can use the margin loss.

\paragraph{Margin loss:} Penalizes whenever we don’t predict correctly by at least a margin of $\gamma$
\[ \phi_{\gamma}(m) = \mathbbm{1}_{m \le \gamma} \]

Then, we smooth the margin loss between $0$ and $\gamma$ into a straight line, leading to the following somewhat complicated form
\begin{align*}
    \tilde\phi_{\gamma}(m) %= \min\left(1, \max\left(0, 1 - \frac m {\gamma}\right)\right)
    = \begin{cases}
        1 & \text{ if } m \le 0 \\
        1 - \frac{m}{\gamma} & \text{ if } 0 < m < \gamma \\
        0 & \text{ if } m \ge \gamma
    \end{cases}
\end{align*}

Key observation: $\tilde\phi_{\gamma}$ is $\gamma^{-1}$-Lipschitz continuous (meaning that $\bigabs{\tilde\phi_{\gamma}(m_1) - \tilde\phi_{\gamma}(m_2) } \le \frac {\bigabs{m_1 - m_2}} {\gamma}$).

\paragraph{Margin-sensitive zero-one loss for linear classifiers:}
Let $\cH$ be a set of linear models.
With probability at least $1  - \delta$,
\begin{align*}
    L_0(\hat h_{\erm}) \le \min_{h\in\cH} L_{\gamma}(h) + \frac {4 R_n (\cH)} {\gamma} + \sqrt{\frac {2 \log(2\delta^{-1})} n}
\end{align*}
Proof sketch:
\begin{itemize}
    \item Using composition of $R_n$, we have: $R_n(\tilde \phi_{\gamma} \circ \cH) \le \gamma^{-1} R_n(\cH)$
    
    \item Using Rademacher complexity applied to $\tilde L_{\gamma}$, we get the relation between $\tilde L_{\gamma}(\hat h_{\erm})$ and $\min_{h\in\cH}\tilde L_{\gamma}(h)$

    \item Notice that $\phi_0 \le \tilde \phi_{\gamma} \le \phi_{\gamma}$.
    Thus, $\min_{h\in\cH} \tilde L_{\gamma} \le \min_{h\in\cH} \min_{h\in\cH} L_{\gamma}$,
    and $L_0(\hat h_{\erm}) \le \tilde L_{\gamma}(\hat h_{\erm})$.
\end{itemize}

\paragraph{Next lecture:} wrap up the proof of Rademacher complexity-based generalization bounds.
Apply it to matrix completion and two-layer neural networks. {\bf Suggested readings:} Chapter 3.9, 3.12 of \cite{liang2016cs229t}.

%\subsubsection{More about concentration estimates}