\section{Statistical modeling of representation learning}

In many modern ML settings, one would like to use a large, pretrained model, to solve a downstream task.
Instead of training from scratch, these models provide a representation of the input that are then used to make predictions on the input data.
While this paradigm has been quite useful in practice, practitioners have also observed problematic outcomes.
For instance, negative transfer refers to a situation where transfer learning from a source task to predict a target task negatively hurts the prediction performance, as compared to learning using the target task data alone.
However, exactly how and why negative transfer occurs has not been fully resolved --- 
Next, we provide a rigorous study of this phenomenon in the setting of transfer learning linear regression.

\subsection{Transfer learning linear regression}

We first introduce the problem setup.
Suppose there are $n_1$ samples drawn from a $p$-dimensional distribution $D_1$ with real-valued labels in a \emph{source} task.
Suppose there are $n_2$ samples drawn from a $p$-dimensional distribution $D_2$ with real-valued labels in the \emph{target} task.
Let $x_1^{(i)}, x_2^{(i)}, \dots, x_{n_i}^{(i)}$ denote feature covariates.
Let $y_1^{(i)}, y_2^{(i)}, \dots, y_{n_i}^{(i)}$ be the prediction labels.
%Recall that $n_1$ and $n_2$ refer to each task's sample size.
We assume a linear model specified by an unknown parameter vector $\beta^{(i)} \in \real^p$:
\begin{align*}
    y_j^{(i)} = \big(x_j^{(i)}\big)^{\top} {\beta^{(i)}} + \varepsilon_j^{(i)}, ~ \text{ for any } \  j = 1,\dots,n_i, \label{eq_linear}
\end{align*}
where $\varepsilon^{(i)}_j\in\real$ denotes a random noise variable with mean zero and variance $\sigma^2$.
We refer to the first task as the \emph{source} task and the second task as the \emph{target} task for ease of presentation.
We assume $n_2 > p$ while $n_1$ can be either less or greater than $p$.

\paragraph{The design matrix.}
In linear regression, there is typically a distinction between fixed designs vs. random designs.
Fixed designs refer to a situation where the covariates are deterministic, e.g., think of the population demographic data for one example.
By contrast, random designs correspond to cases where the covariates may involve observational uncertainties.
For this section, we shall study random designs where the covariates are given by
\[ x_j^{(i)} = \big(\Sigma^{(i)}\big)^{\frac 1 2} z_{j}^{(i)}, \text{ for } i\in\{1,2\} \text{ and for any } j = 1,\dots,n_i, \]
where $z_j^{(i)}$ consists of independent and identically distributed entries of mean zero and variance one, and $\Sigma^{(1)} \in \real^{p \times p}$ and $\Sigma^{(2)} \in \real^{p \times p}$ denote the population covariance matrices.
Let $X^{(i)}=(x_1^{(i)},\ldots,x_{n_i}^{(i)})^\top \in\real^{n_i \times p}$ denote a matrix that corresponds to all the covariates of task $i$, for $i \in \{1, 2\}$.

Let $Y^{(1)}\in \real^{n_1}$, $Y^{(2)}\in \real^{n_2}$ be the vectors of labels.
Then, consider the following objective, parameterized by a shared variable $B \in \real^{p}$:
\begin{align}\label{eq_hps}
    \ell(B) = \bignorm{X^{(1)} B - Y^{(1)}}^2 + \bignorm{X^{(2)} B - Y^{(2)}}^2.
\end{align}
Notice that $B$ provides a shared feature vector for both tasks.
This way of estimation is consistent with the hard parameter sharing (HPS) architecture that has been used in the earlier literature \citep{caruana1997multitask}.
Thus, we shall call the minimizer of $\ell(B)$ the HPS estimator, denoted as $\hat\beta_2^{\MTL}$ for short.

\paragraph{Bias and variance:}
The risk of $\hat{\beta}_i^{\MTL}$ over an unseen sample $(x,y)$ of the target task $i$ is
%$y= x^\top \beta^{(2)} + \ve$.
%The expected prediction risk is
given by (under the mean squared loss):
$$\exarg{x, y}{\left\|y-x^\top \hat{\beta}_i^{\MTL} \right\|^2}= \left\|{\Sigma^{(i)}}^{\frac 1 2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2 + \sigma^2.$$
The excess risk is the difference between the above risk and the expected risk of the population risk optimizer:
\begin{align}\label{HPS_loss}
    L(\hat{\beta}_i^{\MTL}) := \left\| {\Sigma^{(i)}}^{\frac 1 2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2.
\end{align}
%We shall treat task two as the target task and treat task one as the source task.

Next, we present a bias-variance decomposition of the excess risk of HPS.
Denote the sum of both tasks' sample covariance matrix as:
\begin{align}\label{Sigma_a}
    \hat{\Sigma} = {X^{(1)}}^{\top} X^{(1)} + {X^{(2)}}^{\top} X^{(2)}.
\end{align}
The bias and variance formulas are defined as %
\begin{align}
    L_{\bias}  &= \left\| {\Sigma^{(2)}}^{\frac 1 2}\hat \Sigma^{-1} \bigbrace{{X^{(1)}}^\top X^{(1)}} \big(\beta^{(1)}- \beta^{(2)}\big) \right\|^2,  \label{Lbias} \\
    L_{\vari}   &= \sigma^2  \tr\big[{\Sigma^{(2)}\hat \Sigma^{-1}  }\big].  \label{Lvar}
\end{align}

\paragraph{Proof:} The proof of the above is based on algebraic calculations.
By minimizing $\ell(B)$ in \eqref{HPS_loss}, we obtain the minimizer as
\[ \hat\beta^{\MTL}_2 = \left( (X^{(1)})^{\top} X^{(1)} + (X^{(2)})^{\top} X^{(2)} \right)^{-1} 
\left( (X^{(1)})^{\top} Y^{(1)} + (X^{(2)})^{\top} Y^{(2)} \right) \]
By simplifying the above a little bit, we get
\begin{align*}
    \hat\beta^{\MTL}_2 = \beta^{(2)} &+ \left( (X^{(1)})^{\top} X^{(1)} 
    + (X^{(2)})^{\top} X^{(2)} \right)^{-1} \left( (X^{(1)})^{\top} \varepsilon^{(1)} + (X^{(2)})^{\top} \varepsilon^{(2)} \right) \\
    &+ \left( (X^{(1)})^{\top} X^{(1)} + (X^{(2)})^{\top} X^{(2)} \right)^{-1} (X^{(1)})^{\top} X^{(1)} (\beta^{(1)} - \beta^{(2)})
\end{align*}
Plugging this into the excess risk, we get that in expectation over the randomness of $\varepsilon^{(1)}$ and $\varepsilon^{(2)}$,
\[ \ex{L(\hat{\beta}^{\MTL}_2)} = L_{\bias} + L_{\vari} \]

\paragraph{Implications:}
We can now compare the above bias-variance to that of single-task learning on the target task.
That would just be \[ \sigma^{2} \tr\big[\Sigma^{(2)} ( (X^{(2)})^{\top} X^{(2)} )^{-1} \big], \]
which is the variance of the ordinary least squares (OLS) estimator for the target task.
The bias and variance decomposition above allows us to reason about transfer effects by comparing the bias and variance of HPS with that of OLS.
The bias of HPS is always larger than that of OLS.
On the other hand, the variance of HPS is always lower than that of OLS.
By Woodbury matrix identity, the variance of OLS is always higher than formula \eqref{Lvar}:
%\[ \sigma^2 \bigtr{\Sigma^{(2)} ((X^{(2)})^{\top} X^{(2)})^{-1}} \] 
\begin{align}
    \bigtr{\Sigma^{(2)} \big({X^{(2)}}^{\top} X^{(2)}\big)^{-1}}
    \ge
    \bigtr{\Sigma^{(2)} {\hat{\Sigma}}^{-1}}. \label{eq_HPS_better}
\end{align}
Thus, the transfer effect of HPS is determined by the bias-variance decomposition: the bias always increases while the variance always decreases.
Whether or not the transfer effect of HPS is positive depends on which effect dominates.


\paragraph{Numerical example:}
We will consider a random-effects model.
Each $\beta^{(i)}$ consists of two components, in this case, one shared by all tasks and one that is task-specific.
Let $\beta_0$ be the shared component and $\gamma_{i}$ be the $i$-th task-specific component.
For any $i$, the $i$-th model vector is equal to
%\begin{align}
    $\beta^{(i)}=\beta_0 + \gamma_{i}.$ % \label{eq_re}
%\end{align}
The entries of the task-specific component $\gamma_i$ are drawn independently from a Gaussian distribution with mean zero and variance $p^{-1}\mu^2$, for a parameter $\mu > 0$.
In expectation, the Euclidean distance between the two model vectors is equal to $2\mu^2$.

Then, we have the following phase transition in the random-effects model.
  %Suppose Assumption \ref{assm_big1} and condition \eqref{para_rel} hold.  
%    Under the assumptions of \Cref{cor_MTL_loss}, suppose the random-effect model applies and $n_2 \ge 3p$. %Let $a= {p}^{-1}\bigtr{\Sigma^{(1)}}$.
With high probability over the randomness of training samples and model vectors.
\begin{enumerate}%[leftmargin=0.3in]
    \item If $ {\mu^2} \frac {\bigtr{\Sigma^{(1)}}}{p} \le \frac{\sigma^2 p}{2(n_2 - p)}$, then the transfer effect is always positive:
        \begin{align}\label{HPS_le_OLS}
        L(\hat{\beta}_2^{\MTL}) \le L(\hat{\beta}_2^{\STL}) . %+\OO\left(p^{-C}\|\beta_0\|^2 \right).
        \end{align}
    \item If $\frac{\sigma^2 p}{2(n_2 - p)} < {\mu^2} \frac{\bigtr{\Sigma^{(1)}}}{p} < \frac{\sigma^2 n_2}{2(n_2 - p)}$, then there exists a deterministic value $n_0>0$ (which may not be an integer) such that if $n_1  \le n_0$, then equation \eqref{HPS_le_OLS} holds;
        otherwise if $n_1  > n_0$, then
        \begin{align}\label{HPS_ge_OLS}
        L(\hat{\beta}_2^{\STL}) \le L(\hat{\beta}_2^{\MTL}). % +\OO\left(p^{-C}\|\beta_0\|^2 \right).
        \end{align}
    \item If ${\mu^2} \frac{\bigtr{\Sigma^{(1)}}}{p} \ge \frac{\sigma^2 n_2} {2(n_2 - p)}$, then equation \eqref{HPS_ge_OLS} holds for any $n_1 > p$. %(cf. condition \ref{assm2}).
\end{enumerate}
Figure \ref{fig_motivation} demonstrates an illustration of this tradeoff.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/motivation.eps}
    \caption{We illustrate the performance of HPS vs. OLS (ordinary least squares) for different levels of model shifts, indicated by $\mu \approx \bignorm{\beta^{(1)} - \beta^{(2)}} / \sqrt{2}$. We consider two linear regression tasks and vary the sample size of the source task, denoted as $n_1$, and $\mu$, while holding the target task as fixed.
    %Here $\mu$ is given by $^2 \approx 2\mu^2$.
    The gray line refers to the performance of OLS.
    Any point above the gray line represents negative transfer, while any point below represents positive transfer.
    %The estimator is defined by solving equation \eqref{eq_hps}.
    In this simulation, we set $p = 100$, $n_2 = 300$, and $\sigma^2 = \frac 1 4$.
    {We sample the covariates $X$ from a $p$-dimensional isotropic Gaussian and
    plot the excess risk of HPS (see equation \eqref{HPS_loss}).}}
    \label{fig_motivation}
\end{figure*}
%For further details about the setup, see Proposition \ref{claim_model_shift}, Section \ref{sec_sizeratio}.

\subsubsection{Minimax lower bounds}

The above estimator seems like a natural choice in practice.
However, how would we know if there won't be another estimator that is much better the above?
Concretely, suppose we are trying to estimate an unknown parameter denoted as $\beta^{(2)}$.
We are given $n_2$ samples drawn from a linear parametric model, following $\beta^{(2)}$, with isotropic Gaussian covariates $X^{(2)}$, contaminated by Gaussian noise with variance $\sigma^2$.
Then, we are given $n_1$ samples from another linear parametric model, following $\beta^{(1)}$, again with isotropic Gaussian covariates $X^{(1)}$, contaminated by Gaussian noise with variance $\sigma^2$.
The parameter vectors belong to the set \[\Theta(\mu) = \set{\beta^{(1)} \in \real^p, \beta^{(2)} \in \real^p: \|{\beta^{(1)} - \beta^{(2)}}\| \le \mu, \|{\beta^{(1)}}\| \le 1, \|{\beta^{(2)}}\| \le 1}. \]

Let $\hat\beta$ be any estimation procedure that, given the above $n_1 + n_2$ samples, produces an estimate of the unknown vector $\beta^{(2)}$.
We prove the following minimax rate on the estimation error of $\hat\beta$.
%    In the setting described above, let $\hat\beta$ be a fixed estimation procedure.
Assume that $n_1 \ge (1+\tau) p$ and $n_2 \ge (1+\tau) p$ for a constant $\tau>0$.
For any $(\beta^{(1)}, \beta^{(2)})$ within the set $\Theta(\mu)$, we have that
%with high probability over the randomness of $(X^{(1)}, X^{(2)})$, 
\begin{align}
    \inf_{\hat\beta} \sup_{\Theta(\mu)} \ex{\frac 1 {n_2}\bignorm{\wt X^{(2)}\left(\hat\beta - \beta^{(2)}\right)}^2} \ge %\frac{c^2}{256}
    c{\Big({\min\Big(\frac{n_1^2\mu^2}{(n_1+n_2)^2}, \frac{\sigma^2 p} {n_2} \Big) + \frac{\sigma^2 p} {n_1 + n_2}}\Big)}, \label{eq_lb}
\end{align}
where the expectation is over the randomness of $X^{(1)}, X^{(2)}, Y^{(1)}, Y^{(2)}$ and an independently drawn $\wt X^{(2)}$ that follows the same distribution as $X^{(2)}$.
$c$ is a fixed constant that does not grow with $p$.

The lower bound in the right-hand side of equation \eqref{eq_lb} involves two parts:
\begin{itemize}
    \item For the first part, if $\mu$ is large, the source samples are not helpful, so the rate $\frac{\sigma^2 p}{n_2}$ is the OLS rate using only the target task samples. If $\mu$ is small and $n_1$ is large, then the rate $\mu^2$ appears if we use the OLS estimator for the source task.

    \item For the second part, $\frac{\sigma^2 p} {n_1 + n_2}$ is the rate in the case without model drift, i.e., $\beta^{(1)}=\beta^{(2)}$.
\end{itemize}

\paragraph{Takeaway:} It turns out that the naive estimator described above is nearly optimal, with one caveat.
When $\mu$ is very high, then we essentially throw away the source task data.
Otherwise when $\mu$ is very small, then we combine source and target task data as HPS.
It can be shown that this adjusted HPS estimator now matches the minimax lower bound.

\paragraph{Proof sketch:} The proof of the minimax lower bound is via a covering argument.
In particular, we design a cover of the parameter vectors, taking into account the effect of model shifts \citep{wainwright2019high}.

\subsubsection{Extension to multiple tasks}

Suppose there are $t$ tasks whose feature covariates are all equal to $X \in \real^{n \times p}$. The label vector of the $i$-th task follows a linear model with an unknown $p$ dimensional vector $\beta^{(i)}$, for $i=1, 2,\dots, t$:
\begin{align}\label{eq_mtl_data}
    Y^{(i)} = X \beta^{(i)} + \varepsilon^{(i)}.
\end{align}
We assume that $X = Z\Sigma^{\frac 1 2} \in \real^{n \times p}$ is a random matrix.

We combine the samples from all the tasks with a shared feature matrix $B$ and a task-specific prediction vector for each task.
Specifically, let $B \in \real^{p \times r}$ denote the shared feature matrix and let $A = [A_1, A_2, \dots, A_t] \in \real^{r \times t}$ denote the combined prediction variables.
We remark that unlike the two-task setting concerning covariate shifts, we have both $A$ and $B$ as part of the model in the multi-task setting. The reason is two-fold.
First, in this paper, we focus on the underparameterized setting. Thus, in the two-task setting, one can show that the optimal rank of $B$ would be one, i.e., $r = 1$, in which case $B$ becomes a scalar. 
Second, to tackle more than two tasks, we need to incorporate more of the structural information from the other tasks into the model \citep{AZ05}, which is encoded as the low-rank structures of $A$ and $B$. By contrast, in the two-task setting, we can focus on the setting where we have a shared parameter vector for both tasks.

We can now write down the loss objective as follows:
\begin{align}
	\ell(A, B) = \sum_{j=1}^t \bignorm{X B A_j - Y^{(j)}}^2, \label{eq_mtl_same_cov}
\end{align}
In particular, we focus on the case where $r < t$. Otherwise, if $r \ge t$, the problem reduces to single-task learning (for reference, see Proposition 1 from \citep{wu2020understanding}).

Let $(\hat A, \hat B)$ denote the global minimizer of $\ell(A, B)$.
{In particular, we compute the global minimizer of $\ell(A, B)$ by first solving $\hat B$ as a function of $\hat A$---this turns out to be $(X^{\top}X)^{-1} X^{\top}Y$ multiplied by  $A^{\top} (A A^{\top})^+$, with $(AA^{\top})^{+}$ denoting the pseudoinverse of $AA^{\top}$. 
Next, we could find the optimal $\hat A$ by taking the rank-$r$ SVD of $Y^{\top} X (X^{\top}X)^{-1} X^{\top}Y$ to get the leading $r$ left singular vectors as $U_r \in\real^{t\times r}$.
Then, we can obtain $\hat B \hat A$ as $(X^{\top} X)^{-1} X^{\top} Y U_r U_r^{\top}$.}

\paragraph{Deriving the HPS estimator:} For the optimization objective $\ell(A, B)$ in equation \eqref{eq_mtl_same_cov}, using the local optimality condition $\frac {\partial f} {\partial B} = 0$, we can obtain $\hat{B}$ as a function of $A$:
	\begin{align}
		\hat{B}(A) %
		&= (X^{\top} X)^{-1} X^{\top} Y A^{\top} (AA^{\top})^{+}, \label{eq_Bhat}
	\end{align}
where $Y := [Y^{(1)}, Y^{(2)}, \dots, Y^{(t)}]$ and $(AA^{\top})^{+}$ denotes the pseudoinverse of $AA^{\top}$.
	Plugging $\hat{B}(A)$ into equation \eqref{eq_mtl_same_cov}, we obtain the following objective that depends only on $A$ (in matrix notation):
	\begin{align}\label{eq_mtl_output_layer}
		g(A) = \bignormFro{X (X^{\top}X)^{-1}X^{\top} Y A^{\top} (AA^{\top})^{+} A - Y}^2.
	\end{align}
Note that $A^{\top} (AA^{\top})^{+} A$ is a projection onto the subspace spanned by the rows of $A$. For simplicity, we write it into the form
	$$A^{\top} (AA^{\top})^{+} A= U_A U_A^\top,$$
where $U_A \in \real^{t\times r}$ is a $t\times r$ partial orthonormal matrix (i.e. $U_A^\top U_A=\id_{r\times r}$). Hence, we also denote the function $g(A)$ by $g(U_A)$.
    
Now, to find the optimal solution for $U_A$, we write $X (X^{\top} X)^{-1} X^{\top} = P_X$ (which is an $n$ by $n$ projection matrix of rank $p$), and expand $g(A)$ as:
    \begin{align*}
        g(A) = \bignormFro{P_X Y U_A U_A^{\top} - Y}^2
        &= \bignormFro{P_X Y U_A U_A^{\top}}^2 + \bignormFro{Y}^2 - 2\inner{P_X Y U_A U_A^{\top}}{Y} \\
        & = \bignormFro{Y}^2 - \inner{P_X Y U_A U_A^{\top}}{Y} = \bignormFro{Y}^2 - \inner{U_A U_A^{\top}}{Y^{\top} P_X Y},
    \end{align*}
where we use the fact that $P_X^2 = P_X$ since it is a projection matrix and $U_A^{\top} U_A = \id_{r\times r}$.
Thus, to minimize $g(A)$, we just need to maximize the inner project between $Y^{\top} P_X Y$ and $U_A U_A^{\top}$---this can be achieved by finding the best rank-$r$ SVD of $Y^{\top} P_X Y$ and taking the top-$r$ singular vectors, which form the partial orthonormal matrix $U_r = U_{\hat A}$.
Lastly, we can insert $U_r$ as $A$ back to $\hat B A$ to obtain $\hat B \hat A$ as the HPS estimators for all tasks from $1$ to $t$.

We illustrate the behavior of the HPS estimator in the multitask setting as follows.
\begin{figure}[!t]
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/multitask_width.eps}
		\caption{Varying the rank of the shared matrix $B$}
		\label{fig_sec4_width}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/multitask_transfer.eps}
		\caption{Varying model shift $\mu$}
		\label{fig_sec4_transfer}
	\end{subfigure}
	\caption{Illustration of transfer effects in the random-effects model with multiple source tasks.
    In particular, $\mu$ indicates the extent of model shift --- higher values of $\mu$ means greater differences between model vectors across tasks.
	Figure \ref{fig_sec4_width} shows that for rank $r$ from $1$ to $10$, the lowest excess risk of task $t$ is achieved at $r = 1$ (averaged over three random seeds).
%	This simulation uses $\mu = 0.05$ and is .
	Figure \ref{fig_sec4_transfer} fixes $r = 1$ and varies $\mu, n$.
    Similar to Figure \ref{fig_motivation}, the transfer effect of HPS can be either positive or negative depending on $\mu, n$.}
	\label{fig_sec4}
\end{figure}

\paragraph{Next lecture:} Start talking about reinforcement learning.
{\bf Suggested readings:} \cite{wu2020understanding}.
