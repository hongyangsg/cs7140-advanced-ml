\section{Overview (Lecture 1)}

\subsection{What is this course about?}

Machine learning has been increased used in technology platforms and products, affecting our daily lives.\footnote{ChatGPT reportedly has 300 million weekly active users: \href{https://www.cnbc.com/2024/12/04/openais-active-user-count-soars-to-300-million-people-per-week.html}{CNCB news, 2024}; Claude reportedly has 4.5 million monthly active users, \href{https://www.anthropic.com/customers/wrtn}{anthropic}.}
Machine learning involves a collection of models, algorithms, and engineering frameworks:
\begin{itemize}
    \item Regression and classification: least squares estimation, logistic regression, LDA, bias-variance tradeoff, cross-validation.
    \item Neural networks and deep learning: CNNs, backpropagation, foundation models, language modeling.
    \item Unsupervised learning: dimension reduction (e.g., PCA), clustering, contrastive learning.
    \item Causal machine learning: study the cause-and-effect with a powerful machine learning model.
    \item Generative AI: diffusion models, multi-modal learning.
    \item NumPy, Sklearn, PyTorch, TensorFlow, Hugging Face.
\end{itemize}

This course aims to uncover the common \hl{statistical principles} underlying the diverse array of methods.
This class is mostly about the theoretical analysis of learning algorithms and models. Many of the techniques introduced in this course---which involve a beautiful blend of probability, linear algebra, and optimization---are separate fields in their respective discipline with independent interests outside of machine learning.
For example, we will study the supreme of a complex random variable corresponding to the outcome of a learning algorithm applied to train a neural network model.
We will show how to design estimation algorithms when we are working under distribution shifts between training and test datasets.

From a practical point of view, studying the underlying working mechanisms of a learning algorithm can deepen our understanding of how things work.
For example, suppose we want to build a classifier to predict the topic of a document (e.g., sports, politics, technology, etc).
We train a logistic regression model with word frequencies as features and obtain a training accuracy of 90\% on 1000 training documents and a test accuracy of 85\% on 1000 test documents.
\begin{itemize}
    \item How reliable are these numbers? If we resample the training data, can we expect the same results?
    \item How much will the training and test accuracies increase if we double the number of training documents?
    \item What if we increase the number of features (e.g., use tri-occurrence of words)? Does regularization help?
\end{itemize}

There is obviously a clear gap between theoretical analysis and the practical performance of an algorithm.
For instance, theoretical analysis is usually conducted under strong assumptions, which limit the implications one could draw from the theoretical results.
The goal, instead, is to build a deeper understanding through theoretical analysis.

The course materials are divided into three parts: \emph{fundamental concepts of statistical learning} (January), \emph{generalization of neural networks and deep learning} (February), \emph{statistical modeling of representation learning, reinforcement learning, and beyond} (March).\footnote{April will be dedicated to course project presentations.}

\subsection{Basic setup of supervised learning}

Central questions: \emph{Does minimizing training error lead to low test error?}
\emph{How does the generalization ability depend on the model architecture and the training algorithm?}
It turns out that answering these questions is highly non-trivial as it also depends on the underlying data distribution.\footnote{A recent empirical study highlights empirical scaling laws as key metrics for training large language models: \href{https://arxiv.org/abs/2001.08361}{paper} (see also \href{https://openai.com/index/scaling-laws-for-neural-language-models/}{openai} page).}

To formally study these questions, let us first describe the mathematical setup:
\begin{itemize}
    \item Let $\cX$ denote the feature space. Let $\cY$ denote the space of all possible outcomes. Binary classification example: $\cX=\real^d$, $\cY=\set{+1,-1}$
    
    \item Consider the problem of predicting an output $y \in \cY$ given an input $x\in\cX$.

    \item Let $\cH$ be a set of hypotheses. Linear model example: \[ \cH = \set{x \rightarrow \beta^{\top} x + \eta: \forall \beta \in \real^d, \eta\in\real} \]

    \item Let $\ell: (\cX, \cY) \times \cH \rightarrow \real$ be a loss function. For example, the mean squared error (MSE) applied to linear models is
    \[ \ell((x, y), \beta) = \left(\beta^{\top}x + \eta - y\right)^2, \forall \beta\in\real^d, \forall\eta\in\real \]

    \item Given $n$ training data samples, denoted by $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$, the training loss (or empirical risk) of a hypothesis $h\in\cH$ is defined as
    \begin{align}
        \hat L(h) = \frac 1 n \sum_{i=1}^n \ell(h(x_i), y_i), \forall h \in \cH
    \end{align}
\end{itemize}

We make a critical assumption about the data-generating process. We assume that every $x_i, y_i$ pair is drawn independently and identically from an unknown distribution $\bP^{\star}$, supported on $\cX \times \cY$.

The test loss (or expected risk) of a hypothesis $h\in\cH$ is then given by
\begin{align}
    L(h) = \exarg{(x, y)\sim\bP^{\star}}{\ell(h(x), y)}.
\end{align}
\paragraph{Remarks:}
\begin{itemize}
    \item We have assumed the training and test distributions are the same. While this assumption does not hold exactly in practice, morally speaking, the training and test distributions have to be related.
    
    \item Formulating what it means to be related and not related, and dealing with the discrepancy between training and test data is studied under the area of domain adaptation or transfer learning.

    \item The independence assumption, which also does not hold exactly in practice, ensures that more training data gives us more information.
\end{itemize}

\paragraph{Empirical risk minimization:} Consider minimizing the training loss
\begin{align}
    \hat h_{\erm} \leftarrow \argmin_{h \in \cH} \hat L(h).
\end{align}
What can say that the relationship between $\hat L(\hat h_{\erm})$ and $L(\hat h_{\erm})$?
A key challenge is that the randomness of $\hat h_{\erm}$ now depends on $\hat L$. Thus, $\hat L(\hat h_{\erm})$ involves a correlation between the training data samples and the minimizing hypothesis.
A central aspect we will tackle in the first part of the course is to develop the machinery to tackle this challenge.

\paragraph{Uniform convergence:} We show provide statements of the following flavor

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    With probability at least $1-\delta$, the gap between test loss and training loss of any hypothesis is upper bounded by some small $\epsilon$, that is, $L(h) - \hat L(h) \le \epsilon$, where the $\epsilon$ is generally a function that depends on $\delta$ and other aspects of the learning algorithm/model
\end{mdframed}

More rigorously, we would like to show statements of the following:
\begin{align} 
    \Pr\left[L(h) - \hat L(h) > \epsilon\right] \le 1 - \delta,
\end{align}
where the randomness is on the training data samples drawn from $\bP^{\star}$.

Equipped with such a statement, we will then apply the statement to the empirical risk minimizer $\hat h_{\erm}$, since the result essentially holds for any $h \in \cH$, which also subsumes $\hat h_{\erm}$ as a special case.

\subsection{Basic setup of deep networks}




\subsection{Next lecture}


\paragraph{P.S.} 
\begin{itemize}
    \item Have feedback or want to ask a question for the instructor? Leave a note here: \url{https://forms.gle/hPZ8u3d7niDJf1AJ7}.
    \item Want to see the latex source code? You can find the tex files here: \url{https://github.com/hongyangsg/cs7140}.
\end{itemize}