\section{Statistical modeling of representation learning}

In many modern ML settings, one would like to use a large, pretrained model, to solve a downstream task.
Instead of training from scratch, these models provide a representation of the input that are then used to make predictions on the input data.
While this paradigm has been quite useful in practice, practitioners have also observed problematic outcomes.
For instance, negative transfer refers to a situation where transfer learning from a source task to predict a target task negatively hurts the prediction performance, as compared to learning using the target task data alone.
However, exactly how and why negative transfer occurs has not been fully resolved --- 
Next, we provide a rigorous study of this phenomenon in the setting of transfer learning linear regression.

\subsection{Transfer learning linear regression}

We first introduce the problem setup.
Suppose there are $n_1$ samples drawn from a $p$-dimensional distribution $D_1$ with real-valued labels in a \emph{source} task.
Suppose there are $n_2$ samples drawn from a $p$-dimensional distribution $D_2$ with real-valued labels in the \emph{target} task.
Let $x_1^{(i)}, x_2^{(i)}, \dots, x_{n_i}^{(i)}$ denote feature covariates.
Let $y_1^{(i)}, y_2^{(i)}, \dots, y_{n_i}^{(i)}$ be the prediction labels.
%Recall that $n_1$ and $n_2$ refer to each task's sample size.
We assume a linear model specified by an unknown parameter vector $\beta^{(i)} \in \real^p$:
\begin{align*}
    y_j^{(i)} = \big(x_j^{(i)}\big)^{\top} {\beta^{(i)}} + \varepsilon_j^{(i)}, ~ \text{ for any } \  j = 1,\dots,n_i, \label{eq_linear}
\end{align*}
where $\varepsilon^{(i)}_j\in\real$ denotes a random noise variable with mean zero and variance $\sigma^2$.
We refer to the first task as the \emph{source} task and the second task as the \emph{target} task for ease of presentation.
We assume $n_2 > p$ while $n_1$ can be either less or greater than $p$.

\paragraph{The design matrix.}
In linear regression, there is typically a distinction between fixed designs vs. random designs.
Fixed designs refer to a situation where the covariates are deterministic, e.g., think of the population demographic data for one example.
By contrast, random designs correspond to cases where the covariates may involve observational uncertainties.
For this section, we shall study random designs where the covariates are given by
\[ x_j^{(i)} = \big(\Sigma^{(i)}\big)^{\frac 1 2} z_{j}^{(i)}, \text{ for } i\in\{1,2\} \text{ and for any } j = 1,\dots,n_i, \]
where $z_j^{(i)}$ consists of independent and identically distributed entries of mean zero and variance one, and $\Sigma^{(1)} \in \real^{p \times p}$ and $\Sigma^{(2)} \in \real^{p \times p}$ denote the population covariance matrices.
Let $X^{(i)}=(x_1^{(i)},\ldots,x_{n_i}^{(i)})^\top \in\real^{n_i \times p}$ denote a matrix that corresponds to all the covariates of task $i$, for $i \in \{1, 2\}$.

Let $Y^{(1)}\in \real^{n_1}$, $Y^{(2)}\in \real^{n_2}$ be the vectors of labels.
Then, consider the following objective, parameterized by a shared variable $B \in \real^{p}$:
\begin{align}\label{eq_hps}
    \ell(B) = \bignorm{X^{(1)} B - Y^{(1)}}^2 + \bignorm{X^{(2)} B - Y^{(2)}}^2.
\end{align}
Notice that $B$ provides a shared feature vector for both tasks.
This way of estimation is consistent with the hard parameter sharing (HPS) architecture that has been used in the earlier literature \citep{caruana1997multitask}.
Thus, we shall call the minimizer of $\ell(B)$ the HPS estimator, denoted as $\hat\beta_2^{\MTL}$ for short.

\paragraph{Bias and variance:}
The risk of $\hat{\beta}_i^{\MTL}$ over an unseen sample $(x,y)$ of the target task $i$ is
%$y= x^\top \beta^{(2)} + \ve$.
%The expected prediction risk is
given by (under the mean squared loss):
$$\exarg{x, y}{\left\|y-x^\top \hat{\beta}_i^{\MTL} \right\|^2}= \left\|{\Sigma^{(i)}}^{\frac 1 2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2 + \sigma^2.$$
The excess risk is the difference between the above risk and the expected risk of the population risk optimizer:
\begin{align}\label{HPS_loss}
    L(\hat{\beta}_i^{\MTL}) := \left\| {\Sigma^{(i)}}^{\frac 1 2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2.
\end{align}
%We shall treat task two as the target task and treat task one as the source task.

Next, we present a bias-variance decomposition of the excess risk of HPS.
Denote the sum of both tasks' sample covariance matrix as:
\begin{align}\label{Sigma_a}
    \hat{\Sigma} = {X^{(1)}}^{\top} X^{(1)} + {X^{(2)}}^{\top} X^{(2)}.
\end{align}
The bias and variance formulas are defined as %
\begin{align}
    L_{\bias}  &= \left\| {\Sigma^{(2)}}^{\frac 1 2}\hat \Sigma^{-1} \bigbrace{{X^{(1)}}^\top X^{(1)}} \big(\beta^{(1)}- \beta^{(2)}\big) \right\|^2,  \label{Lbias} \\
    L_{\vari}   &= \sigma^2  \tr\big[{\Sigma^{(2)}\hat \Sigma^{-1}  }\big].  \label{Lvar}
\end{align}

\paragraph{Proof:} The proof of the above is based on algebraic calculations.
By minimizing $\ell(B)$ in \eqref{HPS_loss}, we obtain the minimizer as
\[ \hat\beta^{\MTL}_2 = \left( (X^{(1)})^{\top} X^{(1)} + (X^{(2)})^{\top} X^{(2)} \right)^{-1} 
\left( (X^{(1)})^{\top} Y^{(1)} + (X^{(2)})^{\top} Y^{(2)} \right) \]
By simplifying the above a little bit, we get
\begin{align*}
    \hat\beta^{\MTL}_2 = \beta^{(2)} &+ \left( (X^{(1)})^{\top} X^{(1)} 
    + (X^{(2)})^{\top} X^{(2)} \right)^{-1} \left( (X^{(1)})^{\top} \varepsilon^{(1)} + (X^{(2)})^{\top} \varepsilon^{(2)} \right) \\
    &+ \left( (X^{(1)})^{\top} X^{(1)} + (X^{(2)})^{\top} X^{(2)} \right)^{-1} (X^{(1)})^{\top} X^{(1)} (\beta^{(1)} - \beta^{(2)})
\end{align*}
Plugging this into the excess risk, we get that in expectation over the randomness of $\varepsilon^{(1)}$ and $\varepsilon^{(2)}$,
\[ \ex{L(\hat{\beta}^{\MTL}_2)} = L_{\bias} + L_{\vari} \]

\paragraph{Implications:}
We can now compare the above bias-variance to that of single-task learning on the target task.
That would just be \[ \sigma^{2} \tr\big[\Sigma^{(2)} ( (X^{(2)})^{\top} X^{(2)} )^{-1} \big], \]
which is the variance of the ordinary least squares (OLS) estimator for the target task.
The bias and variance decomposition above allows us to reason about transfer effects by comparing the bias and variance of HPS with that of OLS.
The bias of HPS is always larger than that of OLS.
On the other hand, the variance of HPS is always lower than that of OLS.
By Woodbury matrix identity, the variance of OLS is always higher than formula \eqref{Lvar}:
%\[ \sigma^2 \bigtr{\Sigma^{(2)} ((X^{(2)})^{\top} X^{(2)})^{-1}} \] 
\begin{align}
    \bigtr{\Sigma^{(2)} \big({X^{(2)}}^{\top} X^{(2)}\big)^{-1}}
    \ge
    \bigtr{\Sigma^{(2)} {\hat{\Sigma}}^{-1}}. \label{eq_HPS_better}
\end{align}
Thus, the transfer effect of HPS is determined by the bias-variance decomposition: the bias always increases while the variance always decreases.
Whether or not the transfer effect of HPS is positive depends on which effect dominates.


\paragraph{Numerical example:}
We will consider a random-effects model.
Each $\beta^{(i)}$ consists of two components, in this case, one shared by all tasks and one that is task-specific.
Let $\beta_0$ be the shared component and $\gamma_{i}$ be the $i$-th task-specific component.
For any $i$, the $i$-th model vector is equal to
%\begin{align}
    $\beta^{(i)}=\beta_0 + \gamma_{i}.$ % \label{eq_re}
%\end{align}
The entries of the task-specific component $\gamma_i$ are drawn independently from a Gaussian distribution with mean zero and variance $p^{-1}\mu^2$, for a parameter $\mu > 0$.
In expectation, the Euclidean distance between the two model vectors is equal to $2\mu^2$.

Then, we have the following phase transition in the random-effects model.
  %Suppose Assumption \ref{assm_big1} and condition \eqref{para_rel} hold.  
%    Under the assumptions of \Cref{cor_MTL_loss}, suppose the random-effect model applies and $n_2 \ge 3p$. %Let $a= {p}^{-1}\bigtr{\Sigma^{(1)}}$.
With high probability over the randomness of training samples and model vectors.
\begin{enumerate}%[leftmargin=0.3in]
    \item If $ {\mu^2} \frac {\bigtr{\Sigma^{(1)}}}{p} \le \frac{\sigma^2 p}{2(n_2 - p)}$, then the transfer effect is always positive:
        \begin{align}\label{HPS_le_OLS}
        L(\hat{\beta}_2^{\MTL}) \le L(\hat{\beta}_2^{\STL}) . %+\OO\left(p^{-C}\|\beta_0\|^2 \right).
        \end{align}
    \item If $\frac{\sigma^2 p}{2(n_2 - p)} < {\mu^2} \frac{\bigtr{\Sigma^{(1)}}}{p} < \frac{\sigma^2 n_2}{2(n_2 - p)}$, then there exists a deterministic value $n_0>0$ (which may not be an integer) such that if $n_1  \le n_0$, then equation \eqref{HPS_le_OLS} holds;
        otherwise if $n_1  > n_0$, then
        \begin{align}\label{HPS_ge_OLS}
        L(\hat{\beta}_2^{\STL}) \le L(\hat{\beta}_2^{\MTL}). % +\OO\left(p^{-C}\|\beta_0\|^2 \right).
        \end{align}
    \item If ${\mu^2} \frac{\bigtr{\Sigma^{(1)}}}{p} \ge \frac{\sigma^2 n_2} {2(n_2 - p)}$, then equation \eqref{HPS_ge_OLS} holds for any $n_1 > p$. %(cf. condition \ref{assm2}).
\end{enumerate}
Figure \ref{fig_motivation} demonstrates an illustration of this tradeoff.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/motivation.eps}
    \caption{We illustrate the performance of HPS vs. OLS (ordinary least squares) for different levels of model shifts, indicated by $\mu \approx \bignorm{\beta^{(1)} - \beta^{(2)}} / \sqrt{2}$. We consider two linear regression tasks and vary the sample size of the source task, denoted as $n_1$, and $\mu$, while holding the target task as fixed.
    %Here $\mu$ is given by $^2 \approx 2\mu^2$.
    The gray line refers to the performance of OLS.
    Any point above the gray line represents negative transfer, while any point below represents positive transfer.
    %The estimator is defined by solving equation \eqref{eq_hps}.
    In this simulation, we set $p = 100$, $n_2 = 300$, and $\sigma^2 = \frac 1 4$.
    {We sample the covariates $X$ from a $p$-dimensional isotropic Gaussian and
    plot the excess risk of HPS (see equation \eqref{HPS_loss}).}}
    \label{fig_motivation}
\end{figure*}
%For further details about the setup, see Proposition \ref{claim_model_shift}, Section \ref{sec_sizeratio}.

\subsubsection{Minimax lower bounds}

The above estimator seems like a natural choice in practice.
However, how would we know if there won't be another estimator that is much better the above?
Concretely, suppose we are trying to estimate an unknown parameter denoted as $\beta^{(2)}$.
We are given $n_2$ samples drawn from a linear parametric model, following $\beta^{(2)}$, with isotropic Gaussian covariates $X^{(2)}$, contaminated by Gaussian noise with variance $\sigma^2$.
Then, we are given $n_1$ samples from another linear parametric model, following $\beta^{(1)}$, again with isotropic Gaussian covariates $X^{(1)}$, contaminated by Gaussian noise with variance $\sigma^2$.
The parameter vectors belong to the set \[\Theta(\mu) = \set{\beta^{(1)} \in \real^p, \beta^{(2)} \in \real^p: \|{\beta^{(1)} - \beta^{(2)}}\| \le \mu, \|{\beta^{(1)}}\| \le 1, \|{\beta^{(2)}}\| \le 1}. \]

Let $\hat\beta$ be any estimation procedure that, given the above $n_1 + n_2$ samples, produces an estimate of the unknown vector $\beta^{(2)}$.
We prove the following minimax rate on the estimation error of $\hat\beta$.
%    In the setting described above, let $\hat\beta$ be a fixed estimation procedure.
Assume that $n_1 \ge (1+\tau) p$ and $n_2 \ge (1+\tau) p$ for a constant $\tau>0$.
For any $(\beta^{(1)}, \beta^{(2)})$ within the set $\Theta(\mu)$, we have that
%with high probability over the randomness of $(X^{(1)}, X^{(2)})$, 
\begin{align}
    \inf_{\hat\beta} \sup_{\Theta(\mu)} \ex{\frac 1 {n_2}\bignorm{\wt X^{(2)}\left(\hat\beta - \beta^{(2)}\right)}^2} \ge %\frac{c^2}{256}
    c{\Big({\min\Big(\frac{n_1^2\mu^2}{(n_1+n_2)^2}, \frac{\sigma^2 p} {n_2} \Big) + \frac{\sigma^2 p} {n_1 + n_2}}\Big)}, \label{eq_lb}
\end{align}
where the expectation is over the randomness of $X^{(1)}, X^{(2)}, Y^{(1)}, Y^{(2)}$ and an independently drawn $\wt X^{(2)}$ that follows the same distribution as $X^{(2)}$.
$c$ is a fixed constant that does not grow with $p$.

The lower bound in the right-hand side of equation \eqref{eq_lb} involves two parts:
\begin{itemize}
    \item For the first part, if $\mu$ is large, the source samples are not helpful, so the rate $\frac{\sigma^2 p}{n_2}$ is the OLS rate using only the target task samples. If $\mu$ is small and $n_1$ is large, then the rate $\mu^2$ appears if we use the OLS estimator for the source task.

    \item For the second part, $\frac{\sigma^2 p} {n_1 + n_2}$ is the rate in the case without model drift, i.e., $\beta^{(1)}=\beta^{(2)}$.
\end{itemize}

\paragraph{Takeaway:} It turns out that the naive estimator described above is nearly optimal, with one caveat.
When $\mu$ is very high, then we essentially throw away the source task data.
Otherwise when $\mu$ is very small, then we combine source and target task data as HPS.
It can be shown that this adjusted HPS estimator now matches the minimax lower bound.

\paragraph{Proof sketch:} The proof of the minimax lower bound is via a covering argument.
In particular, we design a cover of the parameter vectors, taking into account the effect of model shifts \citep{wainwright2019high}.

\subsubsection{Extension to multiple tasks}

Suppose there are $t$ tasks whose feature covariates are all equal to $X \in \real^{n \times p}$. The label vector of the $i$-th task follows a linear model with an unknown $p$ dimensional vector $\beta^{(i)}$, for $i=1, 2,\dots, t$:
\begin{align}\label{eq_mtl_data}
    Y^{(i)} = X \beta^{(i)} + \varepsilon^{(i)}.
\end{align}
We assume that $X = Z\Sigma^{\frac 1 2} \in \real^{n \times p}$ is a random matrix.

We combine the samples from all the tasks with a shared feature matrix $B$ and a task-specific prediction vector for each task.
Specifically, let $B \in \real^{p \times r}$ denote the shared feature matrix and let $A = [A_1, A_2, \dots, A_t] \in \real^{r \times t}$ denote the combined prediction variables.
We remark that unlike the two-task setting concerning covariate shifts, we have both $A$ and $B$ as part of the model in the multi-task setting. The reason is two-fold.
First, in this paper, we focus on the underparameterized setting. Thus, in the two-task setting, one can show that the optimal rank of $B$ would be one, i.e., $r = 1$, in which case $B$ becomes a scalar. 
Second, to tackle more than two tasks, we need to incorporate more of the structural information from the other tasks into the model \citep{AZ05}, which is encoded as the low-rank structures of $A$ and $B$. By contrast, in the two-task setting, we can focus on the setting where we have a shared parameter vector for both tasks.

We can now write down the loss objective as follows:
\begin{align}
	\ell(A, B) = \sum_{j=1}^t \bignorm{X B A_j - Y^{(j)}}^2, \label{eq_mtl_same_cov}
\end{align}
In particular, we focus on the case where $r < t$. Otherwise, if $r \ge t$, the problem reduces to single-task learning (for reference, see Proposition 1 from \citep{wu2020understanding}).

Let $(\hat A, \hat B)$ denote the global minimizer of $\ell(A, B)$.
{In particular, we compute the global minimizer of $\ell(A, B)$ by first solving $\hat B$ as a function of $\hat A$---this turns out to be $(X^{\top}X)^{-1} X^{\top}Y$ multiplied by  $A^{\top} (A A^{\top})^+$, with $(AA^{\top})^{+}$ denoting the pseudoinverse of $AA^{\top}$. 
Next, we could find the optimal $\hat A$ by taking the rank-$r$ SVD of $Y^{\top} X (X^{\top}X)^{-1} X^{\top}Y$ to get the leading $r$ left singular vectors as $U_r \in\real^{t\times r}$.
Then, we can obtain $\hat B \hat A$ as $(X^{\top} X)^{-1} X^{\top} Y U_r U_r^{\top}$.}

\paragraph{Deriving the HPS estimator:} For the optimization objective $\ell(A, B)$ in equation \eqref{eq_mtl_same_cov}, using the local optimality condition $\frac {\partial \ell} {\partial B} = 0$, we can obtain $\hat{B}$ as a function of $A$:
\begin{align}
    \hat{B}(A) %
    &= (X^{\top} X)^{-1} X^{\top} Y A^{\top} (AA^{\top})^{+}, \label{eq_Bhat}
\end{align}
where $Y := [Y^{(1)}, Y^{(2)}, \dots, Y^{(t)}]$ and $(AA^{\top})^{+}$ denotes the pseudoinverse of $AA^{\top}$.
Plugging $\hat{B}(A)$ into equation \eqref{eq_mtl_same_cov}, we obtain the following objective that depends only on $A$ (in matrix notation):
\begin{align}\label{eq_mtl_output_layer}
    g(A) = \bignormFro{X (X^{\top}X)^{-1}X^{\top} Y A^{\top} (AA^{\top})^{+} A - Y}^2.
\end{align}
Note that $A^{\top} (AA^{\top})^{+} A$ is a projection onto the subspace spanned by the rows of $A$. For simplicity, we write it into the form
	$$A^{\top} (AA^{\top})^{+} A= U_A U_A^\top,$$
where $U_A \in \real^{t\times r}$ is a $t\times r$ partial orthonormal matrix (i.e. $U_A^\top U_A=\id_{r\times r}$). Hence, we also denote the function $g(A)$ by $g(U_A)$.

Now, to find the optimal solution for $U_A$, we write $X (X^{\top} X)^{-1} X^{\top} = P_X$ (which is an $n$ by $n$ projection matrix of rank $p$), and expand $g(A)$ as:
\begin{align*}
    g(A) = \bignormFro{P_X Y U_A U_A^{\top} - Y}^2
    &= \bignormFro{P_X Y U_A U_A^{\top}}^2 + \bignormFro{Y}^2 - 2\inner{P_X Y U_A U_A^{\top}}{Y} \\
    & = \bignormFro{Y}^2 - \inner{P_X Y U_A U_A^{\top}}{Y} = \bignormFro{Y}^2 - \inner{U_A U_A^{\top}}{Y^{\top} P_X Y},
\end{align*}
where we use the fact that $P_X^2 = P_X$ since it is a projection matrix and $U_A^{\top} U_A = \id_{r\times r}$.
Thus, to minimize $g(A)$, we just need to maximize the inner project between $Y^{\top} P_X Y$ and $U_A U_A^{\top}$---this can be achieved by finding the best rank-$r$ SVD of $Y^{\top} P_X Y$ and taking the top-$r$ singular vectors, which form the partial orthonormal matrix $U_r = U_{\hat A}$.
Lastly, we can insert $U_r$ as $A$ back to $\hat B A$ to obtain $\hat B \hat A$ as the HPS estimators for all tasks from $1$ to $t$.

We illustrate the behavior of the HPS estimator in the multitask setting as follows.
\begin{figure}[!t]
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/multitask_width.eps}
		\caption{Varying the rank of the shared matrix $B$}
		\label{fig_sec4_width}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=0.98\textwidth]{figures/multitask_transfer.eps}
		\caption{Varying model shift $\mu$}
		\label{fig_sec4_transfer}
	\end{subfigure}
    \caption{Illustration of transfer effects in the random-effects model with multiple source tasks.
    In particular, $\mu$ indicates the extent of model shift --- higher values of $\mu$ means greater differences between model vectors across tasks.
	Figure \ref{fig_sec4_width} shows that for rank $r$ from $1$ to $10$, the lowest excess risk of task $t$ is achieved at $r = 1$ (averaged over three random seeds).
%	This simulation uses $\mu = 0.05$ and is .
	Figure \ref{fig_sec4_transfer} fixes $r = 1$ and varies $\mu, n$.
    Similar to Figure \ref{fig_motivation}, the transfer effect of HPS can be either positive or negative depending on $\mu, n$.}
	\label{fig_sec4}
\end{figure}

\subsection{A brief foray into reinforcement learning}

In reinforcement learning, the interactions between the agent and the environment are often described by an infinite horizon, discounted Markov decision process (MDP), specified by:
\begin{itemize}
    \item A state space $\cS$, which may be finite or infinite.
    For mathematical convenience, we will assume that $\cS$ is finite or countably infinite.

    \item An action space $\cA$, which may also be discrete or infinite.

    \item A transition function $P: \cS \times \cA \rightarrow \Delta(\cS)$, where $\Delta(\cS)$ is the space of probability distributions over $\cS$ (i.e., the probability simplex).
    $P(s' | s, a)$ is the probability of transitioning into state $s'$ upon taking action $a$ in state $s$.
    We use $P_{s, a}$ to denote the vector $P(\cdot | s, a)$.

    \item A reward function $r: \cS \times \cA \rightarrow [0, 1]$, where $r(s, a)$ is the immediate reward associated with taking action $a$ in state $s$.
    More generally, $r(s, a)$ could be a random variable where the distribution depends on $s, a$.
    %While we largely focus on the case where $r(s, a)$ is determinstic, 

    \item A discount factor $\gamma \in [0, 1)$, which defines a horizon for the problem.

    \item An initial state distribution $\mu \in \Delta(\cS)$, which specifies how the initial state $s_0$ is generated.
\end{itemize}

\paragraph{The objective, policies, and values.}
In a given MDP $M = (\cS, \cA, P, r, \gamma, \mu)$, the agent interacts with the environment according to the following protocol.
The agent starts at some state $s_0 \sim \mu$; at each time step $t = 0, 1, 2, \dots,$ the agent takes an action $a_t \in \cA$, obtains the immediate reward $r_t = r(s_t, a_t)$, and observes the next state $s_{t+1 }$ sampled according to $s_{t + 1} \sim P(\cdot | s_t, a_t)$.
The interaction record at time $t$,
\[ r_t = (s_0, a_0, r_0, s_1, \dots, s_t, a_t, r_t) \]
is called a trajectory, which includes the observed state at time $t$.

In the most general setting, a policy specifies a decision-making strategy in which the agent chooses actions adaptively based on the history of observations.
Precisely, a policy is a mapping from a trajectory to an action, i.e., $\pi: \cH \rightarrow \Delta(\cA)$ where $\cH$ is the set of all possible trajectories (of all lengths) and $\Delta(\cA)$ is the space of probability distributions over $\cA$.

\paragraph{Values.} We now define values for general policies.
For a fixed policy and a starting state $s_0 = s$, we define the value function $V^{\pi}_M: \cS \rightarrow \real$ as the discounted sum of future rewards
\[ V_M^{\pi}(s) = \ex{\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) | \pi, s_0 = s} \]
where expectation is with respect to the randomness of the trajectory, that is, randomness in state transitions and stochasticity of $\pi$.

Similarly, the action-value (or $Q$-value) function $Q_M^{\pi}: \cS \times \cA \rightarrow \real$ is defined as
\[ Q_M^{\pi}(s, a) = \ex{\sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) | \pi, s_0 = s, a_0 = a} \]

\paragraph{Goal.} Given a state $s$, the goal of the agent is to find a policy $\pi$ that maximizes t he value, i.e., the optimization problem the agent seeks to solve is:
\[ \max_{\pi} V_M^{\pi}(s), \]
where the max is over all possibly non-stationary and randomized policies.

\paragraph{Example (Navigation).}
Imagine the navigation of a robot or a delivery agent.
The state of the agent is their current location.
The four actions might be moving one step along each of east, west, north, or south. The transitions in the simplest setting are deterministic.
Taking the north action moves the agent one step north of their location, assuming the step size is standardized.
The agent might have a goal state $g$ they are trying to reach, and the reward is zero until the agent reaches the goal, and one upon reaching the goal state.
Since the discount factor $\gamma < 1$, there is incentive to reach the goal state earlier in the trajectory.
As a result, the optimal behavior in this setting corresponds to finding the shortest path from the initial state to the goal state, and the value function of a state, given a policy is $\gamma^d$, where $d$ is the number of steps required by the policy to reach the goal state.

\paragraph{Conversational agent.} This is another fairly general RL problem.
The state of an agent can be the current transcript of the conversation, along with additional information about the context of the conversation, characteristics of other agents or human in the conversation.
Actions depend on the domain, for instance, we can think of the statement to make in the conversation.
Conversational agents may be designed for task completion, such as travel assistant, tech support, or virtual receptionist.
For these cases, where may be a pre-defined set of slots which the agent needs to fill before they can find a good solution.
For instance, in the case of a travel agent, these may correspond to the dates, source, destination, and mode of travel.
The actions may correspond to the text queries to fill the slots.

For task completion, reward is defined as a binary outcome on whether the task was completed or not, such as whether the travel was successfully booked or not.
Depending on the domain, we could further refine it based on quality or price of travel package.
For more general conversations, the reward is whether the conversation was satisfactory to the other agents/humans or not.

\paragraph{Example (Strategic games).} This is another popular use case where RL has been used to achieve human level performance in Go, Chess, Poker, etc.
The usual setting consists of states being the current game board, actions being the potential next moves and reward being the eventual win/loss outcome on a more detailed score when it's defined in the game.
These are also called multi-agent RL.

\paragraph{Stationary policies.}
Stationary policies satisfy the following consistency conditions.
%\paragraph{Bellman consistency.}
In particular, suppose that $\pi$ is a stationary policy.
Then $V^{\pi}$ and $Q^{\pi}$ satisfy the following Bellman consistency equations for all $s\in\cS$ and $a\in\cA$,
\begin{align*}
    V^{\pi}(s) &= Q^{\pi}(s, \pi(s)) \\
    Q^{\pi}(s, a) &= r(s, a) + \gamma \exarg{a\sim\pi, s'\sim P(\cdot | s, a)}{V^{\pi}(s')}
\end{align*}
Let us denote $P^{\pi}$ as the transition matrix on state-action pairs induced by a stationary matrix $\pi$, specifically:
\[ P^{\pi}_{(s, a), (s', a')} \define P(s' | s, a) \pi(a' | s') \]
In particular, for deterministic policies we have
\begin{align*}
    P^{\pi}_{(s, a), (s', a')} \define
    \begin{cases}
        P(s' | s, a) & \text{ if } a' = \pi(s') \\
        0 & \text{ if } a' \neq \pi(s')
    \end{cases}
\end{align*}
Let $V^{\pi}$ be a vector of length $\cS$ and $Q^{\pi}, r$ as vectors of length $\abs{\cS} \cdot \abs{\cA}$.
Let $P$ be a matrix where $P_{(s, a), s'} = P(s' | s, a)$.
With this notation, we can verify that
\begin{align*}
    Q^{\pi} = r + \gamma P V^{\pi}, \text{ and } Q^{\pi} = r + \gamma P^{\pi} Q^{\pi},
\end{align*}
leading to the solution that $Q^{\pi} = (\id - \gamma P^{\pi})^{-1} r$.

A remarkable and convenient property of MDPs is that there exists a stationary and deterministic policy that simultaneously maximizes $V^{\pi}(s)$ for all $s \in \cS$.
In particular, the Bellman optimality equations \citep{bellman1956dynamic} give a precise characterization of the optimal value function.

\subsubsection{Finite-horizon Markov decision processes}

In some cases, it's natural to work with finite-horizon (and time-dependent) MDPs.
Here, a finite horizon, time-dependent MDP $M = (\cS, \cA, \set{P}_h, \set{r}_h, H, \mu)$ is specified as follows
\begin{itemize} 
    \item A state space $\cS$
    \item An action space $\cA$
    \item A time-dependent transition function $P_h: \cS \times \cA \rightarrow \Delta(S)$
    \item A time-dependent reward function $r_h: \cS \times \cA \rightarrow [0, 1]$
    \item An integer $H$ which defines the horizon of the problem
    \item An initial state distribution $\mu \in \Delta(\cS)$ which specifies how the initial state $s_0$ is generated
\end{itemize}
Here, for a policy $\pi$, a state $s$, and $h \in \set{0, \dots, H-1}$, we define the value function $V_h^{\pi}: \cS \rightarrow \real$ as
\[ V_h^{\pi}(s) = \ex{\sum_{t = h}^{H-1} r_h(s_t, a_t) | pi, s_h = s } \]
Similarly, the state-action value function $Q_h^{\pi}$ is defined as
\[ Q_h^{\pi}(s, a) = \ex{\sum_{t=h}^{H-1} r_h(s_t, a_t) | \pi, s_h = s, a} \]
Given a state $a$, the goal of the agent is to find a policy $\pi$ that maximizes the value $\max_\pi V^{\pi}(s)$.

When the MDP is known, solving the optimal policy can be achieved in polynomial time using LP optimization.

\subsection{Multi-armed bandits}

Above we require knowledge of the MDP. Here, we consider another decision making framework.
For $t = 1, \dots, T$,
\begin{itemize}
    \item Select decision $\pi^t \in \Pi \define = \set{1, 2, \dots, A}$
    \item Observe reward $r^t \in \real$
\end{itemize}
The protocol proceeds in $T$ rounds.
At each round $t \in [T]$, the learning agent selects a discrete decision $\pi^t \in \Pi = \set{1, \dots, K}$ using historical data.
%We allow the learner to randomize the decision at step $t$.
Based on the decision $\pi^t$, the learner receives a random reward $r_a \in [-1, 1]$ from $R(a) \in \Delta([-1, 1])$, which has mean reward
\[ \exarg{r_a \sim R(a)}{r_a} = \mu_a \]
Every iteration $t$, the learner picks an arm $I_t \in [1, 2, \dots, K]$. The cumulative regret is defined as
\[ R_T = T \cdot \max_i \mu_i - \sum_{t=0}^{T-1} \mu_{I_t} \]
Denote $a^{\star} = \arg \min_i \mu_i$ as the optimal arm.
Define gap $\Delta_a = \mu_{a^{\star}} - \mu_a$ for any arm $a$.

The upper confidence bound algorithm is one approach to tackle the MAB problem.
For simplicity, we allocate the first $K$ rounds to pull each arm once.
Then, for $t = 1 \rightarrow T - K$,
\begin{itemize}
    \item Execute arm $I_t = \arg \max_{i \in [K]} \left( \hat \mu_i^t + \sqrt{\frac {\log(TK / \delta)} {N_i^t}} \right)$
    \item Observe $r_t \define r_{I_t}$
\end{itemize}
where $I_t$ is the index of the arm that is picked by the algorithm at iteration $t$,
and the counts of each arm at each iteration $t$ is
\[ N_a^t = 1 + \sum_{i=1}^{t-1} \bm{1}\set{I_i = a} \]
The upper confidence bound states that with probability at least $1 - \delta$,
\[ \bigabs{ \hat \mu_a^t - \mu_a } \le 2 \sqrt{ \frac {\ln(TK / \delta)} {N_a^t}  } \]
where $\hat \mu_a^t$ is the empirical mean for each arm.
It can be shown that UCB achieves a regret of at most $O(\sqrt{K T \ln (KT / \delta)} + K)$.

%\paragraph{Next lecture:} Start talking about reinforcement learning.
\paragraph{Suggested readings:} Chapter 1 in \cite{agarwal2019reinforcement}.
