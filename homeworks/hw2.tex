\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\def\shownotes{1}
\usepackage{source_files/macro}
\usepackage{url}
\usepackage{mdframed}
\usepackage[noend,noline]{algorithm2e}
%\usepackage{enumerate,enumitem}
\SetEndCharOfAlgoLine{}
\SetArgSty{}
\SetKwBlock{Repeat}{repeat}{}

\everymath=\expandafter{\the\everymath\displaystyle}
%%\usepackage{psfig}
\DeclareMathSizes{24}{24}{24}{24}
\newcommand{\lecture}[2]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS7140: Advanced Machine Learning} \hfill Spring 2025}
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #1  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em Instructor: Ryan Zhang  \hfill Due: #2} }
    }
  }
  \end{center}
  \vspace*{4mm}
}
%\newcommand{\dim}{\textup{dim}}


\newcommand*{\diffdchar}{d}    % or {ⅆ}, or {\mathrm{d}}, or whatever standard you’d like to adhere to
\newcommand*{\dd}{\mathop{\diffdchar\!}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\newcommand{\E}{\mathbb{E}}





\usepackage{amsmath, amssymb}
\usepackage{fullpage}
\pagestyle{empty}
\def\pp{\par\noindent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.2}
\newcommand{\problem}[1]{ \bigskip \pp \textbf{Problem #1}\par}
\newcommand{\solution}{\textit{Solution:}\par}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bbZ}    {\mathbb{Z}}
\newcommand{\bbQ}    {\mathbb{Q}}
\newcommand{\bbN}    {\mathbb{N}}
\newcommand{\bbB}    {\mathbb{B}}
\newcommand{\bbR}    {\mathbb{R}}
\newcommand{\bbC}    {\mathbb{C}}
\newcommand{\calP}   {{\cal{P}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\lecture{Problem Set 2}{\textbf{Feb 26, 2025, 11:59pm}}

\textbf{Policy:} We encourage discussions and collaborations on homework. You should write up the solution independently and remember to mention any fellow students you collaborated with. There are up to three late days for all the problem sets and project submissions.
After that, the grade depreciates by 20\% for every extra day.
Late submissions are treated on a case-by-case basis. Please reach out to the instructor at \texttt{ho.zhang@northeastern.edu} to discuss.
All homework submissions are subject to the Northeastern University Honor Code.

\textbf{Submission:} We will use Gradescope for the homework submissions. Please submit your written solutions to Gradescope.
Login to Gradescope through Canvas using your northeastern.edu account.
For code submission, please print out the code file and attach it to the PDF solution file.
We strongly recommend that you write up your solution in LaTeX. For reference, you can find the LaTex source code at \url{https://github.com/hongyangsg/cs7140/tree/main/homeworks}.

\textbf{Length of submissions:} Include as much of the calculations needed to understand the answer. After solving the problem, try to identify the main steps taken and critical points of proof and include them as a rule of thumb.


\newpage
\paragraph{Problem 1 (10 points)}
For any symmetric matrix $M \in \mathbb{R}^{n \times n}$, show that the following definitions of positive semidefinite matrices are equivalent.
\begin{enumerate}
    \item $v^TMv\geq 0$ for all $v\in \mathbb{R}^{n}$.
    \item All the eigenvalues of $M$ are non-negative.
    \item There exists a matrix $B$ such that has $B^{\top} B = M$.
\end{enumerate}

\newpage
\paragraph{Problem 2 (15 points)}
Recall that for a two-layer neural network with activation function $\sigma: \real \rightarrow \real$, the Neural Tangent Kernel (NTK) at $x_i, x_j$ is given as:
$$H_{i,j}^* = x_i^Tx_j\cdot \mathbb{E}_{w\sim N(0,Id)}[\sigma'(w^Tx_i)\cdot\sigma'(w^Tx_j)].$$
When $\sigma(z) = \max (0,z)$ is the ReLU function, show that
$$H_{i,j}^* = x_i^Tx_j\cdot \frac{\pi - \theta}{2\pi},$$
where $\theta$ denote the angle between $x_i$ and $x_j$.

\newpage
\paragraph{Problem 3 (15 points)}
Let $f: \mathbb{R}\rightarrow \mathbb{R}$ be a smooth (infinitely differentiable) function. 
Assume that the $n$-th derivative of $f$ at zero is non-negative  for all $n \in \mathbb{N}$: $f^{(n)}(0)\geq 0$. 
Moreover, the Taylor series of function $f$ has an infinity radius of convergence.
\begin{itemize}
    \item (5 points)  Show that $k(x,x') = f(\langle x,x' \rangle)$ is a kernel.

    Hint: use the fact that the sum and the element-wise product of two PSD matrices are still PSD matrices.

    \item (10 points) Using the conclusion of the above to prove that $(1 + \langle x,x' \rangle)^p$ and $\exp{(\langle x,x' \rangle)}$ are both kernel functions.
\end{itemize}

\newpage
\paragraph{Problem 4 (20 points)}
Chernoff Bound applications. For each question, be sure to specify which Chernoff bound you use:
\begin{itemize}
    \item (10 points) Suppose you are conducting a poll to forecast the upcoming election. Assume you randomly sample 5000 likely voters, what is the probability that the percentage who support candidate $X$ that you compute differs from the true population value (the expected value) by more than an additive $\pm 2\%$? What about differing by more than $5\%$? What about more than $10\%$?
    \item (10 points) How many random voters must you sample if you want to guarantee that with probability at least $0.95$ (over the randomness in the choice of who you poll), you end up with an estimate of the percent of people who will vote for candidate $X$ that is accurate to within an additive $\pm 1\%$? How does this change if you know that the fraction will vote for $X$ is very small, say at most $5\%$?
\end{itemize}


\newpage
\paragraph{Problem 5 (40 points)}
For each of the following questions, provide two to three sentences of explanation
\begin{itemize}

    \item (5 points) What is the VC dimension of a function class? How does it relate to the sample complexity of learning this function class? What is the sample complexity of learning an $L$ layer ReLU neural network with $N$ number of parameters in total?


    \item (5 points) Given a two-layer neural network, what is the path norm of this network, and how does it relate to the generalization performance of the network?

    \item (10 points) Suppose you want to add $\ell_2$ regularization to the machine learning model. How would you add this regularization?
    Name at least two ways to achieve that.
    Similarly, how would you add $\ell_1$ regularization?

    \item (10 points) Suppose you are working on a recommendation system (say at NetFlix).
    You see the movie ratings of users on the platform, assembled as a matrix $X$.
    While running the matrix completion procedure (e.g., nuclear norm regularization), you noticed that some of the recovered entries are negative, which doesn't make sense since the user ratings should be between $1$ (lowest) to $5$ (highest).
    What would you do to mitigate the problem?
    Can you propose an algorithm based on the nuclear norm minimization problem we discussed in class to address this issue? 

    \item (5 points)
    Let $M$ be a matrix of rank-$r$.
    Let the SVD of $M$ be equal to $\sum_{i=1}^r \sigma_i u_i v_i^{\top}$.
    Recall that the best rank-$k$ ($k \le r$) approximation of $M$ in Frobenius norm is attained by $B = \Sigma_{i=1}^k\sigma_i u_i v_i^T.$ Show that $$||M-B||_{F}=\sqrt{\sum_{i=k+1}^r\sigma_i^2}.$$

    [Hint: Use the fact that $\set{u_i}$'s are orthogonal to each other, $\set{v_i}$'s are orthogonal to each other, and they are all unit vectors.]

    \item (5 points) Recall that the nuclear norm of a matrix $A\in \mathbb{R}^{m\times n}$ is defined as its sum of singular values:
    $$||A||_{\star} = \sum_{i=1}^{\min(m, n)} \sigma_i(A),$$
    where $\sigma_i(A)$ is the $i$-{th} largest singular value of $A$.
    Show that the nuclear norm $||\cdot||_{\star}$ is convex;
    that is, for any $\alpha \in [0,1]$ and two arbitrary matrices $A, B\in \real^{m\times n}$, show that the following inequality holds
    \[ ||\alpha A + (1 - \alpha)B||_{\star} \leq\alpha ||A||_{\star} + (1 - \alpha) ||B||_{\star}. \]
\end{itemize}

\end{document}