\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\def\shownotes{1}
\usepackage{macro}
\usepackage{url}
\usepackage{mdframed}
\usepackage{hyperref}
\usepackage[noend,noline]{algorithm2e}
%\usepackage{enumerate,enumitem}
\SetEndCharOfAlgoLine{}
\SetArgSty{}
\SetKwBlock{Repeat}{repeat}{}

\everymath=\expandafter{\the\everymath\displaystyle}
%%\usepackage{psfig}
\DeclareMathSizes{24}{24}{24}{24}
\newcommand{\lecture}[2]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS7140: Advanced Machine Learning} \hfill Spring 2026}
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #1  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em Instructor: Ryan Zhang  \hfill Due: #2} }
    }
  }
  \end{center}
  \vspace*{4mm}
}
%\newcommand{\dim}{\textup{dim}}


\newcommand*{\diffdchar}{d}    % or {ⅆ}, or {\mathrm{d}}, or whatever standard you’d like to adhere to
\newcommand*{\dd}{\mathop{\diffdchar\!}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\newcommand{\E}{\mathbb{E}}





\usepackage{amsmath, amssymb}
\usepackage{fullpage}
\pagestyle{empty}
\def\pp{\par\noindent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.2}
\newcommand{\problem}[1]{ \bigskip \pp \textbf{Problem #1}\par}
\newcommand{\solution}{\textit{Solution:}\par}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bbZ}    {\mathbb{Z}}
\newcommand{\bbQ}    {\mathbb{Q}}
\newcommand{\bbN}    {\mathbb{N}}
\newcommand{\bbB}    {\mathbb{B}}
\newcommand{\bbR}    {\mathbb{R}}
\newcommand{\bbC}    {\mathbb{C}}
\newcommand{\calP}   {{\cal{P}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\lecture{Problem Set 1}{\textbf{Feb 12, 2026, 11:59pm}}

\textbf{Policy:} You should write up the solution independently. You are allowed to use AI agents, but please only use it to look up for references or further results instead of directly completing the homework solutions excessively with the agents.
%There are up to three late days for all the problem sets and project submissions.
%After that, the grade depreciates by 20\% for every extra day.
If you anticipate submitting the homework late, please reach out to the instructor at \texttt{ho.zhang@northeastern.edu}.
All homework submissions are subject to the Northeastern University \href{https://catalog.northeastern.edu/handbook/policies-regulations/honor-code/}{Honor Code}.

\textbf{Submission:} We will use Gradescope for grading your homework submissions. Please submit your written solutions by logging in to Gradescope through Canvas using your northeastern.edu account.
For code submission, please print out the code file and attach it to the PDF solution file.
We strongly recommend that you write up your solution in \LaTeX. For reference, you can find the \LaTeX\ source code of the homework files on \href{https://github.com/hongyangsg/cs7140/tree/main/homeworks}{GitHub}.

When writing down the solution, include as much details as needed in order to understand your answer.
However, excessively long answers are not encouraged --- After solving the problem, try to identify the main steps and critical points of proof and highlight them.


\newpage
\paragraph{Problem 1 (40 points)}
In this problem, you will work on implementing and comparing several transfer learning estimators.
\begin{enumerate}
    \item (10 points) First, generate two tasks from synthetic data distributions with dimension $p = 100$. For instance, you may generate the covariate or feature vectors of each task from an isotropic Gaussian.
    Then, generate the labels according to two linear models $\beta^{(1)}, \beta^{(2)}$ with a Gaussian noise of mean zero and standard deviation $0.1$.

    \item (10 points) Next, fix $n_1 = 200, n_2 = 100$, vary $\delta = \bignorm{\beta^{(1)} - \beta^{(2)}}$ from $0.01$ to $1.00$.
    Implement both the single-task or ordinary least squares (OLS) estimator and the hard transfer or hard parameter sharing (HPS) estimator.
    Draw a plot by varying $\delta$, and compare the test losses of OLS and HPS estimators on task two.

    \item (10 points) Next, implement the soft transfer or soft parameter sharing (SPS) estimator with an additional regularization penalty placed on $z = \beta^{(1)} - \beta^{(2)}$ in the objective function.
    Adjust the regularization hyper-parameter $\lambda$ associated with the regularization penalty, and report the best test loss from varying $\lambda$.

    \item (10 points) Finally, draw a plot by varying $\delta$ as in part 2, and compare the test losses of HPS and SPS estimators on task two.
\end{enumerate}


\newpage
\paragraph{Problem 2 (30 points)}
Suppose $X$ and $Y$ are independent sub-Gaussian random variables with parameters $\sigma_x^2$ and $\sigma_y^2$. Show the following properties of sub-Gaussianity.
\begin{itemize}
    \item (10 points) Show that for any $c > 0$, $cX$ is sub-Gaussian with parameter $c^2\sigma_x^2$.

    \item (10 points) Show that the sum of $X$ and $Y$ is sub-Gaussian with parameter $\sigma_x^2 + \sigma_y^2$.

    \item (10 points) Validate the above two facts in simulations. Implement a numpy function that simulates the empirical histogram of the Gaussian distribution by drawing (say, $1000$) random samples.
    Then, compare the histograms of $cX$ (of variance $\sigma_x^2$) with another Gaussian whose variance is $c^2 \sigma_x^2$.

    Also compare $X + Y$ with another Gaussian whose variance is $\sigma^2_x + \sigma^2_y$.
\end{itemize}

\newpage
\paragraph{Problem 3 (30 points)}
For a random variable $X$, recall that the moment generating function (MGF) of $X$ is defined as $M_X(t) = \mathbb{E}_X[e^{tX}]$.
In this problem, we consider the MGF of several commonly encountered distributions.
\begin{itemize}
    \item (10 points) Let $X\sim{\rm Poisson}(\lambda)$ be a Poisson random variable with expectation $\lambda$. Show that the MGF of $X$ is $e^{\lambda(e^t - 1)}$.
    
    [Hint: Recall that the probability density function of $X$ is $\frac{\lambda^k e^{-\lambda}}{k!}$ for any non-negative integer $k\in \mathbb{N}$.]

    \item (10 points) Let $X\sim{\rm  Bernoulli}(p)$ be a Bernoulli random variable that is equal to one with probability $p$ and zero otherwise. Show that the MGF of $X$ is $1 - p + e^t p$.
    Next, if $X$ is the sum of $n$ independent Bernoulli random variables, what is the MGF of $X$?

    \item (10 points) Let $X$ be a uniform distribution on the interval $[a, b]$. Show that the MGF of $X$ is $\frac{e^{tb} - e^{ta}}{t(b - a)}$, for $t \neq 0$.
    If $X$ is the sum of $n$ independent and identical random variables drawn uniformly from $[a, b]$,
    what is the MGF of $X$?
\end{itemize}

\end{document}