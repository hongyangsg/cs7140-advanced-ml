\section{Generalization theory for supervised prediction}

Recall that we have introduced the empirical loss and expected loss of a hypothesis (denoted by $L(h)$ and $\hat L(h)$) for some $h$ in a hypothesis class $\cH$.
Suppose we minimize the empirical risk to get $\hat h_{\erm}$. Two questions:
\begin{itemize}
    \item Generalization gap: how does the expected and empirical risks compare for ERM, i.e., $L(\hat h_{\erm}) - \hat L(\hat h_{\erm})$? This is called the \hl{generalization gap}.

    \item Excess risk: how well does ERM do with respect to the best possible hypothesis in the hypothesis class, i.e., $L(\hat h_{\erm}) - \min_{h \in \cH} L(h)$?
    This is also called the \hl{excess risk}.
\end{itemize}

A particularly fruitful framework for analyzing learning algorithms is the probably approximately correct (PAC) framework \citep{valiant1984theory}:

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    A learning algorithm $A$ PAC learns a hypothesis class $\cH$ if
    \begin{enumerate}[label=\alph*)]
        \item For any distribution $\bP^{\star}$ supported over $\cX \times \cY$, and any $\epsilon > 0$, $\delta > 0$
        \item Upon taking $n$ I.I.D. samples from $\bP^{\star}$, $A$ produces an output $\hat h \in \cH$ such that with probability at least $1 - \delta$ (over the randomness of the samples)
        \[ L(\hat h) - \min_{h \in \cH} L(h) \le \epsilon \]
        \item Further, $n$ is a polynomial function of $\epsilon^{-1}, \delta^{-1}, d, \abs{\cH}$, and $A$ runs in time polynomial in $n, d, \epsilon^{-1}, \delta^{-1}$ (where $d$ is the dimension of the input)
    \end{enumerate}
\end{mdframed}

\paragraph{Remark:} Notice that the running time complexity places a bound on the sample complexity as well. We will assume that the empirical risk minimizer can be computed efficiently.
For instance, think of a large neural network whose training loss can be efficiently reduced to reach zero using stochastic gradient descent.

\begin{example}[Policy learning]
    Even though the above definition was proposed three decades ago, it remains a very fundamental concept, and applies broadly beyond supervised prediction.

    Consider a finite Markov decision process (MDP) with state space $\cS$, action space $\cA$, horizon $H$, and unknown transition and reward dynamics.
    Let $\Pi$ be a finite class of deterministic policies $\pi: \cS \rightarrow \cA$.
    For any policy $\pi \in \Pi$, we can define the expected loss as
    \[ L(\pi) \define \ex{\sum_{t = 1}^H \ell(s_t, a_t) }, \] 
    where the trajectory $(s_t, a_t)$ is generated by executing $\pi$ in the MDP, and $\ell$ is the loss measured at each step.

    Once we collect $n$ trajectories (e.g., via an exploration policy),\footnote{For instance, a uniform random exploration picks an action $a\in\cA$ uniformly at random at any state. This guarantees unbiased coverage of all actions. An $\varepsilon$-greedy exploration instead follows a fixed deterministic policy with probability $1- \varepsilon$, while choosing a random action with probability $\varepsilon$.} we can write down the empirical loss averaged over the $n$ sampled trajectories.
    Therefore, a policy learning algorithm $A$ PAC-learns the policy space $\Pi$ if $A$ can produce a near-optimal policy with excess loss at most $\epsilon$ with probability at least $1 - \delta$, and $n$ only depends polynomially on $\abs{\cS}, \abs{\cA}, \delta^{-1}$.
\end{example}

\subsection{Learning a realizable, finite hypothesis class (Lecture 3)}

Next, we give a concrete example to illustrate learnability in a finite, realizable hypothesis class.

\paragraph{Assumptions (realizable, finite hypothesis):}
i) The size of the hypothesis space, $\cH$, is finite;
ii) There exists a hypothesis $h^{\star} \in \cH$ such that $h^{\star}$ achieves perfect performance, i.e., \[ L(h^{\star}) = \exarg{(x, y) \in \bP^{\star}}{\ell( h^{\star}(x), y )} = 0. \]

Under these assumptions, we shall prove the following property of ERM:

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    Under the above assumptions, with probability $1 - \delta$,
    \begin{align}\label{eq_realizable} L(\hat h_{\erm}) \le \frac {\log(\abs{\cH}) + \log(\delta^{-1})} {n} \end{align}
    In particular, to reduce the expected risk below $\epsilon$, we want \[ n \ge \frac {\log(\abs{\cH}) + \log(\delta^{-1})} {\epsilon}. \]
\end{mdframed}

\begin{example}
    Imagine a clinic designing a digital diagnostic tool to determine if a patient should be referred to a specialist based on three binary (Yes/No) symptoms:
    1. Fever? 2. Cough? 3. Shortness of breath?

    Since there are only $3$ binary features, there are only $8$ possible patient profiles.
    A hypothesis in this setup is a rule that assigns a ``Refer'' or ``Not Refer'' label to each profile.
    The number of possible ways to assign binary labels to these $8$ profiles is $2^8 = 256$, which is finite.
    A hypothesis is any rule that assigns a binary labeling to the $8$ profiles.
    Thus, the size of the hypothesis space is $256$ in total.

    If a medical board has already decided a specific protocol regarding the referral, then there exists a true target function in the $256$ options.
    Thus, suppose you train the model on historical data from a clinic, you are guaranteed that at least one hypothesis in your class can achieve zero error, because the data was generated by a specific rule.
\end{example}


\begin{proof}
    We’d like to upper bound the probability of the bad event that $L(\hat h_{\erm}) > \epsilon$:
    Let $B \subseteq \cH$ be the set of bad hypotheses: $\set{B \in \cH : L(h) > \epsilon}$

    We can rewrite our goal as upper bounding the probability of selecting a bad hypothesis $\Pr[L(\hat h_{\erm}) > \epsilon] = \Pr[\hat h_{\erm} \in B]$.
    Recall the empirical risk of ERM is always zero because at least $\hat L(h^{\star}) = 0$.
    Hence for any “bad” hypothesis in $B$, they must have zero empirical risk
    \[ \Pr[\hat h_{\erm} \in B] \le \Pr[\exists h \in B : \hat L(h) = 0] \]

    Now we shall deal with the above in two steps.
    First, bound $\Pr[ \hat L(h) = 0 ]$ for a fixed $h \in B$.
    Notice that on a random example from $\cP^{\star}$, the accuracy of $h$ should be $1 - L(h)$.
    Since the training data is drawn IID from $\cP^{\star}$, and $L(h) \ge \epsilon$ for any $h \in B$, we get that
    \[ \Pr[ \hat L(h) = 0 ] \le (1 - L(h))^{n} \le (1 - \epsilon)^n \le \exp^{-\epsilon n},  \]
    where we use the fact that $1 - x \le \exp(-x)$.

    Second, we want the above to hold simultaneously for all $h \in B$.
    We can apply the union bound to bound the probability of all bad events:
    \begin{align*} 
        \Pr[\exists h \in B : \hat L(h) = 0] &\le \sum_{h \in B} \Pr[ \hat L(h) = 0 ] \\
        & \le \abs{B} \exp(- \epsilon n) \\
        & \le \abs{\cH} \exp( - \epsilon n)
    \end{align*}
    By setting the above at most $\delta$, we conclude that $\epsilon$ must be at least $\frac {\log(\abs{\cH}) + \log(\delta^{-1}) } n$.
    This concludes the proof for learning finite, realizable hypothesis spaces.
\end{proof}

Two remarks:
\begin{itemize}
    \item The excess risk only grows logarithmically with the size of the hypothesis class, so we can afford to use an exponentially large hypothesis space.

    \item The result is independent of $\bP^{\star}$. This is nice because typically we don’t know the true distribution.
\end{itemize}

The proof of this result is elementary but illustrates an important pattern that will recur in more complex scenarios.
We are interested in the expected risk, but only have access to empirical risk to choose the ERM:
\begin{itemize}
    \item Step 1 (convergence): for a fixed $h$, show that $\hat L(h)$ is close to $L(h)$ with high probability.
    \item Step 2 (uniform convergence): show that the above holds simultaneously for all hypotheses $h \in \cH$.
\end{itemize}
However, the assumptions are restrictive.
There exists a perfect hypothesis (realizability). What happens when the problem is not realizable? To answer this, we introduce the tools of concentration estimates.

Second, the hypothesis class is finite. What happens when the number of hypotheses is infinite?
To answer this, we need better ways of measuring the ``size'' of a set -- leading to Rademacher complexity, VC dimension, and PAC-Bayes bounds (to name a few).



%We will then dive deeper into the uniform convergence framework.

%\paragraph{Next lecture:}
%We'll talk about, which form the most fundamental techniques for dealing with IID random variables.
%Then we'll talk about a generalization bound for finite (not necessarily realizable) hypothesis classes.
%\textbf{Suggested reading:} Chapter 3.1-3.3 of Statistical learning theory lecture notes by Percy Liang.

\subsection{Concentration estimates (Lecture 4)}

\hl{Concentration inequalities} are powerful tools from probability theory that show the average of independent random variables will be concentrated around its expectation.
Concentration estimates are the basis of a large branch of learning theory \citep{bach2024learning} and high-dimensional statistics \citep{wainwright2019high}.
They are one of the most basic tools for studying supervised learning algorithms and models \citep{zhang2023mathematical}, primarily because much of supervised learning deals with in-distribution samples.

\begin{example}[Mean estimation]
Let $X_1, X_2, \dots, X_n$ be independent and identically distributed random variables with mean $\mu = \ex{X_i}$, for all $i = 1, 2, \dots, n$.
Define the empirical mean as \[ \hat \mu_n = \frac 1 n \sum_{i=1}^n X_i \]
How does $\hat \mu_n - \mu$ behave?
There are at least three types of statements one could make:
\begin{itemize}
    \item \hl{Consistency:} by law of large numbers, $\hat \mu_n - \mu \rightarrow 0$.

    \item \hl{Asymptotic normality:} let the variance of $X_i$ (for all $i$) be equal to $\sigma^2$, by the central limit theorem, we have $\sqrt{n} (\hat \mu_n - \mu) \sim \cN(0, \sigma^2)$.

    \item \hl{Tail estimates:} for our purpose, we would like to draw a statement of the following type $\Pr\left[ \bigabs{\hat \mu_n - \mu} \ge \epsilon \right] \le \delta$.
    For getting such tail estimates, we typically need to study the tail of a distribution, for instance, the tail of a Gaussian distribution, etc.
\end{itemize}
\end{example}

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    \textbf{Markov's inequality:}
    Let $Z \ge 0$ be a non-negative random variable. Then
    \[ \Pr\left[Z \ge t \right] \le \frac {\ex{Z}} t \]
\end{mdframed}

\begin{proof}
Since $Z$ is a non-negative quantity, we always have the condition that
\[ t \mathbbm{1}_{Z \ge t} \le Z \]
To see this, notice that if $Z \ge t$, then the above is true.
On the other hand, if $Z < t$, then the left-hand side above is zero, whereas $Z \ge 0$.
Next, take the expectation over $Z$ on both sides, we get
\[ \ex{t \mathbbm{1}_{Z \ge t}} \le \ex{Z} \Rightarrow \ex{\mathbbm{1}_{Z \ge t}} \le \frac {\ex{Z}} {t} \]
Notice that $\ex{\mathbbm{1}_{Z \ge t}} = \Pr[Z \ge t]$.
Thus, we have shown that Markov's inequality is true.
\end{proof}

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    \textbf{Chebyshev's inequality:}
Let $X$ be a random variable with mean equal to $\mu$. Then
\[ \Pr\left [ \bigabs{X - \mu} \ge \epsilon \right] \le \frac {\Var{X}} {\epsilon^2} \]
\end{mdframed}

\begin{proof}
We will use Markov's inequality to get this result.
Let $Z = (X - \mu)^2$ and let $t = \epsilon^2$.
Notice that $Z \ge 0$. Thus, based on Markov's inequality
\[ \Pr\left[Z \ge t \right] \le \frac {\ex{Z}} {t} = \frac {\ex{(Z - \mu)^2}} t = \frac {\Var{Z}} t,  \]
which completes the proof.
\end{proof}

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    \textbf{Hoeffding's inequality:}
    Let $Z_1, Z_2, \dots, Z_n$ be $n$ independent and identically distributed random variables drawn from a distribution with expectation $\mu$ and whose values are bounded from above by one.
    
    Let $\hat\mu_n = \frac 1 n \sum_{i=1}^n Z_i$ denote the mean of the $n$ random variables.
    Then, for any $\epsilon \in (0, 1)$, we have
    \begin{align} 
        \Pr\left[ \bigabs{\hat\mu_n - \mu} > \epsilon \right] \le 2\exp(-2\epsilon^2 n). \label{eq_hoeff} 
    \end{align}
\end{mdframed}

The Hoeffding's inequality is a very powerful result when we work with the average of $n$ random variables.
Variants of this inequality (which is restricted to bounded random variables) are also called Chernoff bound.\footnote{\url{https://en.wikipedia.org/wiki/Chernoff_bound}}
Next, we shall see a proof through the use of moment generating functions (MGF).

\subsubsection{Moment generating functions}

\begin{definition}[Moment generating functions]\label{def_mgf}
    For a random variable $X$, its MGF is given by
    \[ M_X(t) \define \ex{\exp(t X)} \]
    One can also think of the MGF in terms of Taylor's expansion of $\exp(tX)$ as
    \[ M_X(t) = 1 + t \ex{X} + \frac{t^2}{2} \ex{X^2} + \frac{t^3} 6 \ex{X^3} + \cdots  \]
    Thus, the first moment is the mean of $X$.
    The second moment is the variance of $X$ (assuming the mean of $X$ is zero).
    And so on.
\end{definition}

The MGF of the sum of two independent random variables $X$ and $Y$ is the product of the MGF of $X$ and $Y$, respectively.
\begin{itemize}
    \item To see this, notice that
    \[ M_{X + Y}(t) = \ex{e^{t(X + Y)}} = \ex{e^{tX} e^{tY}} = \ex{e^{tX}} \ex{e^{tY}} = M_X(t) M_Y(t) \]

    \item Here we have used that $X$ and $Y$ are independent, and hence $e^{tX}$ and $e^{tY}$ are independent, to conclude that $\ex{e^{tX} e^{tY}} = \ex{e^{tX}} \ex{e^{tY}}$
\end{itemize}

\begin{example}[Gaussian random variables]
    In this example, we shall work through the above Definition \ref{def_mgf} in the case of a Gaussian random variable.
    %we look at the MGF of Gaussian random variables.
    Let $X \sim \cN(0, \sigma^2)$. Then, we will show that $M_X(t) = e^{\sigma^2 t^2 / 2}$.
    To derive this, we use the definition of the probability density of Gaussian random variables:
    \begin{align*}
        M_X(t) = \ex{e^{tX}} &= \int \frac 1 {\sqrt{2\pi\sigma^2}} \exp\left( - \frac {x^2 - 2\sigma^2 tx} {2\sigma^2} \right) dx \\
        &= \int \frac 1 {\sqrt{2 \pi\sigma^2}} \exp\left(- \frac {(x - \sigma^2 t)^2 - \sigma^4 t^2} {2\sigma^2} \right) dx \\
        &= \exp\left( \frac {\sigma^2 t^2} 2 \right)
    \end{align*}
\end{example}

The high-level idea for showing the Hoeffding's inequality is obtained by applying Markov's inequality to $e^{tX}$ for some well-chosen value $t$.
From Markov's inequality, we can derive the following useful inequality: for any $t > 0$,
\[ \Pr[X \ge a] = \Pr\left[e^{tX} \ge e^{ta}\right] \le \frac {\ex{e^{tX}}} {e^{ta}}  \]
In particular,
\begin{align} \Pr[X \ge a] \le \min_{t > 0} \frac {\ex{e^{tX}}} {e^{ta}} \label{eq_tail_pos} \end{align}
Similarly, for any $t < 0$,
\[ \Pr[X \le a] = \Pr[e^{tX} \ge e^{ta}] \le \min_{t < 0} \frac {\ex{e^{tX}}} {e^{ta}} \]
Bounds for specific distributions are obtained by choosing appropriate values for $t$.
See Chapter 4 in \cite{mitzenmacher2017probability} for further references.

\subsubsection{Chernoff bounds for the sum of Poisson trials}

We now develop the most commonly used version of the Chernoff bound for the tail distribution of a sum of independent $0$-$1$ random variables, which are also known as Poisson trials.\footnote{Poisson trials differ from Poisson random variables.}

Let $X_1, \dots, X_n$ be a sequence of independent Poisson trials with $\Pr[X_i = 1] = p_i$.
Let $X = \sum_{i=1}^n X_i$, and let
\[ \mu = \ex{X} = \ex{\sum_{i=1}^n X_i} = \sum_{i=1}^n \ex{X_i} = \sum_{i=1}^n p_i \]

For a given $\delta > 0$, we are interested in bounds on $\Pr[X \ge (1 + \delta) \mu]$ and $\Pr[X \le (1 - \delta) \mu]$, that is, the probability that $X$ deviates from its expectation $\mu$ by $\delta \mu$ or more.
To develop a Chernoff bound, we need to compute the moment generating function of $X$.
We start with the MGF of each $X_i$:
\begin{align*}
    M_{X_i} (t) = \ex{e^{t X_i}} = p_i e^t + (1 - p_i) &= 1 + p_i (e^t - 1) \\
    &\le e^{p_i(e^t - 1)},
\end{align*}
where in the last step, we have used the fact that, for any $y$, $1 + y \le e^y$.
Since the $X_i$'s are independent from each other, we take the product of the $n$ MGF to obtain
\begin{align*}
    M_X(t) = \prod_{i=1}^n M_{X_i}(t) &\le \prod_{i=1}^n e^{p_i(e^t - 1)} \\
    &= \exp\left(\sum_{i=1}^n p_i (e^t - 1) \right) = e^{(e^t - 1) \mu}
\end{align*}
Based on this result, we now apply Markov's inequality: for any $t > 0$, we have
\begin{align*}
    \Pr[X \ge (1 + \delta) \mu] &= \Pr[e^{tX} \ge e^{t(1 + \delta) \mu}] \\
    &\le \frac {\ex{e^{tX}}} {e^{t(1 + \delta) \mu}}
    \le \frac {e^{(e^t - 1) \mu}} {e^{t(1 + \delta) \mu}}
\end{align*}
For any $\delta >0$, we can set $t = \ln(1 + \delta) > 0$ to get
\[ \Pr[X \ge (1 + \delta) \mu] \le \left( \frac {e^{\delta}} {(1 + \delta)^{1 + \delta}} \right)^{\mu} \]
For $0 < \delta \le 1$, we need to show that
\[ \frac {e^{\delta}} {(1 + \delta)^{1 + \delta}} \le e^{-\delta^2 / 3} \]
Taking the logarithm of both sides, we obtain
\[ f(\delta) = \delta - (1 + \delta) \ln(1 + \delta) + \frac {\delta^2} 3 \le 0 \]
Computing the derivatives of $f(\delta)$, we have:
\begin{align*}
    f'(\delta) = 1 - \frac {1 + \delta}{1 + \delta} - \ln(1 + \delta) + \frac {2 \delta} 3,
    f''(\delta) = - \frac 1 {1 + \delta} + \frac 2 3
\end{align*}
We see that the second derivative is less than zero if $\delta < 1/2$, and it is positive otherwise.
Hence, $f'(\delta)$ first decreases and then increases in the interval $[0, 1]$.
Since $f'(0) = 0$ and $f'(1) < 0$, we conclude that $f'(\delta) \le 0$ in the interval $[0, 1]$.
Since $f(0) = 0$, it follows that $f(\delta) \le 0$, which shows that for any $\delta$ between $0$ and $1$, we have
\[ \Pr[X \ge (1 + \delta)\mu] \le e^{-\mu\delta^2 / 3}\]


Next, let us look at an example of Chernoff bounds.

\begin{example}[Coin flips]
Let $X$ be the number of heads in a sequence of $n$ independent fair coin flips.
That is $X = X_1 + X_2 + \cdots + X_n$, where every $X_i$ is a Bernoulli random variable yielding one with probability $1/2$.
Applying the Chernoff bound, we have
\[ \Pr\left[ \bigabs{X - \frac n 2} \ge \frac 1 2 \sqrt{6n\ln n} \right] \le 2\exp\left( - \frac 1 3 \frac n 2  \frac {6\ln n}{n}\right) = \frac 2 n \]
This demonstrates that the concentration of the number of heads around the mean $n/2$ is very tight.
Most of the time, the deviations from the mean are on the order of $O(\sqrt {n \ln n})$.\footnote{Another use case of Chernoff bound: Suppose we are interested in evaluating the probability that a particular gene mutation occurs in the population.
Given a DNA sample, a lab test can determine if it carries the mutation.
However, the lab test is expensive and we would like to obtain a relatively reliable estimate from a small number of tests.}
\end{example}

\begin{example}[Gaussian random variables]
    Recall that the MGF of a standard Gaussian random variable is $M_X(t) = e^{\sigma^2 t^2 / 2}$.
    We derive a tail bound by plugging in $M_X(t)$ to equation \eqref{eq_tail_pos}:
    \begin{align*}
        \Pr[X \ge \epsilon] \le \inf_t \exp\left(\frac {\sigma^2 t^2} 2 - t\epsilon \right)
    \end{align*}
    The minimum of the RHS is attained by setting $t = \frac{\epsilon} {\sigma^2}$, yielding:
    \[ \Pr[X \ge \epsilon] \le \exp\left( -\frac{ \epsilon^2} {2\sigma^2} \right) \]
\end{example}

What about non-Gaussian random variables? Notice that the bound still holds if we replace $M_X(t)$ with an upper bound.
This motivates the following definition.

\begin{definition}[Sub-Gaussian]
    A mean zero random variable $X$ is \hl{sub-Gaussian} with parameter $\sigma^2$ if its MGF is bounded as follows:
    \begin{align}
        M_X(t) \le \exp\left(\frac{\sigma^2 t^2}2\right)
    \end{align}
    It follows that for sub-Gaussian $X$, we again have that $\Pr[X \ge \epsilon] \le \exp(-\epsilon^2 /(2\sigma^2))$.
\end{definition}

\begin{example}[Revisiting ERM in the lens of sub-Gaussian random variables]
    Imagine you are training a classifier. You want to know if the training error is a good proxy for the test error.
    For most ML loss functions, their loss values are bounded (e.g., between $0$ and $1$ or another small constant $C > 0$).
    Next, we shall verify that any bounded random variable is sub-Gaussian.
    For any $t > 0$, suppose that $X$ is a bounded random variable between $a$ and $b$. %Thus, $Y$ is bounded between $-C/2$ and $C/2$.
    First, the variance of $X$ is at most $(b - a)^2 / 4$.
    To see this, 
    \begin{align*}
        \Var{X} &= \inf_t \ex{(X - t)^2} \tag{because the minimum is achieved when $t = \ex{X}$}\\
        &\le \ex{\left(X - \frac{b + a} 2\right)^2} \tag{one can verify that $\ex{(t - \frac{b+a}2) (\frac{b+a}2 - X)} \le 0$} \\
        &\le \max\left((b - (b+a)/2)^2, (a - (b+a)/2)^2\right), 
    \end{align*}
    which is $(b - a)^2/ 4$.
    
    Next, let $\varphi(\lambda) = \log \exarg{P}{e^{\lambda X}}$.
    Let $Q_\lambda(x)$ be the distribution of $x$ defined by
    \[ dQ_\lambda(x) = \frac{e^{\lambda x}}{\exarg{X\sim P}{e^{\lambda X}}} dP(x) \]
    Next, let us look at $\varphi'(\lambda)$ and $\varphi''(\lambda)$.
    \begin{align*} 
        \varphi'(\lambda) &= \frac{\exarg{P}{X e^{\lambda X}}}{\exarg{P}{e^{\lambda X}}} = \exarg{Q_{\lambda}}{X}, \\
        \varphi''(\lambda) &= \frac{\exarg{P}{X^2 e^{\lambda X}}}{\exarg{P}{e^{\lambda X}}} - \frac{ (\exarg{P}{X e^{\lambda X}})^2 }{ (\exarg{P}{e^{\lambda X}})^2 } = \textup{Var}_{Q_\lambda}{[X]} \le \frac{(b-a)^2} 4
    \end{align*}
    Lastly, by Taylor's theorem, there exists $\lambda'$ between $0$ and $\lambda$ (when $\lambda > 0$; The case when $\lambda < 0$ is similar) such that
    \[ \varphi(\lambda) = \varphi(0) + \varphi'(0) \lambda + \frac 1 2 \varphi''(\lambda') \lambda^2 \le \frac{\lambda^2 (b-a)^2} 8 \]
    Plugging this back to the expression of $\varphi(\lambda)$ completes the proof.
    %Consider $M_X(t) = \ex{e^{Xt}}$, in particular $e^{Xt}$ is convex in $X$ for any $t$ (take the second-derivative of this function w.r.t. $X$).
    %Therefore, the maximum of $M_X(t)$ is attained at the two extreme points in the range of $X$, that is, $M_X(t) = p + (1 - p) e^{Ct}$ for some $p$ between $0$ and $1$.
    %\begin{align}
    %    M_Y(t) = \ex{e^{Yt}} = \int_{0}^C e^{X t} dX \le \int_0^C e^{Ct} dX = e^{Ct}.
    %\end{align}
    %When $t \le 0$, $M_X(t) \le 1 \le e^{Ct}$.
    %Togwhich implies that $X$ is $2C$-sub-Gaussian.
\end{example}

%\paragraph{Next lecture:} Based on the concentration estimates we have established, next time we'll talk about how to get a generalization bound for learning finite hypothesis classes. We'll then start to talk about the Rademacher complexity, which is perhaps the most fundamental tool for deriving generalization bounds. {\bf Suggested readings:} Chapter 3.5 of \cite{liang2016cs229t}, and Chapter 3 of \cite{mitzenmacher2017probability}.

\subsection{Using uniform convergence to reason about generalization (Lecture 5)}

We now give a high-level picture of the logic behind how we can use uniform convergence to reason about generalization in the context of ERM.
This will lead us to the concept of Rademacher complexity, which is a crucial notion in statistical learning theory.
We would like to show that the excess risk of ERM is small:
\begin{align}
    \Pr\left[ L(\hat h_{\erm}) - L(h^{\star}) \ge \epsilon \right] \le \delta
\end{align}
We can expand the excess risk as
\begin{align}
    L(\hat h) - L(h^{\star}) = \underbrace{\left(L(\hat h) - \hat L(\hat h) \right)}_{\text{Uniform convergence}} + \underbrace{\left( \hat L(\hat h) - \hat L(h^{\star}) \right)}_{\le 0} + \underbrace{\left( \hat L(h^{\star} )- L(h^{\star}) \right)}_{\text{Concentration}} 
\end{align}
We'll see how concentration estimates can be used to control this difference in the third part.
However, the same reasoning does not apply to the first part because the ERM $\hat h_{\erm}$ depends on the training examples $\hat L$.
In particular,
\begin{align}
    \hat L(\hat h_{\erm}) = \frac 1 n\sum_{i=1}^n \ell( \hat h_{\erm}(x_i), y_i ).
\end{align}

\begin{example}[Stochastic optimization]
    In practice, $\hat h_{\erm}$ is often computed using \emph{stochastic optimization}, such as stochastic gradient descent (SGD), which updates:
    \[ h^{(t+1)} = h^{(t)} - \eta_t \nabla \ell(h^{(t)}(x_{i_t}, y_{i_t})), \]
    where $i_t$ is sampled uniformly from $1, 2, \dots, n$.

    Each SGD step is an unbiased estimate of the gradient of $\hat L(h)$, but the final iterate $\hat h_{\erm}$ depends on the entire dataset through the optimization trajectory.
\end{example}

Due to the correlation, $\hat L$ is not a sum of independent random variables.
The central thesis of uniform convergence is that if we could ensure that $L(h)$ and $\hat L(h)$ are close for all $h\in\cH$, then $L(\hat h_{\erm})$ must be close to $\hat L(\hat h_{\erm})$ as well.
In summary, the goal of uniform convergence can be stated as
\begin{align}
    \Pr[L(\hat h_{\erm}) - L(h^{\star}) \ge \epsilon]
    \le \Pr\left[ \left(\sup_{h \in \cH} \abs{L(h) - \hat L(h)} \right) \ge \frac {\epsilon} 2 \right]
    \le \delta
\end{align}
In particular, the $1/2$ above comes from combining the error terms from the first and third parts together.
Put it in words, we'd like to upper bound the probability that the largest difference between the empirical risk and the expected risk is larger than $\epsilon /2$.



\iffalse
\subsection{Learning finite hypothesis space (Lecture 4)}

Recall from last lecture that we developed the Hoeffding's inequality.
Next we use this result to bound the excess risk of learning a finite hypothesis class (without the realizable condition).

\paragraph{Learning finite hypothesis classes:}
Let $\cH$ be a finite hypothesis class. Let $\ell$ be the zero-one loss function: $\ell(h(x), y) = \mathbbm{1}_{h(x) \neq y}$.
Suppose we minimize the empirical risk to get the minimizer $\hat h\in\cH$.
Then, with probability at least $1 - \delta$ over the randomness of training samples, the excess risk must be bounded by
\begin{align} L(\hat h) - L(h^{\star}) \le \sqrt{\frac {2(\log(\abs{\cH}) + \log(2\delta^{-1}))} n } \label{eq_finite_real} \end{align}
We can contrast this result with \eqref{eq_realizable} (of learning finite, realizable hypothesis space).
The difference is that we now get a slower convergence rate ($n^{-1}$ to $n^{-1/2}$).

\paragraph{Proof:} Recall that the excess risk can be decomposed to
\begin{align*}
    L(\hat h_{\erm}) - L(h^{\star})
    = L(\hat h_{\erm}) - \hat L(\hat h_{\erm}) + \underbrace{\hat L(\hat h_{\erm}) - \hat L(h^{\star})}_{\le 0} + \hat L(h^{\star}) - L(h^{\star})
\end{align*}
Notice that the second term is at most zero.
Thus, we focus on the first and the third terms.
\begin{itemize}
    \item Use Hoeffding’s inequality to deal with the third term.
    
    \item Apply uniform convergence to upper bound the first term by $\sup_{h\in\cH} \abs{L(h) - \hat L(h)}$ (Use the union bound).
\end{itemize}

\underline{Step 1:} bound $\Pr[L(h) - \hat L(h) \ge \epsilon]$ for a fixed $h \in \cH$.
For a fixed $h \in \cH$, notice that $\hat L(h)$ is the averaged loss among $n$ IID samples.
Each of the loss terms is bounded between zero and one (with expectation equal to $L(h)$).
Therefore, by Hoeffding's inequality,
\[ \Pr\left[ \abs{L(h) - \hat L(h)} \ge \epsilon\right] \le 2\exp(-2n\epsilon^2) \]

\underline{Step 2:} apply Step 1 uniformly over all possible $h \in \cH$.
In particular, we can apply union bound over all the possible bad events to get
\[ \Pr\left[ \sup_{h\in\cH} \abs{L(h) - \hat L(h)} \ge \epsilon \right] \le \abs{\cH} \cdot 2\exp(-2n\epsilon^2) = \delta \]
By setting $\epsilon = \sqrt{\frac {2(\log\abs{\cH} + \log(2\delta^{-1}))} n }$, we can get the probability set to $\delta$.

\paragraph{Remarks:} We have removed the realizable assumption by suffering a $\sqrt n$ factor in the generalization bound. The $\sqrt n$ factor arises from sampling noise.
It makes sense that learning is faster when there is no noise.
\fi

\subsection{Rademacher complexity (Lecture 6)}

Both of our generalization bounds require finite hypothesis classes. What about infinite hypothesis classes?
Note that we cannot directly apply the union bound to infinite hypothesis classes.
Instead, we need a more sophisticated way to measure complexity of a hypothesis class.
With Rademacher complexity, the key idea is \hl{symmetrization}. Along the way, we need an extension of the Hoeffding's inequality to some function of bounded random variables, which is known as McDiarmid's inequality.
Let us first start by motivating why we need these tools.

\subsubsection{Motivation}\label{subsubsec_motiv}

Recall that, within the uniform convergence framework, we want to get a statement of the following
\begin{align}
    \Pr\left[ L(\hat h) - L(h^{\star}) \ge \epsilon \right]
    \le \Pr\left[ \sup_{h\in\cH} \abs{L(h) - \hat L(h)} \ge \frac{\epsilon} 2 \right] \le \delta
\end{align}
Since there are two cases here, let us denote
\[ G_n \define \sup_{h\in\cH} L(h) - \hat L(h), \quad G'_n \define \sup_{h\in\cH} \hat L(h) - L(h) \]
Then we have that
\[ \Pr\left[\sup_{h\in\cH} \abs{L(h) - \hat L(h)} \ge \frac {\epsilon} 2 \right] 
\le \Pr\left[G_n \ge \frac {\epsilon} 2 \right] + \Pr\left[G'_n \ge \frac {\epsilon} 2\right] \]
Let us focus on the first case, since the second case will be similar.

Now, our main object becomes $G_n$: This is a rather non-trivial function, because of taking the supremum over a sum of random variables.
Instead, we will look at its expectation (or first moment).
%Usually, when we encounter a complicated random variable, we start by examining its , then second moment, and so on.

\subsubsection{Definition of Rademacher complexity}

Our main object of interest is now \[ \ex{G_n} = \ex{\sup_{h\in\cH} (L(h) - \hat L(h))}. \]
Recall that $\hat L(h)$ is the empirical risk, and $L(h)$ is the expected risk.

This quantity is still quite difficult because it depends on the expected risk, an expectation over the unknown data distribution.
The key idea of symmetrization is to remove this expected risk term with a simpler term.
%\paragraph{Definition of Rademacher complexity:}
Let us imagine $n$ data points $Z'_1 = (x'_1, y'_1), Z'_2 = (x'_2, y'_2), \dots, Z'_n = (x'_n, y'_n)$, sampled from the true data distribution.
Then, clearly $L(h) = \ex{\hat L'(h)}$, where $\hat L'(h)$ is the empirical risk on this copy dataset.
Hence,
\begin{align*}
    \ex{G_n} &= \exarg{Z_{1:n}}{\sup_{h\in\cH} (L(h) - \hat L(h))} \\
    &= \exarg{Z_{1:n}}{\sup_{h \in\cH} \exarg{Z'_{1:n}} {\hat L'(h) - \hat L(h)} } \\
    &= \exarg{Z_{1:n}} {\exarg{Z'_{1:n}} {\sup_{h \in \cH} (\hat L'(h) - \hat L(h)) }  }
\end{align*}

Let us remove the dependence on the copy dataset. To that end, we introduce independent Rademacher random variables $\sigma_1, \sigma_2, \dots, \sigma_n$ sampled uniformly from $\set{+1, -1}$.
Notice that \[ \hat L'(h) - \hat L(h) = \sum_{i=1}^n (\ell(h(x'_i), y'_i) - \ell(h(x_i), y_i)), \]
is symmetric around zero.
Hence, multiplying each individual term by $\sigma_i$ does not change its distribution.
Thus,
\begin{align}
    \ex{G_n} &\le \exarg{Z_{1:n}, Z'_{1:n}}{\sup_{h \in \cH} (\hat L'(h) - \hat L(h))} \notag \\
    &= \exarg{Z_{1:n}, Z'_{1:n}, \sigma_{1:n}}{\sup_{h\in \cH} \frac 1 n \sum_{i=1}^n \sigma_i (\ell(h(x'_i), y'_i) - \ell(h(x_i), y_i))} \notag \\
    &\le 2 \exarg{Z_{1:n}, \sigma_{1:n}}{\sup_{h\in\cH}\frac 1 n \sum_{i=1}^n \sigma_i \ell(h(x_i), y_i)} \label{eq_Rn_rad}
\end{align}
The last line (without the factor of $2$) is the \hl{Rademacher complexity} of $\cH$.

In summary, let $\cH$ be a hypothesis class consisting of a class of real-valued functions. Example: two-layer neural nets, linear models.
Define the Rademacher complexity (or Rademacher average) of $\cH$ to be
\[ R_n(\cH) \define \exarg{Z_{1:n}, \sigma_{1:n}}{\sup_{h\in\cH} \frac 1 n \sum_{i=1}^n \sigma_i \ell(h(x_i), y_i)}, \]
where $Z_i = (x_i, y_i)$ is a random sample from the underlying data distribution, and $\sigma_i$ is sampled uniformly from $\set{+1, -1}$.

%\paragraph{Next lecture:} In the next lecture, we will use Rademacher complexity as a generic tool to reason about generalization.

\begin{example}[Rademacher complexity of $\ell_2$-norm bounded linear predictors]\label{ex_rad_l2}
    Let $\cH = \set{z \rightarrow \inner{w}{z} \mid w \in \cW}$ be a set of linear functions such that $\bignorm{w} \le B$ for any $w \in \cW$.
    Furthermore, assume the data distribution has a bounded second moment:
    \[ \exarg{z\sim\bP}{\bignorm{z}^2 } \le C^2 \]
    Then we have
    \[ R_n(\cH) = \ex{\sup_{h\in\cH} \frac 1 n \sum_{i=1}^n h(z_i)} \le \frac {B \cdot C} {\sqrt n} \]
\end{example}

\begin{example}[Rademacher complexity of two-layer neural networks with bounded layer norms]\label{ex_rad_nn2}
    Consider a two-layer ReLU neural net with $m$ hidden units.
    Let $\Phi = (w \in \real^m, U\in \real^{m\times d})$ denote the parameters for the first and second layers, respectively.
    
    Given an input vector $x\in\real^d$, let
    \[ f_{\Phi}(x) = w^{\top} \phi(Ux) \in \real, \]
    denote the output of the network, where $\phi(\cdot)$ is the activation function that is $1$-Lipschitz continuous.
    Let us assume the training set $\set{x_i}_{i=1}^n \subseteq \real^d$ lies in some bounded $\ell_2$-norm ball.

    Let $\cH=\set{ f_{\Phi} \mid \bignorm{w} \le B', \bignorms{U} \le B }$. Then the Rademacher complexity of $\cH$ is bounded by \[ R_n(\cH) \le 2B B' \sqrt{\frac {m} {n}} \]
    Imagine we use a two-layer neural network to classify MNIST digits. If we use a network with $m$ neurons. This result says that the Rademacher complexity scales with the operator norm of the first layer weights and the second layer weights.
\end{example}

\subsubsection{Generalization bounds}

To see the power of the concept we just introduced, here is a very general statement where we can always rely on when we work with supervised learning algorithms and models.
%\paragraph{Generalization bounds based on Rademacher complexity:}
Define \[ \cA = \set{(x, y) \rightarrow \ell(h(x), y) : h \in \cH}\] as the loss function composed with the hypothesis space.
Let $\cR_n(\cA)$ denote the Rademacher complexity of the function class $\cA$.
With probability at least $1 - \delta$,
\begin{align} L(\hat h_{\erm}) - L(h^{\star}) \le 4 R_n(\cA) + \sqrt{\frac {2\log(2\delta^{-1})} n} \label{eq_rade_bound}\end{align}
Recall that $\hat h_{\erm}$ is the empirical risk minimizer (ERM), $h^{\star}$ is the expected risk minimizer, and $n$ is the size of the training set.


\begin{example}[Interpreting $G_n$ for linear predictors]
    Let $\cH = \set{ x \mapsto \inner{w}{x} : \bignorm{w}2 \le B }$ and assume the loss $\ell(h(x),y)$ is bounded.
    For a fixed hypothesis $h$, the quantity $L(h)-\hat L(h)$ measures how much the empirical risk underestimates the true risk.
    The supremum in $G_n$ searches for the {worst-case hypothesis} whose empirical performance looks overly optimistic.
    Geometrically, $G_n$ quantifies how well the training data fits noise. If there exists a direction $w$ such that the correlation between $w$ and data align on the sample but not in expectation, then $G_n$ will be large.
    Using symmetrization, we showed that $\ex{G_n} \le 2 R_n(\cA),$ where $\cA={(x,y)\mapsto \ell(h(x),y)\in\cH}$.
    For linear predictors with bounded norm, this implies $\ex{G_n} \lesssim \frac{B C}{\sqrt n},$ where $C^2=\ex{\bignorm{x}^2}$.
    Thus, as the sample size grows, even the most overfit hypothesis cannot exploit randomness in the data beyond $O(n^{-1/2})$ scaling.
\end{example}

\subsubsection{Properties of Rademacher complexity}\label{sec_property_rade}

\paragraph{Boundedness:}
\begin{align*}
    R_n(\cH) \le \max_{h\in \cH} \max_{x, y} \ell(h(x), y)
\end{align*}
This only shows that the Rademacher complexity is bounded by some constant. Usually, we'd like to show it goes to zero as $n$ goes to infinity.

\paragraph{Monotonicity:} If $\cH_1 \subseteq \cH_2$, then $R_n(\cH_1) \le R_n(\cH_2)$.

This is because we now take the supreme over a larger set, hence $R_n$ increases.

\paragraph{Scaling:} $R_n(c\cdot \cH) = c\cdot R_n(\cH)$.

\paragraph{Lipschitz composition:} Suppose $\phi$ is a Lipschitz-continuous function, bounded by some constant $c_{\phi}$. Recall a function is Lipschitz-continuous if changing $x$ only changes the function value by $c_{\phi}$ times.  In addition, $\phi(0) = 0$.

Let $\phi \circ \cH =  \set{(x, y) \rightarrow \phi(h(x), y) : h \in \cH}$, i.e., compose $\phi$ with $h$.
Then, we have that \[ R_n(\phi \circ \cH) \le c_{\phi} \cdot R_n(\cH). \]

The proof essentially uses the contraction property of the sum of Rademacher random variables.
For details, see Corollary 3.17 of \cite{ledoux2013probability}.
This property is useful because we can start by studying a simpler hypothesis class, and then compose more functions with the class without going through the calculation again.

\paragraph{Convex hull:} The convex hull of a set $\cH$ is when we take all possible linear combinations of the hypotheses in $\cH$.
One useful property is that taking the convex of $\cH$ preserves the Rademacher complexity: $R_n(\cH) = R_n(\textup{convex-hull}(\cH))$.

To see this, notice that the supreme over the convex hull must be attained at a vertex of the convex hull.
This property is quite useful if we want to calculate the Rademacher complexity of a polytope. Though it is an infinite set, it suffices to examine the vertices of the polytope. We'll use this property when we examine the $\ell_1$-regularization.

\subsubsection{Proof of Rademacher complexity generalization bounds (optional)}

The proof involves three steps:
\begin{itemize}
    \item Show that empirical Rademacher complexity is close to the expectation.
    \item Use McDiarmid's inequality, which is essentially a concentration result for functions of independent random variables.
    \item Show that the Rademacher complexity upper bounds the excess risk. We've seen this step from Section \ref{subsubsec_motiv}.
\end{itemize}
In more detail, recall that $G_n = \sup_{h\in\cH} L(h) - \hat L(h)$.
Our first claim is to show that $G_n$ is close to $\ex{G_n}$:
\begin{align}
    \Pr\left[ G_n \ge \ex{G_n} + \epsilon \right] \le \exp(-2n\epsilon^2) \label{eq_gn_dev}
\end{align}
This shows that $G_n$ is indeed close to its expectation plus a small error. Hence it suffices to upper bound its expectation.

Our second claim is to show that $\ex{G_n}$ is upper bounded by the Rademacher complexity.
\begin{align*}
    \ex{G_n} \le 2R_n(\cA).
\end{align*}
We have already seen the proof of this claim.
Combined together, we can derive \eqref{eq_rade_bound}.

Recall that \eqref{eq_gn_dev} that we'd like to show $G_n$ and its expectation $\ex{G_n}$ are close to each other.
In order to complete this result, we'll use a tool called McDiarmid’s inequality.

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.04\textwidth,
    rightmargin=0.04\textwidth]
    \textbf{McDiarmid’s inequality} (The bounded differences inequality):
    Let $f$ be a function satisfying the following bounded differences condition for all $i = 1, 2, \dots, n$ and all $x_1, x_2, \dots, x_n$, and any $x'_i$:
    \[ \bigabs{ f(x_1, x_2, \dots, x_n) - f(x_1, \dots, x_{i-1}, x'_i, x_{i+1}, \dots, x_n) } \le c_i \]
    In other words, modifying one coordinate does not change the function value too much.

    Let $X_1, X_2, \dots, X_n$ be independent random variables. Then
    \begin{align}\label{eq_mcdiarmid} \Pr\left[ \bigabs{f(X_1, X_2, \dots, X_n) - \ex{f(X_1, X_2, \dots, X_n)}} \ge \epsilon \right] \le \exp \left( - \frac {2 \epsilon^2} {\sum_{i=1}^n c_i^2} \right) \end{align}
\end{mdframed}

\begin{remark}
McDiarmid’s inequality generalizes Hoeffding’s inequality.
Let the function $f(x_1, x_2, \dots, x_n) = \frac 1 n \sum_{i=1}^n x_i$ be the empirical mean.
Suppose every $x_i$ and $x'_i$ are bounded from above by one.
Then, changing $x_i$ to $x'_i$ changes the function value by at most $\frac 2 n$, leading to $c_i = \frac 2 n$ for all $i = 1, 2, \dots, n$.
Hence, the sum of $c_i^2$ is equal to $\frac 4 {n}$.
Plugging this to \eqref{eq_mcdiarmid} recovers the Hoeffding's inequality.

This result is quite powerful, since it holds for any independent random variables, and $f$ can be complex (say a neural net).
As long as this function is not too sensitive to perturbations in one coordinate, we get good concentration.
\end{remark}

\begin{proof}[Proof of equation \eqref{eq_mcdiarmid}]
Recall the definition of a martingale sequence
A sequence of random variables $Z_0, Z_1, \dots, Z_n$ is a martingale sequence with respect to another sequence of random variables $X_1, X_2, \dots, X_n$ if and only if $Z_i$ only depends on $X_{1:i}$ and $\ex{Z_i | X_{1:i-1}} = Z_{i-1}$.

For a martingale sequence, we have the concentration because of Azuma-Hoeffding’s inequality: Suppose $Z_{1:t}$ is a martingale sequence such that $\ex{Z_i | Z_{< i}} = 0$ and $\abs{Z_i} \le c_i$. Then with probability at least $1 - \delta$,
    \begin{align}\label{eq_azuma}
        \sum_{i = 1}^n Z_i \le \sqrt{2\Big(\sum_{i=1}^n c_i^2\Big) \log(\delta^{-1})}
    \end{align}
    For reference, see Chapter 12.4 of \citet{mitzenmacher2017probability}.\footnote{See also \url{https://en.wikipedia.org/wiki/Azuma\%27s_inequality}}

We’re going to construct a sequence
\begin{align}
    Z_i = \ex{f(X_1, X_2, \dots, X_n) \mid X_{1:i}}
\end{align}
Let’s check the following properties
\begin{itemize}
    \item Clearly, $Z_i$ only depends on the values of $X_{1:i}$, but not of the remaining sequence

    \item Besides,
    \[ \ex{Z_i \mid X_{1:i-1}} = \ex{\ex{f(X_1, X_2, \dots, X_n) \mid X_{1:i}} \mid X_{1:i-1}} = Z_{i-1} \]

    \item Note that for $i = 0$, \[ Z_0 = \ex{f(X_1, X_2, \dots, X_n)} \]

    \item For $i = n$, \[ Z_n = \ex{f(X_1, \dots, X_n) \mid X_{1:n}} = f(X_1, \dots, X_n) \]
\end{itemize}
Using these notations, we’d like to bound the following
\[ \Pr[Z_n - Z_0 \ge \epsilon] \]
Let $D_i = Z_i - Z_{i-1}$. Clearly, $Z_n - Z_0 = \sum_{i=1}^n D_i$.
We’re going to show that $\abs{D_i}$ is bounded by $c_i$.
Therefore, using Azuma-Hoeffding’s inequality, we can get an exponentially decreasing tail bound for $Z_n - Z_0$.
We write down both $Z_i$ and $Z_{i-1}$ as follows:
\begin{align*}
    Z_{i-1} &= \ex{f(X_1, \dots, X_n) \mid X_{1:i-1}} \\
    Z_i &= \ex{f(X_1, \dots, X_n) \mid X_{1:i}}
\end{align*}
The key difference is whether we condition on $X_i$ or not above.
Imagine the set of possible realizations of $X_i$.
We’ll consider the maximum and minimum achieving realizations
\begin{align*}
    L_i &= \inf_{x} \ex{f(X_1, \dots, X_{i-1}, X_i = x, \dots, X_n) \mid X_{1:i-1}, X_i = x} - Z_{i-1} \\
    U_i &= \sup_{x} \ex{f(X_1, \dots, X_{i-1}, X_i = x, \dots, X_n) \mid X_{1:i-1}, X_i = x} - Z_{i-1}
\end{align*}
Clearly, $D_i$ is somewhere between $L_i$ and $U_i$.
Suppose $L_i$ is achieved by $X_i = x_L$ and $U_i$ is achieved by $X_i = x_U$.

By the bounded difference assumption, for any fixed values $X_{1:n}$ except $X_i$ (being $x_L$ or $x_U$), they are at most $c_i$ apart.
Taking expectation over $X_{1:i-1}$ and note that they are independent of $X_i$ (key!), we have that
\begin{align*} 
\bigabs{U_i} &= \bigabs{\ex{ f(X_1, \dots, X_{i-1}, X_i = x_U, \dots, X_n) - f(X_1, \dots, X_{i-1}, X_i, \dots, X_n) \mid X_{1:i-1}}}\\
&\le c_i,
\end{align*}
by the bounded difference condition on $X_i$.
And similarly, $\abs{L_i} \le c_i$.
Thus, we have verified that the sequence $D_1, D_2, \dots, D_n$ is bounded.
In addition, we can check that they are a Martingale sequence based on the properties listed above.

Having checked through McDiarmid's inequality, it remains to show that $G_n$ satisfies the bounded difference condition.
Recall that $G_n = \sup_{h\in \cH} L(h) - \hat L(h)$.
This quantity naturally arises as we derive a uniform convergence claim.
Recall that $\hat L$ is the empirical risk.
Consider changing $Z_i$ to $Z'_i$ in $G_n$, we’d like to bound the difference
\begin{align*}
    \bigabs{ \left(\sup_{h\in\cH} L(h) - \hat L(h) \right) - \left( \sup_{h\in\cH} \left( L(h ) - \hat L(h) + \frac 1 n ( \ell(Z_i, h) - \ell(Z'_i, h) ) \right) \right) } \le \frac 1 n
\end{align*}
This is because the loss values are within $0$ and $1$.
Taking supreme cannot increase the difference.
\end{proof}

We now put all the pieces together to complete the proof of equation \eqref{eq_rade_bound}.
\begin{proof}[Proof of equation \eqref{eq_rade_bound}] 
    We’d like to show that, for $\epsilon = 4R_n(\cA) + \sqrt{\frac {2\log(2\delta^{-1})} n}$,
        \[ \Pr[L(\hat h_{\erm}) - L(h^{\star}) \ge \epsilon ] \le \Pr\left[ \sup_{h\in\cH} \bigabs{L(h) - \hat L(h)} \ge \frac {\epsilon} 2\right] \le \delta \] 
    The latter reduces to showing that
        \[ \Pr\left[\sup_{h \in \cH} \bigabs{L(h) - \hat L(h)} \ge \frac {\epsilon} 2\right] \le \Pr[G_n \ge \frac {\epsilon} 2 ] + \Pr[G'_n \ge \frac {\epsilon} 2]  \]
    By the steps above, we have
        \begin{align*}
            \Pr[G_n \ge \frac{\epsilon} 2 ] &\le \Pr\left[G_n - \ex{G_n} \ge \sqrt{\frac {\log(2\delta^{-1})} {2n}}\right]\\
            &\le \exp\left( -2n \left(\frac {\log(2\delta^{-1})} {2n}\right)\right) = \frac {\delta} 2
        \end{align*}
    The first line is because by \eqref{eq_Rn_rad}, $\ex{G_n} \le 2 R_n(\cA)$.
    A similar proof applies to show that $\Pr[G'_n \ge \frac {\epsilon} 2] \le \frac {\delta} 2$.
\end{proof}

\subsection{Examples of using Rademacher complexity (Lecture 7)}

So far, we have set up Rademacher complexity for bounding the complexity of an infinite hypothesis class.
Now, let us apply Rademacher complexity for the case when the function class is finite.
Then, we will go through several examples where the function class is infinite.
%Recall that we have already derived a nice generalization bound for finite hypothesis classes, using concentration inequalities (see result \eqref{eq_finite_real}).
%Still, this is a nice sanity check through a different route. Besides, we'll derive a useful result that will be helpful later on.

\subsubsection{Learning finite hypothesis classes}

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    \textbf{Massart's finite lemma:} Let $\cH$ be a set of functions/hypotheses.
    Let $M^2$ be a bound on the second moment of functions of $\cH$:
    \[ \sup_{f \in \cH} \frac 1 n \sum_{i = 1}^n (f(Z_i))^2 \le M^2, \]
    where $Z_i = (x_i, y_i)$ denotes an input data sample.
    Then, the empirical Rademacher complexity is bounded by:
    \begin{align}
        \hat{R}_n(F) \le \sqrt{ \frac {2M^2 \log(\abs{\cH})} n } \label{eq_emp_massart}
    \end{align}
    %Notice that if we combine the above result \eqref{eq_emp_massart} with the Rademacher complexity-based generalization bound of equation \eqref{eq_rade_bound}, we recover the result in equation \eqref{eq_finite_real}.
\end{mdframed}

\begin{example}[Model Selection in AutoML]
    In modern AutoML pipelines, we often select a model from a finite library $\mathcal{H}$ of $M$ pre-trained architectures (e.g., different versions of Vision Transformers). Since the models are fixed and we only choose the one with the minimum empirical risk $\hat{R}_n(h)$, our hypothesis class is finite. By Massart's Lemma, the complexity of this selection process scales with $\sqrt{\log M}$, meaning we can safely choose from thousands of models without significantly increasing the generalization gap, provided our validation set $n$ is sufficiently large. For instance, if there are $100$ models, the overfitting risk from picking the best model grows only with $\sqrt{\log(100)}$.
\end{example}

\begin{proof}
For simplicity, let us denote $W_f = \frac 1 n \sum_{i=1}^n \sigma_i f(Z_i)$.
Recall that the empirical Rademacher complexity is defined as
\[ \hat R_n(F) = \exarg{\sigma_{1:n}} {\sup_{f \in \cH} W_f \mid Z_{1:n} } \]
Let us write down the moment generating function of $\sup_{f\in\cH} W_f$, and we'll use the convexity of the exponential function
\begin{align*}
    \exp\left(t \cdot \ex{\sup_{f \in \cH} W_f \mid Z_{1:n}} \right)
    &\le \ex{\exp\left(t \cdot \sup_{f\in \cH} W_f \right) \mid Z_{1:n }} \\
    &= \ex{\sup_{f\in\cH} \exp\left(t \cdot W_f \right) \mid Z_{1:n}}
\end{align*}
The above can be further bounded by
\begin{align*}
    \bigabs{\cH}\cdot \ex{\exp\left(t \cdot W_f \right) \mid Z_{1:n}}
\end{align*}
Finally, recall that $W_f$ is the sum of $n$ independent random variables.
Hence, we invoke the product property of the individual MGFs, to get that $M_{W_f}(t)$ is the product of the individual random variables.
Recall that we've conditioned on $Z_{1:n}$, which can be treated as constants.
Thus, the randomness is now just on $\sigma_{1:n}$---each $\sigma_i$ is either $+1$ or $-1$, scaled by $f(Z_i)$ (which is a constant conditioned on $Z_i$).
We can show that $\sigma_i$ is sub-Gaussian with parameter $1$, since we have the fact that for any random variable $X$ bounded between $a$ and $b$ (for some $a < b$), then $X$ must be $(b-a)^2/4$ sub-Gaussian.\footnote{For a proof see this blog post: \url{https://statisticaloddsandends.wordpress.com/2018/10/05/bounded-random-variables-are-sub-gaussian/}}
%\begin{align*}
%    M_{\sigma_i}(t) = \ex{\exp(t \sigma_i)} = \frac 1 2 \left( \exp(t) + \exp(-t) \right)
%\end{align*}

As a result, $W_f$ is sub-Gaussian with parameter at most $\frac 1 {n^2} \sum_{i=1}^n (f(Z_i))^2 \le M^2 / n$.
Therefore, we now get that
\[ \ex{\exp(t \cdot W_f) \mid Z_{1:n}} \le \exp\left(\frac{t \cdot M^2} {2n}\right) \]
We can now conclude that
\[ \exp(t \hat R_n(f)) \le \bigabs{\cH} \exp\left( \frac{t M^2} {2n} \right) \]
Taking log on both sides and dividing by $t$, we get:
\[ \hat R_n(\cH) \le \frac{\log(\bigabs{\cH})} t + \frac{ t M^2} {2n} \]
By setting $t$ to minimize the right-hand side above, we conclude that $\hat R_n(\cH) \le \sqrt{\frac {2\log(\bigabs{\cH}) M^2} {n}}$.

We can apply the above result along with \eqref{eq_rade_bound} to derive the result for learning from a bounded set of $0$-$1$ valued functions.
\end{proof}

\subsubsection{Norm-constrained linear models}

The next two examples involve learning a linear model from a norm-bounded hypothesis space.
The first example applies to Ridge regression, for instance, training models with weight decay.
The second example applies to LASSO regression, i.e., training with $\ell_1$-regularization instead.
Next, we'll derive the Rademacher complexity of these two examples.

\begin{proof}[Proof of Example \ref{ex_rad_l2}]
    The key idea is to exploit the linear algebraic structure of the Rademacher complexity
    \begin{align*}
        R_n(\cH) = \ex{\sup_{h\in\cH} \frac 1 n \sum_{i=1}^n \sigma_i h(z_i)}
        &= \ex{\sup_{w\in\cW} \frac 1 n\sum_{i=1}^n \sigma_i \inner{w}{z_i}} \\
        &\le \ex{\sup_{w \in \cW} \bignorm{\frac {w} n} \cdot \bignorm{\sum_{i=1}^n \sigma_i z_i}} \\
        &\le \frac{ B } n \cdot \ex{\bignorm{ \sum_{i=1}^n \sigma_i z_i}}
    \end{align*}
    by the condition placed on $\cW$.
    Finally, by Jensen's inequality, the above is at most
    \begin{align*}
        \frac B n \cdot \sqrt{\ex{\bignorm{\sum_{i=1}^n \sigma_i z_i}^2 }}
        = \frac B n \cdot \sqrt{\sum_{i=1}^n \ex{\bignorm{z_i}^2}}
        \le \frac {B \cdot C} {\sqrt n}
    \end{align*}
\end{proof}

\begin{example}[Rademacher complexity of $\ell_1$ balls]
In some applications, we have a finite but large set of features, and we believe that only a small subset of them are relevant to our task.
For such applications, $\ell_1$-regularization has proven to be a working strategy (e.g., pruning, compression).
Assume that the entries of the input vector are bounded from above by some constant $C > 0$: $\max_{j=1}^d z_i[j] \le C$, for $i = 1, 2, \dots, n$.
Let $\cH = \set{z \rightarrow \inner{w}{z} \mid \bignorm{w}_1 \le B}$ be a set of linear functions.
Then, the Rademacher complexity of $\cH$ is bounded as follows
\begin{align*}
    R_n(\cH) \le \frac{ B \cdot C \cdot \sqrt{2\log(2d)}} {\sqrt n}
\end{align*}
\end{example}

\begin{proof}
The key idea is that the $\ell_1$ ball is the convex hull of $2d$ weight vectors aligned with the basis
\[ W = \set{Be_1, -Be_1, Be_2, -Be_2, \dots, Be_d, -Be_d} \]
Since the Rademacher complexity of this class is equal to the Rademacher complexity of its convex hull, we just look at the finite class $W$ since
\[ R_n(\cH) = \ex{\sup_{w\in W} \frac 1 n \sum_{i=1}^n \sigma_i \inner{w}{z_i}} \]
Since $W$ is a finite set, we can apply Massart's finite lemma (see \eqref{eq_emp_massart}).
In order to do so, we shall first check the moment condition:
\[ \frac 1 n \sum_{i=1}^n (\sigma_i \inner{w}{z_i})^2 \le \bignorm{w}_1^2 \cdot \bignorm{z}_{\infty}^2 = B^2 \cdot C^2. \]
Thus, by result \eqref{eq_emp_massart}, we now get that
\[ \hat R_n(\cH) \le \sqrt{\frac {2B^2 C^2 \log(2d)} n}. \]
\end{proof}

\iffalse
\subsubsection{Binary classification}

In the next example, we study the case of binary classification.
For an input with feature $x$ and label $y$, we focus on the set of linear predictors with bounded $\ell_2$-norm: $\cW = \set{w: \bignorm{w} \le B}$.
Next, we select a loss function for our linear predictor.
The output of the linear predictor is given by $y \cdot \inner{w}{x}$.
\begin{itemize}
    \item Zero-one loss: $\ell(m) = \mathbbm{1}_{m \le 0}$

    \item Hinge loss: $\ell(m) = \max(0, 1 - m)$

    \item Logistic loss: $\ell(m) =  {\log(1 + \exp(-m)) }$ %\frac {\log(2)}
\end{itemize}
The function class is $\cH = \set{z \rightarrow \inner{z}{w} \mid w \in\cW}$.
The loss function composed with $\cH$ is thus
\[ \cA = \set{ (x, y) \rightarrow \ell(y \inner{w}{x}) \mid w \in \cW} \]
\begin{itemize}
    \item The linear predictors are constrained inside an $\ell_2$-norm bounded ball

    \item Both the Hinge loss and the logistic loss are Lipschitz-continuous
    
    \item Using the composition property of Rademacher complexity (see Section \ref{sec_property_rade}), we get a generalization bound for solving binary classification using Hinge/logistic loss
\end{itemize}
The zero-one loss, however, is not Lipschitz, so we cannot directly apply our result to the zero-one loss.
Notice that the zero-one loss is not sensitive to the norm of $w$. Think of two points close to zero, one positive and the other negative.
To handle this situation, we can use the margin loss.

\paragraph{Margin loss:} Penalizes whenever we don’t predict correctly by at least a margin of $\gamma$
\[ \phi_{\gamma}(m) = \mathbbm{1}_{m \le \gamma} \]

Then, we smooth the margin loss between $0$ and $\gamma$ into a straight line, leading to the following somewhat complicated form
\begin{align*}
    \tilde\phi_{\gamma}(m) %= \min\left(1, \max\left(0, 1 - \frac m {\gamma}\right)\right)
    = \begin{cases}
        1 & \text{ if } m \le 0 \\
        1 - \frac{m}{\gamma} & \text{ if } 0 < m < \gamma \\
        0 & \text{ if } m \ge \gamma
    \end{cases}
\end{align*}

Key observation: $\tilde\phi_{\gamma}$ is $\gamma^{-1}$-Lipschitz continuous (meaning that $\bigabs{\tilde\phi_{\gamma}(m_1) - \tilde\phi_{\gamma}(m_2) } \le \frac {\bigabs{m_1 - m_2}} {\gamma}$).

\paragraph{Margin-sensitive zero-one loss for linear classifiers:}
Let $\cH$ be a set of linear models.
With probability at least $1  - \delta$,
\begin{align*}
    L_0(\hat h_{\erm}) \le \min_{h\in\cH} L_{\gamma}(h) + \frac {4 R_n (\cH)} {\gamma} + \sqrt{\frac {2 \log(2\delta^{-1})} n}
\end{align*}
Proof sketch:
\begin{itemize}
    \item Using composition of $R_n$, we have: $R_n(\tilde \phi_{\gamma} \circ \cH) \le \gamma^{-1} R_n(\cH)$
    
    \item Using Rademacher complexity applied to $\tilde L_{\gamma}$, we get the relation between $\tilde L_{\gamma}(\hat h_{\erm})$ and $\min_{h\in\cH}\tilde L_{\gamma}(h)$

    \item Notice that $\phi_0 \le \tilde \phi_{\gamma} \le \phi_{\gamma}$.
    Thus, \[\min_{h\in\cH} \tilde L_{\gamma} \le \min_{h\in\cH} L_{\gamma},
    \text{ and }~~ L_0(\hat h_{\erm}) \le \tilde L_{\gamma}(\hat h_{\erm}). \]
\end{itemize}
\fi

\subsection{Two-layer neural networks (Lecture 7)}


\begin{proof}[Proof of Example \ref{ex_rad_nn2}]
Use composition property of Rademacher complexity and result for $\ell_2$-norm bounded linear predictors; Let’s first write the Rademacher complexity
\begin{align*}
    R_n(\cH) = \exarg{\sigma_{1:n}}{ \sup_{\Phi\in\cH} \frac 1 n \sum_{i=1}^n \sigma_i w^{\top} \phi(Ux) }
\end{align*}
Let’s first deal with the supremum over the second layer weight $w$ by setting its direction to be the same as the other vector
\[ R_n(\cH) \le B' \cdot \exarg{\sigma_{1:n}} { \sup_{\Phi\in\cH} \bignorms{\frac 1 n \sum_{i=1}^n \sigma_i \phi(Ux_i) } } \]
Next, because there are $m$ neurons. Hence (by Cauchy-Schwartz inequality), we reduce the above to less than
\[ B' \cdot \sqrt m \cdot \ex{ \sup_{\bignorm{u} \le B} \bigabs{ \frac 1 n \sum_{i=1}^n \sigma_i \phi(u^{\top} x_i)} } \]
The trick here is that since the operator norm of the first layer weight $U$ is bounded by $B$, the norm of every row of $U$ (or neuron) is also bounded by $B$.
It remains to deal with the above.
Here, we can use our composition property: $\phi$ is $1$-Lipschitz continuous.
Furthermore, the absolute value can be divided into two parts. Hence, the above is at most
\[ 2B' \cdot \sqrt m \cdot \ex{ \sup_{\bignorm{u} \le B} { \frac 1 n \sum_{i=1}^n \sigma_i u^{\top} x_i} } \]
To finish the proof, we invoke our Rademacher complexity bound for $\ell_2$-norm bounded linear predictors, to get that the above is at most $2B' B \sqrt{m / n}$.

Provided with the Rademacher complexity of two-layer neural nets, we can get a generalization bound for neural networks, adding an extra factor of $\sqrt{\log(\delta^{-1})/n}$.
\end{proof}

%\paragraph{Second result:} 
The above result scales with $\sqrt m$ -- number of hidden units.
Hence, the result gets worse when we over-parametrize with more hidden units.
Empirically, researchers have observed that generalization error can improve as number of hidden units increases.
Here's the result from training a two-layer neural net on MNIST \citep{neyshabur2017exploring}.
\begin{figure}
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/mnist.png}    
    \end{minipage}\hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/pathnorm.png}
    \end{minipage}
    \caption{Vary the number of neurons $m$ in a two-layer neural net. Path norm scales nicely with the test error.}
    \label{fig:enter-label}
\end{figure}

We’ll show a generalization bound that does not depend on the number of hidden units $m$ using the path norm $P(w, U) = \sum_{i=1}^m \abs{w_i} \cdot \bignorm{u_i}$, where $u_i$ is the $i$-th row of $U$.
Let $\cH = \set{f_{\Phi} \mid P(w, U) \le B}$. Then, the Rademacher complexity of $\cH$ is bounded as follows
\[ R_n(\cH) \le B \sqrt{\frac 1 n}. \]

\begin{proof}
Let’s first write the Rademacher complexity.
Recall that $u_i$ is the $i$-th row of $U$. We expand out the Rademacher complexity using
\[ w^{\top} \phi(Ux_i) = \sum_{j=1}^m w_j \phi(u_j^{\top} x_i) \]
Let $\bar u_j = u_j / \bignorm{u_j}$. Because ReLU activation is $1$-Homogeneous, meaning that $\phi(c x) = c \phi(x)$, hence
\[ w^{\top} \phi(Ux_i) = \sum_{j=1}^m w_j \bignorm{u_j} \phi\big(\bar u_j^{\top} x_i\big) \]
We apply this expansion to the Rademacher complexity
\[ R_n(\cH) = \ex{\sup_{\Phi} \frac 1 n \sum_{i=1}^n \sigma_i \left(\sum_{j=1}^m w_j \bignorm{u_j} \phi(\bar u_j^{\top} x_i)\right). } \]
Taking the supremum over $\Phi$ means that we’ll maximize the path norm above!
Hence, after switching the order of the sum, we can get
\[ R_n(\cH) \le B \cdot \ex{\max_{1\le j\le m} \bigabs{\frac 1 n \sum_{i=1}^n \sigma_i \phi(\bar u_j^{\top} x_i)}} \le 2B \sqrt{\frac 1 n}. \]
\end{proof}


\begin{remark}
    The second result implies the first result. To see this, we use the Cauchy-Schwartz inequality:
    \[ P(w, U) = \sum_{i=1}^m \abs{w_i} \cdot \bignorm{u_i} \le \sqrt{\sum_{i=1}^m w_i^2} \cdot \sqrt{\sum_{i=1}^m \bignorm{u_i}^2} \le B' \cdot B \cdot \sqrt m \]
    Technically speaking, the path norm still scales with the number of hidden units.
    However, as we saw from the previous slide, some neurons turn out to be quite small in the experiments, resulting in a small path norm even as $m$ increases.
\end{remark}

\iffalse
\subsection{Matrix completion (Lecture 6)}

Suppose you are asked to build a recommendation system. This system is used to predict the review rating of a movie.
Related examples: Book reviews on Amazon and music recommendations on YouTube.

\paragraph{Rank-constrained minimization:}
Given an input array $M$, find $X$ as close to $M$ as possible on the observed entries, while subject to a (low) rank constraint:
\begin{align*}
    \min_{X} &~~ \sum_{(i, j) \in \Omega}  (X_{i, j} - M_{i, j})^2 \\
    \mbox{s.t.} &~~~~ \text{rank}(X) = r
\end{align*}

\paragraph{Nuclear norm relaxation:} The rank constraint makes the problem non-convex, search space increases at a combinatorial rate (although good algorithms exist).
Instead, we could relax the rank constraint into a nuclear norm constraint: given a matrix $X$, let $\bignorm{X}_{\star} = \sum_{j} \lambda_j(X)$---the sum of all the singular values of $X$.
One could show that $\bignorm{X}_{\star}$ is convex in $X$; this is called the ``nuclear'' norm of $X$.\footnote{See \url{https://en.wikipedia.org/wiki/Matrix_norm}}
Instead, solve the following, which is convex in $X$:
\begin{align*}
    \min_{X} &~~ \sum_{(i, j) \in \Omega} (X_{i, j} - M_{i, j})^2 \\
    \mbox{s.t.} &~~~ \bignorm{X}_{\star} \le d r,
\end{align*}
where we may replace $dr$ with another parameter as needed.

\paragraph{Low-rank factorization:} An alternative way to formulate the above is by writing it as a convex optimization problem:
\[ \min \sum_{(i, j) \in \Omega} (X_{i, j} - M_{i, j})^2 + \lambda \bignorm{X}_{\star} \]
This is a semi-definite program (SDP), which is convex in $X$.
Instead, we may consider the re-parametrized optimization problem:
\[ \min_{U, V} \sum_{(i, j) \in \Omega} (U_i^{\top} V_j - M_{i, j})^2, \]
where $U_i, V_j$ are both $r$-dimensional vectors, for any $i, j$.
Put together, $U = [U_1, U_2, \dots, U_{d_1}]$, $V = [V_1, V_2, \dots, V_{d_2}]$ are both rank-$r$ matrices.

\paragraph{How well does nuclear norm minimization work?}
Imagine we want to recover an unknown low-rank matrix $M \in \real^{d_1 \times d_2}$.
We observe $m$ entries from $M$ uniformly at random.
Then, given another matrix $X$, we may define the mean squared recovery error as
\[ \frac 1 {d_1} \frac 1 {d_2} \sum_{i=1}^{d_1} \sum_{j=1}^{d_2} (X_{i, j} - M_{i, j})^2 \]
Without further assumptions, we would need to see the entire matrix:
\begin{itemize}
    \item Imagine the unknown matrix only has an extremely large entry but remains nonzero elsewhere
    
    \item Without sampling this entry, we are only observing zeros
\end{itemize}
Therefore, we shall assume that $M$ is bounded in all entries---this is a reasonable assumption for modeling many applications (e.g., movie ratings).
In general, one may assume that $M$ is incoherent (meaning that the row norms of the low-rank factors are bounded at the order of $O(n^{-1})$).

\paragraph{Recovery result:} Let $d r$ be the upper bound we place on the matrix in the nuclear norm minimization program. Let $\hat M$ denote the output of the program. Then, the recovery error of $\hat M$ is at most:
\[ 2r \sqrt{\frac {\log (d)} {m d}}, \]
where $d = \max(d_1, d_2)$.
For example, if we want the test error less than $\epsilon$, then the number of samples needs to be at least
\[ m \ge \frac {d r^2} { \epsilon^2 \log (d) } \]
We're going to use Rademacher complexity to tackle this. Here we can think that nuclear norm minimization constrains how large the matrix can be.
Thus, we define the hypothesis class to be the set of nuclear norm bounded and entrywise bounded matrices as follows:
\[ \cH = \bigset{ X \in \real^{d_1 \times d_2} \mid \bignormnuc{X} \le d r, \bignorm{X}_{\infty} \le 1} \]
Let $\Omega\subseteq[d_1]\times[d_2]$ be the indices of the set of observed entries.
Let $X_{\Omega}$ denote the observed matrix that pads zero for the unobserved entries.
Then, we define the Rademacher complexity of $\cH$ as
\[ R_n(\cH) = \exarg{\Omega, \sigma_{1:m}}{\sup_{X\in\cH} \frac 1 m \sum_{(i, j) \in \Omega}  \sigma_{i, j} \cdot \bigabs{X_{i, j} - M_{i, j}}} \]

Here we're going to use the following result.
Suppose $\cH$ is a set of matrices whose nuclear norm is bounded by $C$:
\[ \cH = \set{X\in\real^{d_1\times d_2} \mid \bignormnuc{X} \le C} \]
Then, the Rademacher complexity of nuclear norm bounded matrices is at most
\begin{align}
    R_n(\cH) = \exarg{\Omega}{ \sup_{X\in\cH} \frac 1 m \sum_{ (i, j) \in \Omega } \sigma_{i, j} \bigabs{ X_{i, j} - M_{i, j} } } \le 2 C \sqrt{\frac {\log d} {m d}} \label{eq_nuc_norm_rad}
\end{align}
Recall our result from \eqref{eq_rade_bound}, if $m \ge d  r^2 \log (d\delta^{-1}) / \epsilon^2$, then the above Rademacher complexity is at most $\epsilon$.
Therefore, based on \eqref{eq_rade_bound}, we could say that with probability at least $1 - \delta$, the recovery error of the minimizer must be at most $O(\epsilon)$.

It remains to show that \eqref{eq_nuc_norm_rad} is true.
By symmetry, the absolute function inside $R_n(\cH)$ can be skipped:
\begin{align*}
    R_n(\cH) = \ex{\sup_{X\in\cH} \frac 1 m \sum_{(i, j) \in \Omega} \sigma_{i, j} (X_{i, j} - M_{i, j})}
\end{align*}
Let us introduce a zero-one matrix $\Sigma$ such that $\Sigma_{i, j} = \sigma_{i, j}$ if $(i, j) \in \Omega$, and $\Sigma_{i, j} = 0$ otherwise.
Hence, the above is equal to
\begin{align*}
    \exarg{\Omega, \sigma_{1:n}}{\sup_{X\in\cH} \frac 1 m \inner{\Sigma}{ X - M }}
    &\le \ex{\sup_{X\in\cH} \frac 1 m \bignorms{\Sigma} \cdot \bignormnuc{X - M}} \\
    &\le \frac {2C} {m} \ex{\bignorms{\Sigma}}
\end{align*}
Above we use the fact that both $X, M$ have nuclear norm bounded above by $C$.
Hence, to finish the proof, we are left to deal with the expectation of $\bignorms{\Sigma}$.
Here we'll use the claim that
\[ \ex{\bignorms{\Sigma}} \le \sqrt{ \frac {m \log d} {d}} \]
Showing this result requires using some facts related to the spectrum of random matrices, we'll cover this result at a later point in class.
In summary, we have thus shown that
\[ R_n(\cH) \le 2C \sqrt{\frac {\log(d)} {m d}} \]
\fi