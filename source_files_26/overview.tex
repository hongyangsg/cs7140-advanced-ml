\section{Overview}

\paragraph{Background.} Machine learning has been increased used in technology platforms and products, affecting our daily lives.%\footnote{ChatGPT reportedly has 300 million weekly active users: \href{https://www.cnbc.com/2024/12/04/openais-active-user-count-soars-to-300-million-people-per-week.html}{CNCB news, 2024}; Claude reportedly has 4.5 million monthly active users, \href{https://www.anthropic.com/customers/wrtn}{anthropic}.}
Machine learning involves a collection of models, algorithms, and engineering frameworks:
\begin{itemize}
    \item Regression and classification: least squares estimation, logistic regression, LDA, bias-variance tradeoff, cross-validation.
    \item Neural networks and deep learning: CNNs, backpropagation, foundation models, language modeling.
    \item Unsupervised learning: dimension reduction (e.g., PCA), clustering, contrastive learning.
    \item Causal machine learning: study the cause-and-effect with a powerful machine learning model.
    \item Generative AI: diffusion models, multi-modal learning.
    \item NumPy, Sklearn, PyTorch, TensorFlow, Hugging Face.
\end{itemize}

\subsection{What is this course about? (Lecture 1)}

This course aims to uncover the common \hl{mathematical and statistical principles} underlying the diverse array of methods.
This class is mostly about the theoretical analysis of learning algorithms and models. Many of the techniques introduced in this course---which involve a beautiful blend of probability, linear algebra, and optimization---are separate fields in their respective discipline with independent interests outside of machine learning.
For example, we will study the supreme of a complex random variable corresponding to the outcome of a learning algorithm applied to train a neural network model.
We will show how to design estimation algorithms when we are working under distribution shifts between training and test datasets.

From a practical point of view, studying the underlying working mechanisms of a learning algorithm can deepen our understanding of how things work.
For example, suppose we want to build a classifier to predict the topic of a document (e.g., sports, politics, technology, etc).
We train a logistic regression model with word frequencies as features and obtain a training accuracy of 90\% on 1000 training documents and a test accuracy of 85\% on 1000 test documents.
\begin{itemize}
    \item How reliable are these numbers? If we resample the training data, can we expect the same results?
    \item How much will the training and test accuracies increase if we double the number of training documents?
    \item What if we increase the number of features (e.g., use tri-occurrence of words)? Does regularization help?
\end{itemize}

There is obviously a clear gap between theoretical analysis and the practical performance of an algorithm.
For instance, theoretical analysis is usually conducted under strong assumptions, which limit the implications one could draw from the theoretical results.
The goal, instead, is to build a deeper understanding through theoretical analysis.

The course materials are divided into three parts: \emph{fundamental concepts of statistical learning theory} (January), \emph{generalization and optimization of neural networks and deep learning} (February), \emph{statistical modeling of emerging learning paradigms} (March).\footnote{April will be dedicated to course project presentations.}

\subsection{Supervised prediction (Lecture 1)}

Central questions: \emph{Does minimizing training error lead to low test error?}
\emph{How does the generalization ability depend on the model architecture and the training algorithm?}
It turns out that answering these questions is highly non-trivial as it also depends on the underlying data distribution.\footnote{A recent empirical study highlights empirical scaling laws as key metrics for training large language models: \href{https://arxiv.org/abs/2001.08361}{paper} (see also \href{https://openai.com/index/scaling-laws-for-neural-language-models/}{openai} page).}

To formally study these questions, let us first describe the mathematical setup:
\begin{itemize}
    \item Let $\cX$ denote the feature space. Let $\cY$ denote the space of all possible outcomes. Binary classification example: $\cX=\real^d$, $\cY=\set{+1,-1}$
    
    \item Consider the problem of predicting an output $y \in \cY$ given an input $x\in\cX$.

    \item Let $\cH$ be a set of hypotheses. Linear model example: \[ \cH = \set{x \rightarrow \beta^{\top} x + \eta: \forall \beta \in \real^d, \eta\in\real} \]

    \item Let $\ell: (\cX, \cY) \times \cH \rightarrow \real$ be a loss function. For example, the mean squared error (MSE) applied to linear models is
    \[ \ell((x, y), \beta) = \left(\beta^{\top}x + \eta - y\right)^2, \forall \beta\in\real^d, \forall\eta\in\real \]

    \item Given $n$ training data samples, denoted by $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$, the training loss (or empirical risk) of a hypothesis $h\in\cH$ is defined as
    \begin{align}
        \hat L(h) = \frac 1 n \sum_{i=1}^n \ell(h(x_i), y_i), \forall h \in \cH
    \end{align}
\end{itemize}

We make a critical assumption about the data-generating process. We assume that every $x_i, y_i$ pair is drawn independently and identically from an unknown distribution $\bP^{\star}$, supported on $\cX \times \cY$.

The test loss (or expected risk) of a hypothesis $h\in\cH$ is then given by
\begin{align}
    L(h) = \exarg{(x, y)\sim\bP^{\star}}{\ell(h(x), y)}.
\end{align}

\begin{example}[Linear regression]
    
\end{example}


\paragraph{Remarks:}
\begin{itemize}
    \item We have assumed the training and test distributions are the same. While this assumption does not hold exactly in practice, morally speaking, the training and test distributions have to be related.
    
    \item Formulating what it means to be related and not related, and dealing with the discrepancy between training and test data is studied under the area of \emph{domain adaptation} or \emph{transfer learning}.

    \item The independence assumption, which also does not hold exactly in practice, ensures that more training data gives us more information.
\end{itemize}

\paragraph{Empirical risk minimization:} Consider minimizing the training loss
\begin{align}
    \hat h_{\erm} \leftarrow \argmin_{h \in \cH} \hat L(h).
\end{align}
What can say that the relationship between $\hat L(\hat h_{\erm})$ and $L(\hat h_{\erm})$?
A key challenge is that the randomness of $\hat h_{\erm}$ now depends on $\hat L$. Thus, $\hat L(\hat h_{\erm})$ involves a correlation between the training data samples and the minimizing hypothesis.
A central aspect we will tackle in the first part of the course is to develop the machinery to tackle this challenge.

\begin{example}[Pretraining and fine-tuning]

\end{example}

\paragraph{Uniform convergence:} We show provide statements of the following flavor

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    With probability at least $1-\delta$, the gap between test loss and training loss of any hypothesis is upper bounded by some small $\epsilon$, that is, $L(h) - \hat L(h) \le \epsilon$, where the $\epsilon$ is generally a function that depends on $\delta$ and other aspects of the learning algorithm/model
\end{mdframed}

More rigorously, we would like to show statements of the following:
\begin{align} 
    \Pr\left[L(h) - \hat L(h) > \epsilon\right] \le 1 - \delta,
\end{align}
where the randomness is on the training data samples drawn from $\bP^{\star}$.

Equipped with such a statement, we will then apply the statement to the empirical risk minimizer $\hat h_{\erm}$, since the result essentially holds for any $h \in \cH$, which also subsumes $\hat h_{\erm}$ as a special case.

\subsection{Multi-layer neural networks and generative models (Lecture 1)}

Consider the case of a basic one-layer network:
\begin{align}\label{eq_xmap}
    x \rightarrow \sum_{i=1}^m a_i \sigma(w_i^{\top} x + b_i), \text{ where }.
\end{align}
\begin{itemize}
    \item $\sigma$ is the nonlinear activation function. Typical choices of $\sigma$: ReLU $x\rightarrow\max(0, x)$, sigmoid $x \rightarrow \frac 1 {1 + \exp(-x)}$. Key property: Lipschitz-continuity: A function $f: \real \rightarrow \real$ is said to be $C$-Lipschitz-continuous if the following is true: \[ \bigabs{f(x) - f(y)} \le C \cdot \bigabs{x - y}. \]
    
    \item $Z = \set{\alpha = (a_i, w_i, b_i)}_{i=1}^m$ are trainable parameters of the network. By varying them we could define the function class $\cH$ as
    \[ \cH = \set{f_{\alpha} : \forall \alpha \in Z}. \]
    
    \item $\cH$ essentially represents a set of one-hidden-layer neural networks with $m$ neurons.

    \item We can define the weight matrix $W = [w_1, w_2, \dots, w_m]$, and the bias vector $b = [b_1, b_2, \dots, b_m]$. Thus, we may write map \eqref{eq_xmap} as $x\rightarrow a^{\top} \sigma(W x + b)$.
\end{itemize}

By extending the above setup, we may write a deep network as
\begin{align}
    f_{\alpha}(x) = \sigma_L\left( W_L \sigma_{L-1} \left( \cdots \sigma_2 \left(W_2 \sigma_1 \left( W_1 x + b_1 \right) + b_2 \right) \cdots \right) \right)
\end{align}
The depth of the network is given by $L$.
The width is given by $\max(m_1, m_2, \dots, m_l)$, i.e., the layer with the most neurons in the layer.

Motivating questions: \emph{How could we analyze the training and test losses of a deep network?} \emph{How well does a deep network generalize, and how does it depend on its depth and width?}
\emph{How does this ability to learn and to generalize rely on the data distributions, and what is the role of optimization algorithms used to train the network?}

\paragraph{Language models:} A language model specifies a conditional probability distribution $\Pr_{\theta}(\cdot \mid P)$, given a prompt sequence $P$.

\begin{example}[In-context learning]

\end{example}

\paragraph{Next lecture:}
In the next lecture, we will wrap up this overview by giving a setup about how to rigorously model several emerging learning paradigms.
Then, we will dive deeper into the uniform convergence framework.