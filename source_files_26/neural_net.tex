\section{Optimization and generalization in neural networks}

In the previous section, we saw the concept of uniform convergence, and showed that with this concept, we can get pretty strong results on a variety of problems, including $\ell_2$/$\ell_1$-regularized linear functions, matrix completion, and two-layer neural networks.
However, it turns out that moving beyond these settings with the techniques we've developed is quite challenging.
A couple of key challenges are:
\begin{itemize}
    \item In terms of the sample/learning complexity, how could we go beyond two layers? How does this complexity depend on the data distribution?

    \item How could we incorporate the inductive bias induced by the choice of specific optimization algorithms into the learning complexity?
\end{itemize}
The goal of this section is to tackle the above questions.
In particular, we shall begin by establishing the folklore intuition that the learning complexity of an ML model scales with its number of parameters.
This kind of folklore is especially intriguing in the context of deep networks, as these models have millions (and now billions) of parameters.
However, this high complexity clearly does not explain/corroborate with the empirical results we typically see with supervised fine-tuning and in-context learning.

\subsection{Neural tangent kernels (Lectures 10-11)}

%\subsubsection{Basics of kernel methods}
The performance of a machine learning model breaks down into two parts:
\begin{align*}
    L(\hat f) - \inf_f L(f) = \underbrace{L(\hat f) - \inf_{f\in F} L(f)}_{\text{Estimation error}} + \underbrace{\inf_{f\in F} L(f) - \inf_{f} L(f)}_{\text{Approximation error}}
\end{align*}
Both optimization and generalization results aim to reduce estimation error.
Approximation error results, on the other hand, relate to the expressivity of a function class.

\begin{example}[Linear models and kernel features]
For linear methods (i.e., $f_w(x) = w^{\top} x$ for some parameter $w\in\real^d$ and input feature vector $x\in\real^d$), the estimation error is small, but the approximation error is large.

Kernel methods represent one way to move beyond linear methods.
In kernel methods, we replace $\inner{x}{w}$ with $\inner{\phi(x)}{w}$, where $\phi(x)$ is an arbitrary feature map of $x$.
Then, we may use the kernel features in a regression model.
Suppose we are minimizing the mean squared error using a kernel method:
\[ \ell(w) = \frac 1 {2n} \sum_{i=1}^n (\inner{\phi(x_i)}{w} - y_i)^2  \]
\end{example}

\begin{definition}[Kernels] A function $k: \cX \times \cX \rightarrow \real$ is a positive semi-definite kernel if and only if for every subset of inputs $\set{x_1, x_2, \dots, x_n} \subseteq \cX$, the matrix $K\in\real^{n\times n}$ defined by using the kernel map
\[ K_{i, j} = k(x_i, x_j), \text{ for every } 1 \le i, j \le n \]
is a positive semi-definite matrix.\footnote{Recall that a symmetric matrix $K$ is positive semi-definite if for every $x$, $x^{\top} K x \ge 0$.}
\end{definition}

\begin{example}
    Linear kernels: $k(x, x') = \inner{x}{x'}$. This follows from the fact that $K = XX^{\top}$ is positive semi-definite.

    Gaussian kernels: $k(x, x') = \exp\left( - \frac {\bignorm{x - x'}^2} {2\sigma^2} \right)$.
    This is followed by the fact that either the sum or the product of two kernels (the element-wise product (Hadamard product) of two PSD matrices is still PSD) is still a kernel.

    Polynomial kernels: $k(x, x') = (1 + \inner{x}{x'})^p$.\footnote{{Exercise:} verify that both the polynomial kernel and the Gaussian kernel are indeed kernels}
\end{example}

An alternative definition of kernels is through feature maps.
Recall that a feature map $\phi$ maps an input $x$ to a feature embedding $\phi(x)$.
Consider the kernel $k: \cX \times \cX \rightarrow \real$ defined by
\[ k(x, x') = \inner{\phi(x)} {\phi(x')} \]
This must be a kernel.
To show this, we will follow the definition of a kernel by taking a vector $\alpha$ and let
\[ \alpha^{\top} K \alpha = \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \inner{\phi(x_i)}{\phi(x_j)} = {A}^{\top} {A}, \]
where $A = \sum_{i=1}^n \alpha_i \phi(x_i)$.

\subsubsection{Motivation}

It has been widely observed that gradient-based optimization algorithms often converge to small training errors on a complex neural network model.
A widely believed explanation for this surprising phenomenon is that the neural net is over-parametrized.
The neural tangent kernel represents one of the earliest attempts to mathematically formulate the theory of over-parameterized neural networks.
The neural tangent kernel arises from the dynamics of the predictions applied to the training data.

First, we state the problem setup. Given a training dataset $\set{(x_i, y_i)}_{i = 1}^n$, where $x_i \in\real^d$ and $y_i \in\real$, let $w\in\real^N$ represent the parameters of a neural network.
Let $f_w(x)$ denote the output of the network.
We shall restrict our attention to the mean squared loss (for solving regression problems)
\[ \ell(w) = \frac 1 2 \sum_{i=1}^n (f_w(x_i) - y_i)^2 \]
For the purpose of illustration, let us consider the gradient flow update.
Let $w(t)$ denote the iterate configuration at time $t \ge 0$.
Then, $w(t)$ evolves according to
\[ \frac {d w(t)} {d t} = -\nabla \ell(w(t)) \]

We now show that the learning dynamics are characterized by a positive semi-definite matrix $H(t) \in \real^{n \times n}$, defined as
\begin{align} H_{i, j}(t) = \biginner {\frac {\partial f_{w(t)}(x_i)} {\partial w_i}} {\frac {\partial f_{w(t)}(x_j)} {\partial w_j}} \label{eq_ht}
\end{align}
Then, let the network outputs at time $t$ be defined as
\[ u(t) = [ f_{w(t)}(x_1), f_{w(t)}(x_2), \dots, f_{w(t)}(x_n) ], \]
for any $t \ge 0$.
Finally, let $y = [y_1, y_2, \dots, y_n]$.
Then, we will show that the network outputs will follow the dynamics as
\[ \frac {d u(t)} {dt} = - H(t) ( u(t) - y ) \]

\begin{proof} Recall that the evolution of the network parameters follows an update as
\[ \frac {d w(t)} {d t} = -\nabla \ell(w(t)) = - \sum_{i=1}^n ( f_{w(t)}(x_i) - y_i ) \frac {d f_{w(t)}(x_i)} {d w} \]
Using chain rule and multi-variate calculus, the evolution of the network output satisfies
\[ \frac {d f_{w(t)}(x_i)} { d t } = \biginner {\frac {d f_{w(t)}(x_i)} {d w}} {\frac {d w(t)} {dt}} \]
By applying the first step to the second step, we get
\begin{align*} 
    \frac {d f_{w(t)}(x_i)} {d t} 
    &= \biginner{ \frac {d f_{w(t)}(x_i)} {dw} } {- \sum_{j=1}^n (f_{w(t)}(x_j) - y_j) \frac {d f_{w(t)}(x_j)} {d w}} \\
    &= - \sum_{j=1}^n (f_{w(t)}(x_j) - y_j) \underbrace{\biginner{ \frac {d f_{w(t)}(x_i)} {d w} } {\frac {d f_{w(t)} (x_j)} {d w}} }_{\text{$i, j$-th entry of the $H(t)$ matrix}} \\
    &= - H(t) (u(t) - y)
\end{align*}

In summary, the learning dynamics are captured by a symmetric matrix $H(t)$.
Now, how does the NTK arise?
We will define an ultra-wide neural net whose width goes to infinity.
In the limit, it can be shown that the matrix $H(0)$ remains constant during training, i.e., equal to $H(0)$.
Moreover, under a random initialization of parameters, $H(0)$ converges to a deterministic kernel matrix $H^{\star}$ --- the Neural Tangent Kernel (NTK).
\end{proof}

\subsubsection{Definition}

Consider the mapping of a two-layer neural network
\begin{align} x \rightarrow \sum_{i=1}^m a_i \sigma(w_i^{\top} x), \label{eq_twolayer} \end{align}
where $\sigma$ represents a nonlinear activation function (such as ReLU or sigmoid).
Now consider the mapping rescaled by $\frac 1 {\sqrt m}$:
\[ x \rightarrow \frac 1 {\sqrt m} \sum_{i=1}^m a_i \sigma(w_i^{\top} x) \]
We shall linearize the right-hand side around the initialization $w_i(0)$, for all $i = 1, 2, \dots, m$;
Essentially, performing Taylor's expansion to derive the following
\begin{align*} 
    &x \rightarrow \frac 1 {\sqrt m} \sum_{i=1}^m a_i \left( \sigma( (w_i(0))^{\top} x ) + (w_i - w_i(0))^{\top} x \sigma'( w_i(0)^{\top} x ) \right) \\
    & = \frac 1 {\sqrt m } \sum_{i=1}^m a_i \left( \sigma( (w_i(0))^{\top} x ) - (w_i(0))^{\top} x \sigma'( (w_i(0))^{\top} x )  \right) + \frac 1 {\sqrt m} x^{\top} \left(\sum_{i=1}^m a_i w_i \sigma'( (w_i(0))^{\top} x ) \right)
\end{align*}

Next, %{Defining the ultra-wide kernel:}
recall that the dynamics of predictions is governed by $H(t) \in \real^{n \times n}$ (see equation \eqref{eq_ht}).
Recall that $f$ is given in equation \eqref{eq_twolayer}.
Therefore, the $i, j$-th entry of the kernel matrix $H(t)$ is
\[ H_{i, j}(0) = x_i^{\top} x_j \sum_{r = 1}^m \biginner{\frac {a_r \sigma'((w_r(0)^{\top} x_i) )} {\sqrt m}} { \frac {a_r \sigma'( (w_r(0))^{\top} x_j )} {\sqrt m} }  \]

Suppose we sample every $w_r(0)$ from a standard Gaussian distribution.
In addition, suppose we sample $a_r$ uniformly between $\set{+1, -1}$.
Then, one can view the above as the average of $m$ independent random variables.
When $m$ becomes very large, then by the law of large numbers, the average is close to its expectation.
This gives rise to the Neural Tangent Kernel evaluated at $x_i, x_j$ as follows:
\[ H^{\star}_{i, j} = x_i^{\top} x_j \exarg{ w \sim \cN(0, \id)  } { \sigma'(w^{\top} x_i) \sigma'(w^{\top} x_j) } \]

\begin{example}[Two-layer ReLU networks with second-layer fixed]
    Consider a two-layer network $f(x) = \frac{1}{\sqrt{m}} \sum_{r=1}^m a_r \sigma(w_r^\top x),$
    where $\sigma(z)=\max(0,z)$, $a_r \in \set{\pm1}$, and $w_r(0) \sim \mathcal{N}(0, \id_d)$. For ReLU, the derivative is $\sigma'(z) = \mathbbm{1}_{z > 0}.$
    Hence, the NTK between two inputs $x, x'$ is
    \[ H^\star(x,x') = x^\top x' \mathbb{E}_{w \sim \mathcal{N}(0, \id_d)} \big[ \mathbbm{1}_{w^\top x > 0} \cdot \mathbbm{1}_{w^\top x' > 0} \big]. \]

    Let $\theta$ denote the angle between $x$ and $x'$ such that $\cos \theta = \frac{x^\top x'}{\bignorm{x}\cdot \bignorm{x'}}.$
    For a Gaussian random vector, the pair $(w^\top x, w^\top x')$ is jointly normal, and
    \[ \Pr(w^\top x > 0, w^\top x' > 0) = \frac14 + \frac{1}{2\pi}\arcsin(\cos\theta) = \frac{\pi - \theta}{2\pi}, \]
    using the fact that $\arcsin(\cos\theta)=\frac{\pi}{2}-\theta$ for  $\theta\in[0,\pi]$.
    Another argument is that since $w$ is rotation-invariant, we can essentially project $x, x'$ to a two-dimensional plane, and instead imagine that we are drawing $w$ from a unit circle.
    Therefore, the NTK becomes
    $H^\star(x,x') = x^{\top}x' \cdot \frac{\pi - \theta}{2\pi}.$

    As an example, let $x_1=(1,0)$ and $x_2=(0,1)$ in $\mathbb{R}^2$. 
    Then $x_1^\top x_2 = 0$,  and $H^\star(x_1,x_2) = 0.$ 
    Since $x_1$ and $x_2$ are both unit vectors, we also have $H^\star(x_1,x_1)=H^\star(x_2,x_2) = \frac{1}{2}.$
    Thus, the NTK matrix for ${x_1,x_2}$ is $H^\star = [\frac{1}{2}, 0; 0, \frac{1}{2}].$
\end{example}

\begin{example}[Kernel ridge regression]
One way to apply the above kernel definition is via kernel ridge regression (KRR), which learns a function in the associated reproducing kernel Hilbert space (RKHS) by solving
\[ \min_{f \in \mathcal{H}} \sum_{i=1}^n (f(x_i)-y_i)^2 + \lambda \bignorm{f}_{\mathcal{H}}^2, \]
where $\lambda>0$ is a regularization parameter.
Based on the representer theorem, the optimal solution admits the finite-dimensional form
\[ f(x) = \sum_{i=1}^n \alpha_i k(x_i,x), \]
for some coefficients $\alpha \in \mathbb{R}^n$.
Let $K \in \mathbb{R}^{n \times n}$ be the kernel matrix with $K_{i, j} = k(x_i,x_j)$, and let $y = (y_1,\dots,y_n)^\top$.
Substituting the representation into the objective gives the quadratic problem
\[ \min_{\alpha\in\real^n} \bignorm{K\alpha - y}^2 + \lambda \alpha^\top K \alpha, \]
whose solution is $\alpha = (K + \lambda \id)^{-1} y.$

For a new input $x$, the prediction is $f(x) = k_x^\top \alpha$, where $k_x = \big(k(x_1,x),\dots,k(x_n,x)\big)^\top$.
In the NTK regime, the kernel $k(x,x')$ is given by the neural tangent kernel $H^\star(x,x')$. Training a sufficiently wide neural network with squared loss is equivalent to performing kernel ridge regression with kernel matrix $H^\star$.
\end{example}

\subsubsection{Convergence analysis}

Based on the NTK defined above, we now show that for this particular initialization, the neural network stays close to initialization long enough so that the loss decreases to a sufficiently low level.
There are essentially two steps here:
\begin{itemize}
    \item Step 1: For a sufficiently wide network, the randomly initialized neural net is close to the expectation -- the NTK.
    
    \item Step 2: For a sufficiently wide network, the kernel matrix at time $t$ remains close to the initialization.
\end{itemize}
For reference, much of the proof in this section is taken from the monograph by \cite{arora2020theory}.

\paragraph{$H(0)$ is close to $H^{\star}$ for sufficiently large $m$:}
Suppose the activation function (e.g., ReLU) is $1$-Lipschitz continuous.
Suppose for every input $x_i$, the Euclidean norm of $x_i$ is less than $1$, for any $i = 1, 2, \dots, n$.
For any $\epsilon > 0$, if $m \ge O(n^2 \log(\delta^{-1}) / \epsilon^2)$ (recall $n$ is the number of samples at the input), then with probability at least $1 - \delta$, we have that
\[ \bignormFro{H(0) - H^{\star}} \le \epsilon \]

To show that this is true, we will first show that every entry of $H(0)$ is close to $H^{\star}$ with probability $1 - O(\delta / n^2)$.
First, we can see that every individual entry of $H(0)$ is bounded from above by $1$:
\[ \bigabs{x_i^{\top} x_j \cdot \sigma'( (w_r(0))^{\top} x_i ) \sigma'( (w_r(0) )^{\top} x_j )} \le 1 \]
Now we shall apply Hoeffding's inequality (see \eqref{eq_hoeff}), and set the deviation as $\epsilon / n$.
Then, the failure probability becomes
\[ 2\exp\left( - \frac{2\epsilon^2 m} {n^2} \right) \]
When $m \ge O(\frac {n^2 \log(n\delta^{-1})} {\epsilon^2})$, the above probability can be reduced below $O(\delta / n^2)$.
By taking a union bound over all the entries of $H(0)$, we have that the above will hold with probability $1 - \delta$.

Finally, we convert the entry-wise error bound to the operator norm bound:
\begin{align*}
    %\bignorms{H(0) - H^{\star}}
    %\le
    \bignormFro{H(0) - H^{\star}}
    \le \sqrt{n^2 \cdot \frac {\epsilon^2} {n^2}} = \epsilon
\end{align*}

\paragraph{$H(t)$ remains close to $H^{\star}$:}
Suppose that $y_i = O(1)$ for all $i = 1, 2, \dots, n$.
Let $t > 0$, suppose that for all $0 < s < t$, $u_i(s) = O(1)$ for all $i = 1, 2, \dots, n$.
Then if $m \ge O(\frac {n^6 t^2 \log(n \delta^{-1})} {\epsilon^2})$, with probability $1 - \delta$,
\[ \bignormFro{H(t) - H^{\star}} \le \epsilon \]
There are two steps in the proof of this result:
\begin{itemize}
    \item We first show that every weight vector remains close to the initialization if the width is large.

    \item We then show that this implies the kernel matrix remains close to the NTK.
\end{itemize}

Consider the movement of a single weight vector $w_r$:
\begin{align*}
    \bignorm{ w_r(t) - w_r(0) } &= \bignorm{ \int_{0}^t \frac {d w_r(\tau)} {d \tau} d \tau } \\
    &= \bignorm{ \int_{0}^t \frac 1 {\sqrt m} \sum_{i = 0}^n (u_i(\tau) - y_i ) a_r x_i \sigma'( w_r(\tau)^{\top} x_i ) d \tau } \\
    &\le \int_{0}^t \bignorm{ \frac 1 {\sqrt m} \sum_{i= 0}^n (u_i(\tau) - y_i) a_r x_i \sigma'(w_r (\tau)^{\top} x_i) d\tau } \\
    &\le \frac 1 {\sqrt m} \sum_{i=0}^n \int_0^t \bignorm{ (u_i(\tau) - y_i) a_r x_i \sigma'(w_r(\tau)^{\top} x_i)  }d\tau \le O\left(\frac {t n} {\sqrt m}\right),
\end{align*}
where we use the fact that the inputs $x_i, y_i$ and $a_r, u_i(\tau)$ are all bounded by some constants, and the derivative of $\sigma$ is also bounded by a constant (e.g., sigmoid).

Consider the movement of a single entry of the kernel matrix
\begin{align*}
    H_{i, j}(t) - H_{i, j}(0) =& \frac 1 m \bigabs{\sum_{r = 1}^m \left( \sigma'(w_r(t)^{\top} x_i) \sigma'(w_r(t)^{\top} x_j) - \sigma'(w_r(0)^{\top} x_i) \sigma'(w_r(0)^{\top} x_j)\right) } \\
    \le& \frac 1 m \bigabs{\sum_{r = 1}^m \sigma'(w_r(t)^{\top} x_i) ( \sigma'(w_r(t)^{\top} x_j ) - \sigma'(w_r(0)^{\top} x_j) ) } \\
    &+ \frac 1 m \bigabs{\sum_{r = 1}^m \sigma'(w_r(0)^{\top} x_j) (\sigma'(w_r(t)^{\top} x_i - \sigma'(w_r(0)^{\top} x_i))} \\
    \lesssim& \frac 1 m \sum_{r = 1}^m \max_{i = 1}^n \bignorms{ \sigma'(w_r(t)^{\top} x_i) - \sigma'(w_r(0)^{\top} x_i) } \\
    \le& O\left( \frac {t n} {\sqrt m} \right)
\end{align*}

Taken together, we have shown that if $m$ is large enough, then $H(t)$ will stay close to $H^{\star}$ throughout the entire gradient descent dynamic.

%\subsubsection{Implications}
In summary, the dynamics of network outputs are governed by the following approximation using the NTK matrix $H^{\star}$:
\[ \frac {d u(t)} {d t} \approx - H^{\star} (u(t) - y) \]
We use this to describe two implications:
i) Convergence to global minima;
ii) Explaining why NTK converges faster using original labels than using random labels.

Notice that this is a linear dynamical system.
Denote the eigen-decomposition of $H^{\star}$ as $\sum_{ i= 1 }^n \lambda_i v_i v_i^{\top}$.
Now we consider the dynamics of $u(t)$ on each eigenvector separately.
Consider one eigenvector $v_i$ for instance.
We have
\[ \frac {d v_i^{\top} u(t)} {d t} \approx - v_i^{\top} H^{\star} (u(t) - y) = - \lambda_i v_i^{\top} u(t) + \lambda_i v_i^{\top} y, \]
which is equivalent to
\[ \frac {d v_i^{\top} (u(t) - y)} {d t} = - \lambda_i v_i^{\top} (u(t) - y) \]
One function that satisfies this differential equation is when
\[ v_i^{\top} (u(t) - y) = \exp(- \lambda_i t) \]
Notice that $u(t) - y$ is the difference between predictions and training labels at time $t$.

We would like to show that the difference converges to zero when $t$ is large.
The above implies that each component of $u(t) - y$ projected to eigenvector $v_i$ converges to zero exponentially fast at a rate of $\exp(- \lambda_i t)$.

Also notice that the eigenvectors $\set{v_i}_i$ form an orthonormal basis.
Since we know that each component converges to zero, together they imply that $u(t) - y$ must also converge to zero.
\hl{This shows that the outputs will eventually converge to the training labels, meaning the training loss will go to zero.}

Next, we can see that each component goes to zero at different rates.
For larger eigenvalues, the convergence is faster.
Hence in order to have fast convergence, we would like the projections of the labels onto the top eigenvectors to be large.

For a set of labels, if they align with top eigenvectors, then gradient descent converges quickly.
On the other hand, if the projections are uniform, or aligns with eigenvectors with small eigenvectors, then gradient descent converges with a slow rate.
In Figure \ref{fig:ntk_eig}, taken from \cite{neyshabur2017exploring}, we compare convergence rates of gradient descent between using original labels, random labels, and worst-case labels.
This experiment is performed to separate two classes of digits from MNIST, and it uses the forward model as $f(a, w) = \frac 1 {\sqrt m} \sum_{i=1}^m a_i \sigma'(w_i^{\top} x)$.

\begin{figure}
    \centering
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/randomlabel1.png}    
    \end{minipage}\hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/randomlabel2.png}
    \end{minipage}
    \caption{Illustration of the loss and the projected values onto the eigen-space.}
    \label{fig:ntk_eig}
\end{figure}

\subsection{Implicit regularization (Lectures 11-12)}

Recall that we discussed the matrix completion problem in Section \ref{sec_mc}.
Now, we shall study the gradient descent dynamic of this problem in an over-parameterized setting.

\begin{example}[Linear regression]
Suppose we are doing linear regression on $n$ input samples:
\[ \ell(w) = \frac 1 n \sum_{i=1}^n (x_i^{\top} \beta - y_i) \]
Consider gradient descent:
\[ w_{t+1} = w_t - \eta \nabla \ell(w_t) \]
Suppose $n$ is smaller than the dimension of $w \in \real^d$.
Suppose $w_0 = 0$.
Then, given that $\nabla \ell(w_t) = \frac 2 n\sum_{i=1}^n (x_i^{\top} \beta - y_i) x_i$, $w_{t+1}$ will always lie inside the span of $\set{x_1, x_2, \dots, x_n}$, whose dimension is smaller than $d$.
In other words, gradient descent searches inside a space whose dimension is much smaller than the actual dimension of the entire space.
\end{example}

Next we write the problem setup.
Suppose there is an underlying rank-$r$ matrix $X^{\star} \in \real^{d \times d}$.
In particular, for the illustration of the analysis, we shall focus on the case of $X^{\star}$ being positive semi-definite.
As for the input to the problem, suppose we have $m$ random Gaussian matrices whose entries are all drawn from $N(0, 1)$, denoted as $A_1, A_2, \dots, A_m$, all with dimension $d$ by $d$.
In addition, suppose they are symmetric, for example, we could transform each $A_i$ as $(A_i + A_i^{\top}) / 2$.
Let $y_i = \inner{A_i} {UU^{\top}}$, for $i = 1, 2, \dots, n$.

A solution would be to find another positive semi-definite matrix $UU^{\top}$ so that the distance between $UU^{\top}$ and $X^{\star}$ is minimized.
In particular, we shall consider the case that $U \in \real^{d \times d}$.
Then, we set up a mean squared loss as follows:
\[ \ell(U) = \frac 1 n \sum_{i=1}^n \left( \inner{UU^{\top}} {A_i} - y_i \right)^2 \]
In practice, we see that this loss function has many solutions that render its loss to near zero, even though $UU^{\top}$ can be quite far from $X^{\star}$.
A key observation is that the generalization behavior of $U$ should depend on the initialization.
Next we shall make this precise by analyzing the gradient descent dynamic.
For reference, this proof is based on the work of \cite{li2018algorithmic}.

Let $U_0 = \alpha \id$, for some small values of $\alpha$.
Then, let the gradient descent update be given by
\[ U_{t + 1} = U_t - \eta \nabla \ell(U_t) = (\id - \eta M_t) U_t, \]
where \[ M_t = \frac 1 m \sum_{i=1}^m \inner{A_i}{U_t U_t^{\top} - X^{\star}}A_i \]

We shall restrict the analysis to the rank-$1$ case where $r = 1$, where we can write $X^{\star} = u^{\star} {u^{\star}}^{\top}$, for some $u^{\star} \in \real^d$.
Let $\id_{u^{\star}} = \id - \frac {u^{\star} {u^{\star}}^{\top}} {\bignorm{u^{\star}}^2}$.
For simplicity, let's take the norm of $u^{\star}$ as $1$.
Then, we decompose the iterates $U_t$ into two parts into the subspace of $u^{\star}$ and its complement:
\begin{align*} 
    U_t &= u^{\star} {u^{\star}}^{\top} U_t + (\id - \id_{u^{\star}} {u^{\star}}^{\top}) U_t \\
        &\define u^{\star} r_t^{\top} + E_t,
\end{align*}
where we denote by $r_t = U_t^{\top} u^{\star}$, and $E_t = (\id - \id_{u^{\star}}) U_t$.

We will show that the spectral norm and the Frobenius norm of the ``error term'' $E_t$ remains small throughout the iterations, whereas the ``signal'' term $r_t$ grows exponentially fast (in the sense that the norm of $r_t$ grows to the norm of $u^{\star}$.)
Note that any solution with $\bignorm{r_t} = 1$ and $E_t = 0$ will give exact recovery, and for the purpose of this section we will show that $\bignorm{r_t}$ will converge approximately to $1$ and $E_t$ stays small.

Based on the gradient update above, we derive the update for $E_t$:
\begin{align}
    E_{t+1} &= (\id - \id_{u^{\star}}) (\id - \eta M_t) U_t \notag\\
    &= E_t - \eta (\id - \id_{u^{\star}}) M_t U_t \label{eq_Et_1}
\end{align}

We shall also assume that the set of measurement matrices $(A_1, A_2, \dots, A_m)$ satisfies the restricted isometry property.
A set of linear measurement matrices $A_1, \dots, A_m$ satisfies $(r, \delta)$-restricted isometry property (RIP) if for any $d$ by $d$ matrix $X$ with rank at most $r$, we have that
\begin{align}\label{eq_rip}
    (1 - \delta) \bignormFro{X}^2 \le \frac 1 m \sum_{i=1}^m \inner{A_i}{X}^2 \le (1 + \delta) \bignormFro{X}^2
\end{align}
In the following, we shall assume that the set of measurement matrices satisfies $(4, \delta)$-RIP with $\delta \le c$ (for some small enough constant $c$).
One corollary of the above definition is that for any matrices $X, Y \in \real^{d \times d}$ with rank at most $r$, we also have:
\begin{align}\label{eq_rip_cross} \bigabs{ \frac 1 m \sum_{i=1}^m \inner{A_i}{X} \inner{A_i}{Y} - \inner{X}{Y} } \le \delta \bignormFro{X} \bignormFro{Y}
\end{align}
If $Y$ is at most rank $r$ but $X$ may not be, then we have the following instead:
\begin{align}\label{eq_rip_oneside}
    \bigabs{\frac 1 m \sum_{i=1}^m \inner{A_i}{X} \inner{A_i}{Y} - \inner{X}{Y}} \le \delta \bignorm{X}_{\star} \bignormFro{Y}
\end{align}

\begin{theorem}[Convergence of gradient descent from small initialization in over-parameterized matrix sensing]
Suppose $\alpha \le \delta \sqrt{\frac 1 d \log(\frac 1 {\delta})}$ and $\eta \lesssim c \delta^2 \log^{-1} (\frac 1 {\delta \alpha})$.
Then after $T = \Theta(\frac {(\alpha \delta)^{-1}} {\eta})$ iterations, we have that
\begin{align}
    \bignormFro{U_T U_T^{\top} - X^{\star}} \lesssim \delta \log(\delta^{-1})
\end{align}
\end{theorem}

\subsubsection{Error dynamics (optional)}

Based on the update rule \eqref{eq_Et_1} for $E_t$, we have that
\[ \bignormFro{E_{t+1}}^2 = \bignormFro{E_t}^2 - 2\eta \inner{E_t}{(\id - \id_{u^{\star}}) M_t U_t} + \eta^2 \bignormFro{(\id - \id_{u^{\star}}) M_t U_t }^2 \]
When $\eta$ is sufficiently small, and $\bignormFro{M_t}, \bignorms{U_t}$ are both bounded from above, the third term on the right-hand side is negligible compared to the second term.
Therefore, we focus on the second term.
\begin{align*}
    \inner{E_t}{(\id - \id_{u^{\star}}) M_t U_t} 
    &= \frac 1 m \sum_{i=1}^m \inner{A_i} {U_t U_t^{\top} - X^{\star}} \inner{A_i} {(\id - \id_{u^{\star}}) E_t U_t^{\top}} \\
    &= \frac 1 m \sum_{i=1}^m \inner{A_i} {U_t U_t^{\top} - X^{\star}} \inner{A_i} {E_t U_t^{\top}}
\end{align*}
We use \eqref{eq_rip_cross} to analyze the above, but one gap here is that $U_t U_t^{\top}$ is not necessarily low-rank.
Thus, we decompose it into a low-rank part and an error part with small trace norm.
\begin{align}
    & \frac 1 m \sum_{i=1}^m \inner{A_i} {U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \inner{A_i} {E_t r_t {u^{\star}}^{\top}} \\
    \ge & \inner{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} {E_t r_t {u^{\star}}^{\top} } - \delta \bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \bignormFro{E_t r_t {u^{\star}}^{\top}} \\
    \ge & \inner{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}}{E_t r_t {u^{\star}}^{\top}} - 2\delta \bignorm{E_t r_t}
\end{align}
Above we use the fact that $\bignormFro{U_t U_t^{\top} - X^{\star} E_t E_t^{\top}}, \bignorm{r_t}, \bignorm{u^{\star}}$ are all bounded from above by $1$.
As for the $E_t E_t^{\top}$ part, we first note that $\inner{A_i}{E_t E_t^{\top}}^2 \ge 0$.
Thus, we can focus on the cross terms.
\begin{align*}
    \frac 1 m \sum_{i=1}^m \inner{A_i} {E_t E_t^{\top}} \inner{A_i} {E_t r_t {u^{\star}}^{\top}}
    &\ge \inner{E_t E_t^{\top}} {E_t r_t {u^{\star}}^{\top}} - \delta \bignorm{E_t E_t^{\top}}_{\star} \bignorm{E_t r_t {u^{\star}}^{\top}} \\
    &\ge \inner{E_t E_t^{\top}}{E_t r_t {u^{\star}}^{\top}} - \frac {\delta} 2 \bignorm{E_t r_t},
\end{align*}
where we are using the fact that $\bignorm{E_t E_t^{\top}}_{\star}$ is bounded from above by some constant.
The other cross term would be
\begin{align*}
    & \frac 1 m \sum_{i=1}^m \inner{A_i}{E_t E_t^{\top}} \inner{A_i}{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \\
    \ge& \inner{E_t E_t^{\top}} {U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} - \delta \bignorm{E_t E_t^{\top}}_{\star} \bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} \\
    \ge& \inner{E_t E_t^{\top}}{U_tU_t^{\top} - X^{\star}} - \delta \bignorm{E_t E_t^{\top}}_{\star}
\end{align*}
Because $E_t^{\top} X^{\star} = 0$ and also $\bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}}$ is bounded from above by $1$.
All put together, we can conclude that
\begin{align*}
    \inner{E_t}{(\id - \id_{u^{\star}}) M_t U_t}
    \ge \inner{U_t U_t^{\top} - X^{\star}}{E_t U_t^{\top}} - 2.5\delta \bignorm{E_t r_t} - \delta \bignormFro{E_t}^2
\end{align*}
Next, we have
\begin{align*}
    \inner{U_t U_t^{\top} - X^{\star}} {E_t U_t^{\top}}
    = \inner{U_t U_t^{\top}} {E_t U_t^{\top}}
    &= \inner{U_t^{\top}} {U_t^{\top} E_t U_t^{\top}} \\
    &= \inner{U_t^{\top}} {E_t^{\top} E_t U_t^{\top}} \\
    &= \inner{E_t U_t^{\top}} {E_t U_t^{\top}} \ge 0
\end{align*}
In summary, we have proved that
\begin{align}
    \bignormFro{E_{t+1}}^2 \le (1 + \eta \delta) \bignormFro{E_t}^2 + 2.5\delta\eta \bignorm{E_t r_t} + O(\eta^2)
\end{align}
We could also control the growth of the largest singular value of $E_t$ as
\begin{align}
    E_{t+1} &= E_t - \eta (\id - \id_{u^{\star}}) M_t U_t \\ 
    &= E_t - \eta (\id - \id_{u^{\star}}) E_t U_t^{\top} U_t + \eta(\id - \id_{u^{\star}}) (M_t U_t - E_t U_t^{\top} U_t)
\end{align}
Next, notice that $(\id - \id_{u^{\star}}) (U_t U_t^{\top} - X^{\star}) U_t = E_t U_t^{\top} U_t$, and
\begin{align*}
    & \bignorms{(\id - \id_{u^{\star}}) M_t U_t - (\id - \id_{u^{\star}}) (U_t U_t^{\top} - X^{\star}) U_t} \\
    \le & 4 \delta \left(\bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} + \bignorm{E_t E_t^{\top}}_{\star} \right) \bignorms{U_t} \\
    \le & 8 \delta \bignorms{U_t} \le 8 \delta (\bignorm{r_t} + \bignorms{E_t})
\end{align*}
Here, we are using two more properties about RIP measurements:
if $X$ is at most rank $r$, then
\begin{align} \bignorms{\frac 1 m \sum_{i=1}^m \inner{A_i}{X} A_i R - XR} \le \delta \bignormFro{X} \bignorms{R} \label{eq_rip_ur_rank}\end{align}
In addition, for any $X$, we have
\begin{align} \bignorms{\frac 1 m \sum_{i=1}^m \inner{A_i}{X}A_i R - XR} \le \delta \bignorm{X}_{\star} \bignorms{R} \end{align}
The same is true if we have another matrix $U$ where we insert before $A_i$:
\begin{align} \label{eq_rip_ur}
    \bignorms{\frac 1 m \sum_{i=1}^m \inner{A_i}{X} U A_i R - U X R}
    \le \delta \bignorms{U} \bignorm{X}_{\star} \bignorms{R}
\end{align}

Therefore, we can conclude that
\begin{align*}
    \bignorms{E_{t+1}} &\le \bignorms{E_t (\id - \eta U_t^{\top} U_t)} + 2 \eta \delta(\bignorm{r_t} + \bignorms{E_t}) \\
    &\le (1 + 2\eta\delta) \bignorms{E_t} + 2\eta\delta \bignorm{r_t}
\end{align*}

\subsubsection{Signal dynamics (optional)}

Next, we show that
\begin{align}
    \bignorm{r_{t+1} - (1 + \eta(1 - \bignorm{r_t}^2)) r_t }
    \le \eta \bignorms{E_t}^2 \bignorm{r_t} + 2\eta \delta (\bignorms{E_t} + \bignorm{r_t})
\end{align}
By the update rule $U_{t+1} + (\id - \eta M_t) U_t$.
We have that
\begin{align*}
    r_{t+1} = U_{t+1}^{\top} u^{\star}
    = U_t^{\top} (\id - \eta M_t^{\top}) u^{\star}
    = r_t - \eta U_t^{\top} M_t^{\top} u^{\star}
\end{align*}
Recall that $M_t = \frac 1 m \sum_{i=1}^m \inner{A_i}{U_t U_t^{\top} - X^{\star}} A_i$.
Using \eqref{eq_rip_ur_rank} and \eqref{eq_rip_ur}, we have that
\begin{align}
    \bignorm{r_{t+1} - (r_t - \eta U_t^{\star} (U_t U_t^{\top} - X^{\star}) u^{\star})}
    \le \delta (\bignormFro{U_t U_t^{\top} - X^{\star} - E_t E_t^{\top}} + \bignorm{E_t E_t^{\top}}_{\star}) \bignorms{U_t}
\end{align}
Here we observe that
\begin{align}
    U_t^{\top} (U_t U_t^{\top} - X^{\star}) u^{\star}
    = U_t^{\top} U_t r_t - r_t
    = (r_t r_t^{\top} + E_t^{\top} E_t) r_t - r_t
    = (\bignorm{r_t}^2 - 1) r_t - E_t^{\top} E_t r_t
\end{align}
Therefore, we have that
\begin{align}
    \bignorm{r_{t+1} - (1 + \eta(1 - \bignorm{r_t}^2)) r_t}
    \le \eta \bignorm{E_t^{\top} E_t r_t} + 2 \eta\delta \bignorms{U_t}
\end{align}
The above suggests that $r_t$ will eventually drift towards a vector whose norm is equal to one.

Putting things together, we essentially showed that $E_t$ will remain relatively smaller than $r_t$, since $E_t$ only grows by a rate of $1 + \eta \delta$.
Meanwhile, $r_t$ grows by a rate of $1 + O(\eta)$, when it is still bounded away from $1$.